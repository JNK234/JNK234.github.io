<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>The Transformer architecture has revolutionized the landscape of natural language processing in different applications like machine translations, summarization or chatbots. This is primarily due to the power of the attention mechanism of the Transformer. In this blog, let’s understand more about the working of the attention mechanism and how it played a crucial role in Transformer’s success. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6115" height="4375" data-attrs='{"src":"https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":4375,"width":6115,"resizeWidth":null,"bytes":null,"alt":"a person holding a baseball bat","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="a person holding a baseball bat" title="a person holding a baseball bat" srcset="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@medion4you" rel="external nofollow noopener" target="_blank">Norbert Braun</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>Components of the Transformer</h2> <p>The transformer architecture comprises two main parts: the Encoder and the Decoder. The Transformer's architecture in the figure below has Encoder and Decoder blocks connected to perform sequence-to-sequence tasks like machine translation. Later research shows these blocks can be used as stand-alone models for classification or text generation tasks. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Q-qk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 424w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 848w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1272w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png" width="1000" height="1217" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":1217,"width":1000,"resizeWidth":null,"bytes":205249,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":false,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 424w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 848w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1272w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png</figcaption></figure></div> <h6> </h6> <p>The transformer model architecture comprises the following components: </p> <ol> <li><p>Model Input/Output - Tokenization</p></li> <li><p>Embeddings</p></li> <li><p>Positional Encoding</p></li> <li><p>Multi-headed Attention for the Encoder and masked Multi-Head Attention for the Decoder.</p></li> <li><p>Layer addition and Normalization blocks. </p></li> <li><p>Feedforward neural networks </p></li> <li><p>Linear and Softmax layers</p></li> </ol> <p>Let’s have a brief overview of each of the components of the transformer architecture.</p> <h3>Tokenization</h3> <p>Models don't accept raw text as inputs. The raw input is broken down into smaller words or tokens using world-based or sub-word tokenization techniques. After Tokenization, the <strong>vocabulary</strong> is created with a list of all unique tokens from the input. </p> <pre><code># Example of tokenization
input_text = "Tokenization converts text into tokens"
print(tokenizer.tokenize(input_text))

<strong>Out:</strong> ['token', '##ization', 'converts', 'text', 'into', 'token', '##s']</code></pre> <p>We can see in the above code block example that the entire sentence is broken down into sub-words. The tokenizer used above is called the <strong>Sub-Word tokenizer, </strong>which splits the input sentence into possible smaller subwords or tokens. </p> <p>After a vocabulary is created by combining all possible tokens, these tokens are encoded (or assigned) with a unique number for every token. This encoded output is fed as input to the model. </p> <pre><code># Encode the text to get list of numbers
input_text = "Tokenization converts text into tokens"
print(tokenizer.encode(input_text))

<strong>Out:</strong> [101, 19204, 3989, 19884, 3793, 2046, 19204, 2015, 102]</code></pre> <p>The input text is now broken down into tokens and then converted into numbers, ready to be fed into the model. This end-to-end process is called <strong>Tokenization</strong>. </p> <p>Since the Encoder block has bi-directional attention, the entire input is fed into the model. But the Decoder block has casual or autoregressive attention, i.e. only the inputs before the target word is considered. </p> <h3>Token Embedding Layer</h3> <p>Further, these tokens are matched into a high-dimensional vector space called embeddings. <strong>Embeddings</strong> are high-order representations of every word or token in vector space. This will capture meaningful representations and relationships between words learnt during the model training. </p> <p>The transformer model will convert the tokenized input tokens to embeddings. This ensures that knowledge representation about the input word is captured. </p> <h3>Positional Encoding Layer</h3> <p>Token embeddings are just higher-order representations of input words. But this doesn’t capture the order of words in the input sentence. Therefore, <strong>positional embeddings</strong> are added to the existing word embeddings. In this way, the model can preserve the word order information. </p> <p>Positional embeddings are embedding representations of the index of the word as they occur in the input sequence. Therefore, when added with the word embeddings, the model will learn the order in which the word has happened in the sequence. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!cUxD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cUxD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png" width="1372" height="966" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":966,"width":1372,"resizeWidth":null,"bytes":42279,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cUxD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png</figcaption></figure></div> <h6></h6> <h3>Understanding Self Attention</h3> <p>The token embedding (word embeddings) and the positional embeddings are passed to the attention layers of the Encoder/Decoder. </p> <p>The attention layer will compute the self-attention weights for every word of the input to identify the relationship between the words. This is done by computing an attention map that will be learnt by the model during the training process and will define the context and dependence of every word with respect to other words in the input. </p> <p>Here is an example of visualization of the self-attention mechanism in transformers created using the <strong><a href="https://github.com/jessevig/bertviz" rel="external nofollow noopener" target="_blank">Bertviz</a> </strong>library. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!AYRs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AYRs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 424w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 848w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1272w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png" width="812" height="778" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":778,"width":812,"resizeWidth":null,"bytes":108267,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AYRs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 424w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 848w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1272w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Example of the Self-Attention Mechanism </figcaption></figure></div> <p>In the above example, the highlighted words on the left indicate the values of attention given to a corresponding word on the right. Therefore, the model can identify the reference of the word <strong>it</strong> in the input sequence and map it to the word <strong>monkey. </strong></p> <p>This ability to provide attention to the words and their relationship with other words helps the model to capture the context and consider the long-term dependencies. </p> <h3>Multi-Headed Attention &amp; Masked Multi-Headed Attention</h3> <p>In the above example, we explored how one layer of attention is used to identify the relationship between a pronoun (i.e. <strong>it</strong>) and the subject (i.e. <strong>monkey</strong>) in the same sentence. This single layer of attention learning the pronoun-subject relationship is called an <strong>Attention head</strong>. </p> <p>In the transformer architecture, there are usually multiple attention heads, i.e., 12-100 heads, present to learn and understand simultaneously different aspects and patterns of the language. These attention heads will specialize in various tasks learned during the model training. This is known as <strong>Multi-Headed Self Attention</strong>.</p> <p>The Encoder block of the transformer consists of the Multi-Headed Self-attention after the positional encoding layer to learn different features, aspects, patterns of the input sequence, and various dependencies of words within the text. Since the Encoder considers the entire input sequence, i.e. both the left and right side context of a word in the input sequence, to extract features, this type of attention is called <strong>Bi-Directional attention</strong>. </p> <p>Since the Decoder block mainly performs autoregressive tasks like text generation, i.e. next-word prediction by considering the past input tokens, the entire context of the input must not be considered for training, and the right side context of the word must be hidden from the attention layers. </p> <p>Therefore, the right side context of the input is masked during the training of the Multi-Head self-attention layers by using the mask token. This will prevent information about the future from leaking into the past. This type of attention is called <strong>Autogregressive Attention,</strong> and the <strong>Masked Multi-Headed Self Attention</strong> <strong>Layers</strong> are used for training the Decoder of the transformer model. </p> <h3>Layer Addition and Normalization Layers</h3> <p>The <strong>Layer Addition or Residual connections</strong> add the gradient output from the attention layers and the input to the attention layer before passing them to feedforward networks. This ensures that information is not lost during training and that the vanishing gradients problem is addressed. </p> <p>The <strong>Normalization</strong> layers are introduced to reduce unnecessary variations in the hidden vectors. This is done by normalizing every layer by scaling the inputs by setting unit mean and standard deviation. This also helps in speeding up the training process significantly. </p> <h3>Feed Forward Neural Networks</h3> <p>After processing through the Residual and Normalization layers, the attention layer’s outputs are passed to the fully connected feedforward layers for further post-processing of the output vector. This is primarily used to introduce the non-linearities in the model architecture and enable the deep learning magic to identify hidden patterns from the input data. </p> <p>Residual and Normalization operations are again applied to the outputs of the Feedforward layers before passing it through the linear and the softmax layers.</p> <h3>Linear layers with Softmax activation</h3> <p>Finally, the outputs are passed through the linear layers to obtain the logits for every token in the vocabulary. Further, these logits are passed through the softmax activation to convert and obtain the probabilities and then choose the final predicted word based on the highest probability. </p> <h2>Encoder-Only Model Architecture </h2> <p>The architecture of the Encoder-only model includes,</p> <ul> <li><p>Tokenization of entire inputs</p></li> <li><p>Positional Encoding layer = Token embeddings + positional embeddings</p></li> <li><p>Bi-directional or multi-headed attention layer</p></li> <li><p>Addition and Normalization layer </p></li> <li><p>Feed Forward Neural network </p></li> <li><p>Linear and Softmax layers </p></li> </ul> <p>This model type is best suited for extracting features and patterns from the input text and performing tasks like keyword extraction, named entity recognition and sentiment analysis. Examples of Encoder-only models are BERT, RoBERTa, etc.</p> <h2>Decoder-Only Model Architecture</h2> <p>The architecture of the Decoder-only model includes,</p> <ul> <li><p>Tokenization of the right-shifted inputs </p></li> <li><p>Positional Encoding layer = Token embeddings + positional embeddings</p></li> <li><p>Autoregressive or Masked-headed attention</p></li> <li><p>Addition and Normalization layer</p></li> <li><p>Feed Forward Neural network </p></li> <li><p>Linear and Softmax layers </p></li> </ul> <p>The Decoder model type is best suited for generating text, i.e. next word prediction task. Examples of Decoder-only models are GPT2, LLama and Falcon.</p> <h2>Encoder-Decoder based Model Architecture</h2> <p>The architecture of the Encoder-Decoder based model includes both the Encoder and the Decoder layers, as mentioned above. However, the outputs of the Encoder are fed into the second multi-headed attention block of the Decoder along with the past results of the multi-headed attention block, as per the architecture of transformers shown in the figure - I. </p> <p>Encoder-Decoder model architecture best suits sequence-to-sequence tasks like answering questions and machine translation. Examples of encoder-decoder models are T5 and BART. </p> <h2>Summary</h2> <p>We have a brief overview of the architecture and building blocks of the transformer model. To summarise,</p> <ul> <li><p>Transformer architecture consists of two blocks: the Encoder and the Decoder blocks.</p></li> <li><p>Converting the raw input text into tokens is known as tokenization. </p></li> <li><p>Tokens are converted to high dimensional vector space called Embeddings, which better represent the inputs and relationships between the tokens.</p></li> <li><p>Token embeddings are passed through the positional encoding layer to encode the positional information with token embeddings. </p></li> <li><p>Multi-headed attention computes attention weights to learn different aspects of the language and the dependence of words with every word.</p></li> <li><p>Multi-headed attention is present in the Encoder, whereas masked multi-headed attention is used in the Decoder.</p></li> <li><p>Residual connections and layer normalization are performed to retain important information and speed up the training. </p></li> <li><p>Output logits and probabilities are computed by passing through the linear layers and the softmax activation. Then, the final token is chosen based on the highest probability score. </p></li> </ul> <p>Thanks for reading! </p> <div><hr></div> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Step into a world of discovery with our technical newsletter. Delve deep into applied Machine Learning, Generative AI and advanced Deep Learning concepts as we unravel fundamental concepts and unveil the latest research trends. Embark on this exhilarating journey of learning and growth with us by subscribing to the newsletter! 🚀🤖</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your email…" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> </body></html>