<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>Reinforcement Learning (RL) is a branch of artificial intelligence that focuses on creating agents capable of making smart decisions by interacting with their environment through trial and error. This interaction is facilitated by a feedback loop involving states, actions, and rewards. The environment provides a state, the agent takes an action, and the environment responds with a reward and a new state. The goal of RL is to find a policy that maximizes the expected return when the agent acts according to it.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3448" height="4592" data-attrs='{"src":"https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":4592,"width":3448,"resizeWidth":null,"bytes":null,"alt":"white robot toy holding black tablet","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="white robot toy holding black tablet" title="white robot toy holding black tablet" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">Owen Beard</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h3>Policy and Decisions - Recap</h3> <p>At the heart of RL is the concept of a policy, which <strong>determines what action to take given the current state</strong>. Policies can be deterministic, always returning the same action for a given state, or stochastic, outputting a probability distribution over possible actions. The ultimate goal is to find the optimal policy (π*) that maximizes the expected return. </p> <p>There are two approaches to <strong>finding an optimal policy</strong> for the RL problem at hand:</p> <h4>Policy-based Approach</h4> <ul> <li><p>Policy-based methods directly train the policy to determine which action to take in a given state. </p></li> <li><p>This is achieved by optimizing the policy function to maximize the expected rewards. The policy is typically represented by a neural network and is trained without a value function (used to tell how good the agent is at particular state or to take particular action - Used in Value based Approach). </p></li> <li><p>The policy is not defined by hand but is learned through training.</p></li> </ul> <h4><strong>Value-Based Approach</strong></h4> <ul> <li><p>Value-based methods work indirectly by learning a value function that estimates how good it is to be in a particular state or which action to take in a given state.</p></li> <li><p>The value function is trained in value based approach and the policy is defined by hand i.e. there is a fixed policy function. For example,<strong> a greedy policy </strong>always chooses the action/state that leads to the highest value.</p></li> <li><p>Based on the information provided by the Value function (i.e. usually which state is more valuable or which action is best to take), the policy will decide the next action/state to move to.</p></li> </ul> <p>In value-based methods, the link between the value function and policy is crucial. The <strong>policy uses the value function to make decisions</strong>. The trained value function outputs the action-value pair for each state, which is used by the predefined policy function to choose the relevant action.</p> <p>For example, the <strong>epsilon-greedy policy</strong> balances exploration and exploitation by choosing the action with the highest value most of the time but occasionally selects a random action.</p> <p>Going on we will focus more on the Value based functions and its different variations.</p> <h3>Different types of Value based Functions</h3> <p>There are two types of value based functions that can be used to to get the expected return or reward for agent to take a decision according to a fixed policy. </p> <h4><strong>State-Value Function</strong></h4> <p>The state-value function, <em><strong>V(s)</strong></em>, outputs the expected return if the <strong>agent</strong> starts in a <strong>state s</strong> and follows the policy forever afterward. It is defined as:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"V(s) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t=s \\right]\n\n","id":"OAVNLLZQGZ"}' data-component-name="LatexBlockToDOM"></div> <p><strong>Interpretation</strong>:</p> <ul> <li><p><strong>Rt</strong>: The immediate reward received at time t.</p></li> <li><p><strong>γ</strong>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li> <li><p><strong>S_t = s</strong>: The condition that the agent is in state s at time t.</p></li> </ul> <p>This formula calculates the expected cumulative reward starting from state s and following the policy. It considers the immediate reward and the discounted future rewards.</p> <p>Imagine a maze where each state has a value representing how good it is to be in that state. The state-value function assigns these values based on the expected return from starting in that state and following the policy.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ju4u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" width="1400" height="788" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":788,"width":1400,"resizeWidth":null,"bytes":34576,"alt":null,"title":null,"type":"image/jpeg","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c</figcaption></figure></div> <h4>Action-Value Function</h4> <p>The action-value function, <em><strong>Q(s,a)</strong></em>, returns the expected return if the agent starts in <strong>state s</strong>, takes <strong>action a</strong>, and then follows the policy forever after. It is defined as:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"Q(s,a) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t=s, A_t=a \\right]","id":"ELWFLVAQWL"}' data-component-name="LatexBlockToDOM"></div> <p><br><strong>Interpretation</strong>:</p> <ul> <li><p><em><strong>Rt</strong></em>: The immediate reward received at time t.</p></li> <li><p><em><strong>γ</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li> <li><p><em><strong>S_t = s</strong></em>: The condition that the agent is in state <em><strong>s</strong></em> at time t.</p></li> <li><p><em><strong>A_t = a</strong></em>: The condition that the agent takes action <em><strong>a</strong></em> at time t.</p></li> </ul> <p><strong>Visual Representation</strong>:<br>Picture a similar maze but with each state-action pair having a value. The action-value function evaluates how good it is to take a specific action in a given state, considering the expected return from that action and the subsequent policy.</p> <h3>The need for Bellman Equation</h3> <p>In the different types of Value Functions mentioned above, we can see that the expected return to make a decision/take an action depends on agent starting in a state <em><strong>s</strong></em> and following the policy forever or until the episode ends and then summing the rewards. This is a computationally expensive process if it has to be repeated at every state. </p> <p>The Bellman equation provides a solution by breaking down the value function into smaller, manageable parts. It simplifies the calculation by considering the <strong>immediate reward</strong> and the <strong>discounted value of the next state</strong>:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V(s) = \\max_a \\left( R(s,a) + \\gamma V(s') \\right)\n&quot;,&quot;id&quot;:&quot;HIRLROAHVO&quot;}" data-component-name="LatexBlockToDOM"></div> <p><strong>Interpretation</strong>:</p> <ul> <li><p><em><strong>V(s)</strong></em>: The value of being in state <em><strong>s</strong></em>.</p></li> <li><p><em><strong>R(s,a)</strong></em>: The immediate reward received when taking action <em><strong>a</strong></em> in state <em><strong>s</strong></em>.</p></li> <li><p><em><strong>γ</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li> <li><p><em><strong>V(s')</strong></em>: The value of the next state s'.</p></li> </ul> <p>This recursive equation allows for efficient computation of state values without needing to calculate the expected return for each state from scratch. The Bellman equation is crucial for making value-based methods computationally feasible.</p> <h2><strong>Conclusion</strong></h2> <p>Understanding these concepts is essential for grasping the fundamentals of reinforcement learning. The Bellman equation plays a pivotal role in making value-based methods efficient, and visualizing state-value and action-value functions helps in understanding how agents make decisions based on these values. </p> <div><hr></div> <p>This article is part of our ongoing series on Reinforcement Learning and represents Issue #5 in the "Insights in a Jiffy" collection. We encourage you to read our previous issues in this series for a more comprehensive understanding <a href="https://neuraforge.substack.com/t/reinforcement-learning" rel="external nofollow noopener" target="_blank">here</a>. Each article builds upon the concepts introduced in earlier posts, providing a holistic view of the Reinforcement Learning landscape.</p> <p>If you enjoyed this blog, please click the ❤️ button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM","text":"Share","action":null,"class":"button-wrapper"}' data-component-name="ButtonCreateButton"><a class="button primary button-wrapper" href="https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM" rel="external nofollow noopener" target="_blank"><span>Share</span></a></p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p></p> <p></p> </body></html>