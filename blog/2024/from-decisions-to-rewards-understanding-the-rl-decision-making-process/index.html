<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>Imagine you're new to a city and trying to find the best restaurant for dinner. Each time you eat out, you rate your experience. Sometimes, you return to restaurants you enjoy (exploitation), while other times, you try new places (exploration). Over time, you learn which restaurants consistently provide the best meals, helping you make better dining choices.</p> <p>This everyday scenario mirrors the core principles of Reinforcement Learning (RL). In this blog post, we'll explore three fundamental concepts in RL: the mechanics of rewards, the mathematics of discounting, and the strategic balance between exploration and exploitation.</p> <p>Let's delve into these concepts, examining their technical foundations and practical implications using our restaurant-finding scenario as a running example.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3024" height="4032" data-attrs='{"src":"https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":4032,"width":3024,"resizeWidth":null,"bytes":null,"alt":"group of people inside the restaurant","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="group of people inside the restaurant" title="group of people inside the restaurant" srcset="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">kayleigh harrington</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>The Mechanics of Rewards in Reinforcement Learning</h2> <h3>Defining Rewards</h3> <p><strong>Rewards</strong> in Reinforcement Learning (RL) are crucial because they are the only feedback an agent receives to determine whether an action is good or bad. Rewards are numerical feedback signals that the environment provides to the agent after each action. In our restaurant scenario, the reward could be your satisfaction rating after each meal, perhaps on a scale from 1 to 10.</p> <p>The cumulative reward at each time step <em>t</em> represents the total rewards an agent has accumulated up to that point. In our example, this would be the sum of all your restaurant ratings over time.</p> <p>Formally, the cumulative reward R(τ) at time step <em>t</em> can be expressed as:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"R(\\tau) = r_{t} + r_{t+1} + r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} r_{t+k}\n","id":"FAVYNMJJQN"}' data-component-name="LatexBlockToDOM"></div> <p>Where R(τ) is the cumulative reward.</p> <p>However, this approach assumes that all rewards, regardless of when they are received, are equally valuable, which isn't typically the case in real-world scenarios. In reality, rewards that occur sooner are often more valuable and predictable than those received in the distant future. This is where <strong>reward discounting</strong> comes into play, allowing the agent to weigh short-term and long-term rewards differently.</p> <h3>Reward Discounting </h3> <p>To account for the varying importance of rewards over time, we introduce a <strong>discount factor</strong> <em>γ</em>, where <em>γ</em> is a value between 0 and 1 (usually between 0.95 and 0.99). The discount factor helps prioritize short-term versus long-term rewards:</p> <ul> <li><p>A larger <em>γ</em> (closer to 1) implies a smaller discount, meaning the agent values future rewards more and is more focused on long-term success.</p></li> <li><p>A smaller <em>γ</em> (closer to 0) implies a larger discount, meaning the agent values immediate rewards more and focuses on short-term gains.</p></li> </ul> <p>The formula for the <strong>discounted cumulative reward</strong>​ at time step <em>t</em> is given by:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"G_t = \\gamma^0 r_{t} + \\gamma^1 r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots\n","id":"QBZJFCUFNV"}' data-component-name="LatexBlockToDOM"></div> <div class="latex-rendered" data-attrs='{"persistentExpression":"G_t = r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n","id":"FGNJDJMZWU"}' data-component-name="LatexBlockToDOM"></div> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!VTZr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VTZr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 424w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 848w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1272w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png" width="1404" height="672" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":672,"width":1404,"resizeWidth":null,"bytes":108790,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VTZr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 424w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 848w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1272w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p>In our restaurant example, a high <em>γ</em> might represent your willingness to try a new restaurant with great reviews, even if it's farther away or more expensive. A low γ would mean you're more likely to stick with nearby, familiar options.</p> <h2>Exploration and Exploitation Tradeoff</h2> <p>In RL, agents face the challenge of balancing <strong>exploration</strong> and <strong>exploitation</strong>:</p> <ul> <li><p><strong>Exploration</strong> involves trying out new actions to discover more information about the environment. This helps the agent learn about potentially better actions that it hasn't encountered before.</p></li> <li><p><strong>Exploitation</strong> involves using the knowledge the agent has already acquired to maximize its reward by choosing the best-known actions.</p></li> </ul> <p>Given that RL operates under the <strong>reward hypothesis</strong>—the idea that the goal is to maximize cumulative reward—agents might be tempted to exploit known actions repeatedly. However, this can trap the agent in suboptimal behaviour, preventing it from exploring potentially better actions.</p> <p>To avoid this, a balance between exploration and exploitation is necessary. The agent must explore enough to discover better strategies while exploiting its current knowledge to achieve high rewards. Striking this balance is key to successful learning in RL.</p> <p> In our scenario:</p> <ul> <li><p>Exploration is trying new restaurants you've never visited before.</p></li> <li><p>Exploitation is returning to restaurants you know you like.</p></li> </ul> <p>If you only ate at your favourite restaurant (pure exploitation), you might miss out on discovering even better places. Conversely, you might have too many mediocre meals if you always try new restaurants (pure exploration).</p> <h2>Conclusion</h2> <p>In conclusion, rewards and discounting play a crucial role in helping reinforcement learning agents evaluate actions over time. However, the agent also relies on a <strong>policy</strong>, which serves as its decision-making framework. The policy dictates how the agent chooses actions based on the current state, ultimately guiding it toward maximizing long-term rewards. Next, we’ll explore how policies are formed and optimized in RL.</p> <div><hr></div> <p>If you enjoyed this blog, please click the ❤️ button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p></p> </body></html>