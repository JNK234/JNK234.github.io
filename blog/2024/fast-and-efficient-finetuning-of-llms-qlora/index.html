<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities across a wide range of tasks. However, fine-tuning these massive models for specific applications presents significant challenges, particularly in terms of computational resources and memory requirements. Enter <strong>QLoRA</strong> (Quantised Low-Rank Adaptation), an innovative technique that combines the benefits of <strong>quantization</strong> and <strong>low-rank adaptation</strong> to enable cheap, fast and efficient fine-tuning of LLMs when hardware resources are limited. </p> <p>In this blog post, we'll explore QLoRA's quantisation and low-rank adaptation, implementation, and impact on training LLMs.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="2617" height="1608" data-attrs='{"src":"https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":1608,"width":2617,"resizeWidth":null,"bytes":null,"alt":"time lapse photography of three men cycling","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="time lapse photography of three men cycling" title="time lapse photography of three men cycling" srcset="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">paolo candelo</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>Background - Quantisation and LoRA</h2> <p>Before we delve into QLoRA, let's establish a foundational understanding of its key components, i.e. Quantisation and Low-Rank adaptation (LoRA)</p> <h3>Quantization </h3> <p>Quantization is a technique for reducing the precision of a model's parameters and activations. By representing these values with fewer bits, quantization significantly reduces memory usage and computational requirements, often with minimal impact on model performance. </p> <h4>Precision-based Quantisation</h4> <p>Here are different precision quantisation <strong>representation</strong> data that are currently used for training models:</p> <ol> <li> <p><strong>FP32 (32-bit floating-point)</strong>: </p> <ul> <li><p>Standard precision is used in most deep-learning training.</p></li> <li><p>Provides a wide dynamic range (1.18 × 10^-38 to 3.4 × 10^38) and high precision. It has 32 bits, 8 exponent bits and 23 fraction bits.</p></li> <li><p>The memory required to store one value is 4 bytes.</p></li> <li><p>Serves as the baseline for comparing other quantization methods.</p></li> </ul> </li> <li> <p><strong>FP16 (16-bit floating-point)</strong>: </p> <ul> <li><p>Reduces memory usage by half compared to FP32. </p></li> <li><p>Dynamic range of 6.10 × 10^-5 to 6.55 × 10^4. It has 16 bits, 5 exponent bits and 10 fraction bits.</p></li> <li><p>The memory required to store one value is 2 bytes.</p></li> <li><p>Commonly used in mixed-precision training to balance accuracy and efficiency.</p></li> </ul> </li> <li> <p><strong>BF16 (Brain Floating Point)</strong>:</p> <ul> <li><p>Uses 16 bits like FP16 but with a different distribution: 16 bits, 8 exponent bits, and 7 fraction bits.</p></li> <li><p><strong>BF16</strong> is a compromise between FP32 and FP16. It is designed to maintain much of FP32's dynamic range while offering the memory and computational benefits of a 16-bit format.</p></li> <li><p>Offers a larger dynamic range than FP16 (1.18 × 10^-38 to 3.4 × 10^38), making it more suitable for training.</p></li> <li><p>The memory required to store one value is 2 bytes.</p></li> <li><p>Increasingly popular in modern AI hardware due to its balance of range and precision.</p></li> </ul> </li> <li> <p><strong>INT8 (8-bit integer)</strong>:</p> <ul> <li><p>Represents values using 8 bits, typically in the range -128 to 127 or 0 to 255. It has 8 bits, 8 exponent bits and 7 fraction bits.</p></li> <li><p>Dramatically reduces memory usage and increases inference speed.</p></li> <li><p>Requires careful calibration to maintain accuracy, often using techniques like quantization-aware training or post-training quantization.</p></li> </ul> </li> <li> <p><strong>INT4 (4-bit integer)</strong>: </p> <ul> <li><p>Pushes the boundaries of low-precision representation, using only 4 bits per value.</p></li> <li><p>Requires advanced techniques to maintain model quality.</p></li> <li><p>The focus of recent research, including QLoRA, for ultra-efficient model compression.</p></li> </ul> </li> </ol> <h4>Quantization Schemes</h4> <p>A quantization scheme is a method for mapping a large set of input values to a smaller set of output values. It is typically used to reduce the precision of data representation. For example, a quantization scheme will be used to convert and represent data from FP32 format to BF16 or INT8 format.</p> <p>Some important quantization factors to be considered are:</p> <ul> <li> <p><strong>Scaling Factor</strong>: </p> <ul> <li><p>The value is used to convert between the original floating-point values and the quantised integer values.</p></li> <li><p>It helps maintain the relative relationships between values while mapping them to a smaller integer range.</p></li> <li><p>Formula: <strong>scale = (float_max - float_min) / (int_max - int_min)</strong></p></li> <li><p>Usage: <strong>quantized_value = round(original_value / scale)</strong></p></li> </ul> </li> <li> <p><strong>Zero-point</strong>:</p> <ul> <li><p>The zero-point is the integer value that represents the real-value zero in the quantized space.</p></li> <li><p>It allows the representation of both positive and negative values using only unsigned integers.</p></li> <li><p>Formula: <strong>zero_point = round(-float_min / scale)</strong></p></li> <li><p>Usage: <strong>quantized_value = round(original_value / scale) + zero_point</strong></p></li> </ul> </li> </ul> <p>Based on the above parameters, some important quantization schemes include the following:</p> <ol><li> <p><strong>Linear Quantization</strong>: </p> <ul> <li><p>Maps floating-point values to integers using a linear scaling factor and zero-point.</p></li> <li><p>Quantization formula: <strong>q = round(x / scale) + zero_point</strong></p></li> <li><p>Dequantization formula: <strong>x = (q - zero_point) * scale</strong></p></li> <li><p>Simple to implement but may not capture the distribution of weights effectively.</p></li> </ul> </li></ol> <ol start="2"> <li> <p><strong>Non-linear Quantization</strong>: </p> <ul> <li><p>It uses non-linear mapping between floating-point and quantized values.</p></li> <li><p>Can better represent the typical distribution of weights in neural networks, which often follow a normal or log-normal distribution.</p></li> <li><p>Examples include <strong>logarithmic quantization</strong> and the <strong>NormalFloat</strong> scheme used in QLoRA.</p></li> </ul> </li> <li> <p><strong>Symmetric vs Asymmetric Quantization</strong>: </p> <ul> <li><p>Symmetric: Uses a zero-point at 0, simplifying computations. Formula: <br><strong>q = round(x / scale)</strong></p></li> <li><p>Asymmetric: Allows for a non-zero offset, potentially capturing the weight distribution better. Uses the full formula: <br><strong>q = round(x / scale) + zero_point</strong></p></li> </ul> </li> </ol> <p> Now, let’s understand the Low-Rank Adaptation in detail.</p> <h3>Low-Rank Adaptation (LoRA) in Depth</h3> <p>LoRA, introduced by Hu et al. (2021), is a parameter-efficient fine-tuning (PEFT) method that <strong>freezes the pre-trained model</strong> weights and <strong>injects trainable low-rank matrices </strong>into each layer of the transformer architecture.</p> <p>The fundamental idea behind LoRA is to represent the weight updates during fine-tuning as the product of two low-rank matrices rather than updating the entire model. This approach significantly reduces the number of trainable parameters while allowing for effective model adaptation to new tasks.</p> <h4>Mathematical Formulation of LoRA</h4> <p>Pre-trained LLMs have a low intrinsic dimension and can still learn effectively despite being randomly projected to a smaller space. </p> <p>Let <code>W₀ ∈ ℝᵈˣᵈ</code> be the pre-trained weights of a layer in the original model. During fine-tuning with LoRA, instead of directly updating W₀, we introduce a low-rank update:</p> <p><code>W = W₀ + BA</code></p> <p>Where:</p> <ul> <li><p><code>B ∈ ℝᵈˣʳ</code> is a matrix of dimension <code>d × r</code></p></li> <li><p><code>A ∈ ℝʳˣᵈ</code> is a matrix of dimension <code>r × d</code></p></li> <li><p><em>r</em> is the rank of the update (typically much smaller than d and k)</p></li> </ul> <p>The product <code>BA</code> represents the weight update and <strong>only A and B are trained during fine-tuning</strong>. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!RXtj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!RXtj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 424w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 848w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1272w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png" width="1247" height="528" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":528,"width":1247,"resizeWidth":null,"bytes":69174,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!RXtj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 424w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 848w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1272w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p>The above weight matrix weight decomposition is applied to the self-attention modules of the transformer. Therefore, only these modules will be trained during training while the remaining pre-trained model weights are frozen. </p> <h4>Key Components of LoRA</h4> <ol> <li> <p><strong>Rank (r)</strong>:</p> <ul> <li><p>Determines the expressiveness of the update.</p></li> <li><p>A lower rank means fewer parameters but potentially less adaptability.</p></li> <li><p>Typically, it ranges from 1 to 64, with 8 or 16 being common choices.</p></li> <li> <p>The number of trainable parameters introduced by LoRA is </p> <p><code>2 * r * (d + d) </code>for each adapted layer.</p> </li> </ul> </li> <li> <p><strong>Scaling Factor (α)</strong>: </p> <ul> <li><p>Used to adjust the magnitude of the LoRA update.</p></li> <li><p>The actual update is scaled: <code>α(BA) / r</code></p></li> <li><p>It helps in balancing the contribution of the pre-trained weights and the LoRA update.</p></li> <li><p>It can be thought of as an additional hyperparameter controlling the learning rate of LoRA parameters.</p></li> </ul> </li> <li> <p><strong>Target Modules</strong>: </p> <ul> <li><p>Specifies which layers or sub-modules of the model to apply LoRA to.</p></li> <li><p>Common choices include <strong>attention layers (query and value projections)</strong> in transformer models.</p></li> <li><p>Selecting appropriate target modules can significantly impact the efficiency-performance trade-off.</p></li> </ul> </li> </ol> <h4>LoRA in Practise </h4> <p>During fine-tuning, LoRA is implemented as follows:</p> <ol> <li> <p><strong>Initialization</strong>: </p> <ul> <li><p>LoRA matrices (A and B) are typically initialized randomly using a normal distribution with a small variance.</p></li> <li><p>The scaling factor α is set to a small value (e.g., 1) at the start of training.</p></li> </ul> </li> <li> <p><strong>Training Process</strong>: </p> <ul> <li><p>During the forward pass, the LoRA update is added to the output of the target modules (as shown in the figure above)<br><code>y = W₀x + BAx</code></p></li> <li><p>Gradients are computed with respect to A and B using standard backpropagation, leaving W₀ unchanged.</p></li> <li><p>The effective learning rate for LoRA parameters is scaled by <code>α/r</code>.</p></li> </ul> </li> <li> <p><strong>Inference</strong>: </p> <ul> <li><p>For efficient inference, the LoRA updates can be merged with the original weights: W = W₀ + BA</p></li> <li><p>This allows for using the adapted model without additional computational overhead during inference.</p></li> </ul> </li> </ol> <p>Now, let’s understand the big picture of how QLoRa is implemented as a combination of LoRA and Quantization. </p> <h3>QLoRA: Combining Quantisation and LoRA</h3> <p>QLoRA, introduced by Dettmers et al. (2023), integrates advanced quantization techniques with LoRA to create a highly efficient fine-tuning method for large language models.</p> <p>Before we delve into the specifics of the QLoRa workflow, we should note some key innovations introduced in the QLoRA paper. </p> <h4>Key Innovations</h4> <ol><li><p><strong>4-bit NormalFloat Quantization</strong></p></li></ol> <p>Normal float is a novel quantization data type optimized for training neural networks while maintaining performance levels.</p> <ul> <li><p>Assumes a normal distribution of weights, which is common in trained neural networks.</p></li> <li><p>Uses a <strong>non-linear quantization scheme</strong> to provide better precision for values near zero.</p></li> <li><p>Outperforms other 4-bit quantization methods for large language models.</p></li> </ul> <p>The NormalFloat quantization process involves:</p> <ol> <li><p>Estimating the parameters (μ, σ) of the normal distribution that best fits the weight tensor.</p></li> <li><p>Defining non-linear quantization boundaries based on the cumulative distribution function (CDF) of the normal distribution.</p></li> <li><p>Mapping weights to 4-bit integers based on these boundaries.</p></li> </ol> <p>4-bit Normal Float is the datatype used to quantise and store base model weights during QLoRA training. </p> <ol start="2"><li><p><strong>Double Quantization</strong></p></li></ol> <p>Double quantization is a technique that further reduces memory usage:</p> <ol> <li><p>First, it quantizes model weights to 4-bit precision using NormalFloat.</p></li> <li><p>Then, it quantizes the resulting quantization constants (scaling factors and zero-points) to 8-bit precision.</p></li> </ol> <p>This two-step process significantly reduces the memory footprint of quantization constants, which can be substantial in large models.</p> <ol start="3"><li><p><strong>Paged Optimizers</strong></p></li></ol> <p>Paged optimizers efficiently manage memory by:</p> <ul> <li><p>Utilizing CPU memory as a backup when GPU memory is exhausted.</p></li> <li><p>Implementing a sophisticated paging system to swap data between GPU and CPU memory.</p></li> <li><p>Minimizing the performance impact of using CPU memory through careful optimization and prefetching strategies.</p></li> </ul> <h4>Integration of Quantization and LoRA</h4> <p>QLoRA combines quantization and LoRA in a synergistic manner:</p> <ol> <li> <p><strong>Quantized Base Model</strong>:</p> <ul> <li><p>The <strong>pre-trained model weights (W₀)</strong> are quantized to 4-bit precision using <strong>NormalFloat</strong>.</p></li> <li><p>This drastically reduces the memory footprint of the base model.</p></li> </ul> </li> <li> <p><strong>Full-precision LoRA Updates</strong>:</p> <ul> <li><p>LoRA matrices (A and B) are kept at higher precision (typically 16-bit, i.e. BF16) during training.</p></li> <li><p>This allows for accurate gradient computation and weight updates.</p></li> </ul> </li> <li> <p><strong>Quantization-aware Training</strong>:</p> <ul> <li><p>During the forward pass, quantized weights are dequantized, the LoRA update is applied, and the result is requantized.</p></li> <li><p>This process ensures that the model learns to perform well within the constraints of quantization.</p></li> </ul> </li> <li> <p><strong>Memory-efficient Optimization</strong>:</p> <ul> <li><p>Paged optimizers manage the memory of both the quantized base model and the full-precision LoRA parameters.</p></li> <li><p>Gradient accumulation is used to simulate larger batch sizes without increasing memory requirements.</p></li> </ul> </li> </ol> <h4>End-to-End QLoRA Workflow</h4> <ol> <li><p>Load the pre-trained model and quantize it to 4-bit precision using NormalFloat.</p></li> <li><p>Add LoRA adapters to the quantized model, initializing them in 16-bit precision (BF16)</p></li> <li><p>Use paged optimizers and gradient accumulation for memory-efficient training.</p></li> <li><p>During training, perform quantization-aware forward and backward passes.</p></li> <li><p>Update only the LoRA parameters, keeping the quantized base model fixed.</p></li> <li><p>For inference, merge the LoRA updates with the quantized base model or keep them separate for task-specific adaptation.</p></li> </ol> <p>This combination of techniques allows <strong>QLoRA to fine-tune models with billions of parameters on consumer-grade hardware, democratizing access to state-of-the-art language models.</strong></p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!58n6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!58n6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 424w, https://substackcdn.com/image/fetch/$s_!58n6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 848w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1272w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif" width="600" height="439" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":439,"width":600,"resizeWidth":null,"bytes":322447,"alt":null,"title":null,"type":"image/gif","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!58n6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 424w, https://substackcdn.com/image/fetch/$s_!58n6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 848w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1272w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p>Now that we have understood the theory of how QLoRA works in detail let’s implement it in code and fine-tune a large language model.</p> <h2>Implementation of QLoRA </h2> <ol><li><p>First, Install the necessary libraries:</p></li></ol> <p>Let's walk through a practical implementation of QLoRA using the Hugging Face Transformers and PEFT libraries.</p> <pre><code>pip install transformers peft bitsandbytes accelerate</code></pre> <ol start="2"><li><p>Load and Quantize the model.</p></li></ol> <p>Here, we are loading a 6.7 billion-parameter model in a <strong>4-bit NormalFloat </strong>datatype with <strong>double quantization</strong> enabled. The LoRA parameters are loaded in the <strong>BF16</strong> datatype.</p> <pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "facebook/opt-6.7b"  # Example large model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)</code></pre> <ol start="3"><li><p>Configure LoRA</p></li></ol> <p>Convert the pre-trained model into the LoRA model by adding the LoRA adapters and specifying parameters like rank (r), alpha, and target modules. </p> <pre><code>from peft import LoraConfig, get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)</code></pre> <ol start="4"><li><p>Load and Prepare a Dataset</p></li></ol> <p>For this example, let’s use the IMDb dataset from the datasets library to fine-tune our LLM.</p> <pre><code>from datasets import load_dataset

dataset = load_dataset("imdb")  # Example dataset

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])</code></pre> <ol start="5"><li><p>Configure Training Arguments</p></li></ol> <p>Let’s configure the training arguments before we start training the model using the <strong>transformers</strong> library. Note the optimizer is set as <strong>paged_adamw_8bit </strong>to efficiently manage memory with the CPU as a backup.</p> <pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    save_total_limit=3,
    logging_steps=100,
    optim="paged_adamw_8bit"
)</code></pre> <ol start="6"><li><p>Train the model</p></li></ol> <pre><code>from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

trainer.train()</code></pre> <p>Post training the model is saved for further inference.</p> <h2>Conclusion </h2> <p>QLoRA (Quantized Low-Rank Adaptation) represents a significant leap forward in model fine-tuning. By combining quantization techniques with low-rank adaptation, QLoRA dramatically reduces the memory footprint required for training while maintaining model quality. This breakthrough allows for faster, more efficient fine-tuning of large language models on consumer-grade hardware, opening up new possibilities for customization and specialization of AI models.</p> <p>The advent of QLoRA, alongside other innovative training methods like LoRA, PEFT, and instruction fine-tuning, is democratizing access to powerful language models. These techniques are making it possible for researchers, developers, and organizations of all sizes to work with and adapt state-of-the-art LLMs for specific applications. As these methods continue to evolve, we're moving closer to a future where advanced AI capabilities are not limited to tech giants but are accessible to a global community of innovators.</p> <h3>References</h3> <ol> <li><p>Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314.</p></li> <li><p>Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</p></li> <li><p>https://huggingface.co/blog/4bit-transformers-bitsandbytes</p></li> </ol> <div><hr></div> <p>If you enjoyed this blog, please click the ❤️ button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community. Thank you!</p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p></p> </body></html>