<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>The abilities and effectiveness of large language models (LLMs) increasingly resemble human-like qualities, surpassing human capabilities in certain aspects, thanks to their remarkable text generation capabilities. A key factor contributing to this success is the ability to fine-tune these models to produce coherent and creative text. By adjusting the mechanisms behind text generation, we can steer LLMs to generate text in desired ways. Continuing our blog series, we will delve into additional sampling methods, such as nucleus sampling, custom sampling, and beam search sampling, to further enhance the quality of text generation.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3308" height="4135" data-attrs='{"src":"https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":4135,"width":3308,"resizeWidth":null,"bytes":null,"alt":"person holding light bulb","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="person holding light bulb" title="person holding light bulb" srcset="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@jdiegoph" rel="external nofollow noopener" target="_blank">Diego PH</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>Exploring Text Generation Techniques</h2> <h3><strong>1. Top-k Sampling</strong></h3> <p>Top-k sampling is a widely used variation of random sampling that limits the pool of tokens we can sample at each timestep. The core concept of top-k sampling is to eliminate low-probability options by only considering the <em>k</em> tokens with the highest probability of sampling. This technique can often produce more natural-sounding text than other techniques.</p> <h4><strong>How Top-k Sampling Works</strong></h4> <ul> <li><p>Top-k sampling restricts the possible next words to the k most probable ones, effectively shrinking the dictionary it uses for prediction.</p></li> <li><p>Lower k values in top-k sampling promote conservative text generation by prioritizing highly probable words within the context.</p></li> <li><p>For example, if k is 50, we choose the top 50 tokens from the vocabulary and then perform random sampling from these 50 tokens to generate text. </p></li> </ul> <h4><strong>Implementation</strong></h4> <p>Top-k Sampling can be activated easily in the transformers <code>generate()</code> function by setting the <code>do_sample</code> parameter to True, specifying the <code>temperature</code> parameter to control creativity and <code>top_k</code> to define the k tokens from which to sample. The choice of the parameter <code>k</code> is to be manually chosen based on the vocabulary size and probability distribution. </p> <p>Below is an example of how to implement top-k sampling with the <strong>GPT-2 XL </strong>model using the transformers library:</p> <p><strong>Case I: With </strong><code>temperature=2.0</code><strong> (high) and </strong><code>top_k=70</code></p> <pre><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup the model and tokenizer
checkpoint = "gpt2-xl"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

# Encode the inputs
input_text = "Once upon a time, there was a man who"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Top-k sampling
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=2.0)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who wanted an autographeter a lot in honor "something his old band, Bon Iver's indie icon brothers Scott and Jared had made for him", thus started "This Old Radio Show" in July 2016 by Aaron Schock's son Kyle as guest cohost. [1] As per [note2].""&lt;|endoftext|&gt;</code></pre> <p>The above-generated output is very unusually diverse and doesnâ€™t have clarity as to what it is conveying. Setting a high <code>temperature</code> parameter value (i.e. &gt;1.0) is responsible for the high variation and diversity in the generated content.</p> <p><strong>Case II: With </strong><code>temperature=0.9</code><strong> and </strong><code>top_k=70</code></p> <pre><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_k=50)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who made a beautiful thing, a beautiful thing that would bring happiness, joy, beauty, and hope to everyone - one that had a heart and soul that would not be broken.

The man, who made this, called it "I am the Sun".

His name was, and is, J.C. Sutter.</code></pre> <p>We can observe that the above text has better clarity and coherence than the previous output. </p> <p>Now, letâ€™s explore another sampling method that aids in restricting the output distribution but with a dynamic cutoff, unlike top-k, where we have a fixed cut-off of tokens. </p> <h3><strong>2. Nucleus Sampling</strong></h3> <p> Nucleus Sampling, or top-p sampling, is another important technique for controlling the randomness and creativity of the generated text. With top-p sampling, instead of choosing a fixed cutoff value, we set a condition for when to cut off. This condition is when a certain probability mass in the selection is reached. </p> <p>While top-k sampling focuses on the total number of considered words, top-p focuses on the total probability or cumulative probability captured. </p> <h4><strong>How Nucleus Sampling Works</strong></h4> <ul> <li><p>This method sets a probability threshold (<code>p</code>) instead of a fixed number of words (<code>k</code>). </p></li> <li><p>Then, it selects all the tokens from the vocabulary sorted in descending order until the cumulative probability reaches or exceeds the threshold <code>p</code>. </p></li> <li><p>For example, if p is 95%, we choose all the tokens whose cumulative probability equals 95%. </p></li> </ul> <p>This technique can be more flexible than top-k because the number of words considered can be dynamically changed based on probability distribution. </p> <h4><strong>Implementation</strong></h4> <p>Nucleus sampling can be activated in the transformerâ€™s <code>generate()</code> function by setting the probability threshold using the <code>top_p</code> parameter. Then, we can control the randomness and diversity using the <code>temperature</code> parameter. </p> <p>Below is an example of how to implement top-p sampling with the <strong>GPT-2 XL </strong>model using the transformers library:</p> <pre><code><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_p=0.9)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who came to the King of the Fairies and asked, "what is the best way to gain eternal life?" He heard that the Fairy Queen had a secret name, and would answer nothing, only that she was "the most beautiful lady in all the world." He asked her, "Whence comes that name?" And she answered that she had it</code></pre> <p>Now, letâ€™s understand how custom sampling performs for text generation, where we can employ both top_k and top_p parameters.</p> <h3><strong>3. Custom Sampling</strong></h3> <p>Custom sampling combines top_k and top_p sampling techniques to achieve the best of both worlds. If we set top_k=50 and top_p=0.9, this corresponds to the rule of choosing tokens with a probability mass of 90% from a pool of at most 50 tokens.</p> <p>This method provides a finer degree of control over the generated text, helps generate grammatically correct text, and is potentially more creative than individual sampling techniques. </p> <h4><strong>Implementation</strong></h4> <p>We can implement this by defining both top_k and top_p parameters. Consider the example below.</p> <pre><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_p=0.9, top_k=50)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time ive been the sole proprietor of a small business, and since its inception i have been very much influenced by the people i have met through the website, and i will continue to do so.

My business goal is to grow a small business and take it to the next level, and i want to do that through a forum like this, </code></pre> <h2>Conclusion</h2> <p>The potential and prowess of large language models (LLMs) in generating text, images, and multimedia are advancing at an exponential rate and show no signs of slowing down as research and development efforts persist. As such, we must grasp and master various text generation techniques, which play a pivotal role in achieving desired outcomes from LLMs through appropriate fine-tuning. In conclusion, our exploration of these techniques is not just an academic exercise but a practical necessity in harnessing the full capabilities of LLMs in the ever-evolving landscape of artificial intelligence.</p> <p>Happy Learning !!</p> <div><hr></div> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! ðŸš€ðŸ¤–</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your emailâ€¦" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> <p></p> <p></p> <p></p> </body></html>