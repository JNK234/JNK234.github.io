<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1>Introduction</h1> <p>As deep learning continues to advance artificial intelligence applications, PyTorch has established itself as a fundamental framework powering everything from computer vision systems to large language models. Originally developed by Metaâ€™s AI Research lab, PyTorch combines Python's flexibility with deep learning capabilities through a powerful, intuitive interface.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6000" height="4000" data-attrs='{"src":"https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":4000,"width":6000,"resizeWidth":null,"bytes":null,"alt":"blue building block lot","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="blue building block lot" title="blue building block lot" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">Iker Urteaga</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h3>Core Components of PyTorch</h3> <p>PyTorch's architecture rests on three key components that work together to enable efficient deep learning development:</p> <ol> <li> <p><strong>Dynamic Tensor Library</strong></p> <ul> <li><p>Extends NumPy's array programming capabilities</p></li> <li><p>Provides seamless CPU and GPU acceleration</p></li> <li><p>Implements efficient mathematical operations for deep learning computations</p></li> </ul> </li> <li> <p><strong>Automatic Differentiation Engine (Autograd)</strong></p> <ul> <li><p>Computes gradients automatically through computational graphs</p></li> <li><p>Manages backpropagation for neural network training</p></li> </ul> </li> <li> <p><strong>Deep Learning Framework</strong></p> <ul> <li><p>Delivers modular neural network components</p></li> <li><p>Implements optimized loss functions and optimizers</p></li> </ul> </li> </ol> <h2>Getting Started with PyTorch</h2> <h3>Installation and Setup</h3> <p>PyTorch can be installed directly using pip, Python's package installer:</p> <pre><code><code>pip install torch</code></code></pre> <p>However, for optimal performance, it's recommended to install the version specifically compatible with your system's hardware. VisitÂ <a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">pytorch.org</a>Â to get the appropriate installation command based on your:</p> <ul> <li><p>Operating system</p></li> <li><p>Package manager preference (pip/conda)</p></li> <li><p>CUDA version (for GPU support)</p></li> <li><p>Python version</p></li> </ul> <h3>GPU Support and Compatibility</h3> <p>PyTorch seamlessly integrates with NVIDIA GPUs through CUDA. To verify GPU availability in your environment:</p> <pre><code>import torch

# Check GPU availability
gpu_available = torch.cuda.is_available()
print(f"GPU Available: {gpu_available}")

# Get GPU device count if available
if gpu_available:
    print(f"Number of GPUs: {torch.cuda.device_count()}")</code></pre> <p>If a GPU is detected, you can move tensors and models to GPU memory using:</p> <pre><code># Create a tensor
tensor = torch.tensor([1.0, 2.0, 3.0])

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
tensor = tensor.to(device)</code></pre> <h3>Apple Silicon Support</h3> <p>For users with Apple M1/M2/M3 chips, PyTorch provides acceleration through the Metal Performance Shaders (MPS) backend. Verify MPS availability:</p> <pre><code>import torch

# Check MPS (Metal Performance Shaders) availability
mps_available = torch.backends.mps.is_available()
print(f"MPS Available: {mps_available}")

# If MPS is available, you can use it as device
if mps_available:
    device = torch.device("mps")
    # Move tensors/models to MPS device
    tensor = tensor.to(device)</code></pre> <p>For ease of usage, I recommend using <a href="https://colab.research.google.com/" rel="external nofollow noopener" target="_blank">Google Colab</a> i.e. a popular jupyter notebookâ€“like environment, which provides time-limited access to GPUs.</p> <h2>Understanding Tensors</h2> <h3>What Are Tensors?</h3> <p>Tensors are mathematical objects that generalize vectors and matrices to higher dimensions. In PyTorch, tensors serve as fundamental data containers that hold and process multi-dimensional arrays of numerical values. These containers enable efficient computation and automatic differentiation, making them essential for deep learning operations. PyTorch tensors are similar to Numpy arrays in basic sense. </p> <h3>Scalers, Vectors, Matrices and Tensors</h3> <div class="image-gallery-embed" data-attrs='{"gallery":{"images":[{"type":"image/png","src":"https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png"}],"caption":"Source: https://dev.to/mmithrakumar/scalars-vectors-matrices-and-tensors-with-tensorflow-2-0-1f66","alt":"Tensors","staticGalleryImage":{"type":"image/png","src":"https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png"}},"isEditorNode":true}'></div> <p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorchâ€™sÂ `Tensor`Â class using theÂ `torch.tensor`Â function as shown in the following listing.</p> <pre><code>import torch

# Scalar (0-dimensional tensor)
scalar = torch.tensor(1)     

# Vector (1-dimensional tensor)
vector = torch.tensor([1, 2, 3])    

# Matrix (2-dimensional tensor)
matrix = torch.tensor([[1, 2], 
                      [3, 4]])     

# 3-dimensional tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]], 
                        [[5, 6], [7, 8]]])</code></pre> <p>Each tensor type maintains its specific dimensionality, accessible through theÂ <strong>.shape</strong>Â attribute:</p> <pre><code>print(f"Scalar shape: {scalar.shape}")      # torch.Size([])
print(f"Vector shape: {vector.shape}")      # torch.Size([3])
print(f"Matrix shape: {matrix.shape}")      # torch.Size([2, 2])
print(f"3D tensor shape: {tensor3d.shape}") # torch.Size([2, 2, 2])</code></pre> <h3>Tensor Data Types and Precision</h3> <p>PyTorch supports various data types with different precision levels, optimized for different computational needs:</p> <p>Some of the common torch datatypes available with torch are <code>float32</code>, <code>float64</code>, <code>float16</code>, <code>bfloat16</code>, <code>int8</code>, <code>uint8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>.</p> <p>The choice of precision impacts both memory usage and computational efficiency:</p> <ul> <li><p><code>float32</code>: Standard for most deep learning tasks</p></li> <li><p><code>float16</code>: Reduced precision, useful for memory optimization</p></li> <li><p><code>bfloat16</code>: Brain Floating Point, balances precision and range</p></li> </ul> <h3>Floating Data Types</h3> <p>PyTorch supports various floating-point precisions for tensors, each serving different computational needs:</p> <ul> <li><p><code>torch.float32</code> (default): 32-bit precision offering 6-9 decimal places, optimal for most deep learning tasks</p></li> <li><p><code>torch.float64</code>: 64-bit double precision with 15-17 decimal places, suitable for high-precision numerical computations</p></li> <li><p><code>torch.float16</code>: 16-bit half precision with 3-4 decimal places, useful for memory-efficient operations</p></li> <li><p><code>torch.bfloat16</code>: Brain floating point format with 2-3 decimal precision, balancing range and precision</p></li> </ul> <pre><code><code>import torch

float32_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  
float64_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)  
float16_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16)  
bfloat16_tensor = torch.tensor([1.0, 2.0], dtype=torch.bfloat16)</code></code></pre> <h3>Integer Types</h3> <p>PyTorch supports various integer data types, each with specific memory allocations and value ranges:</p> <ul> <li><p><code>int8</code>: 8-bit signed integers (-128 to 127)</p></li> <li><p><code>uint8</code>: 8-bit unsigned integers (0 to 255)</p></li> <li><p><code>int16</code>: 16-bit signed integers (-32768 to 32767)</p></li> <li><p><code>int32</code>: 32-bit signed integers (-2^31 to 2^31-1)</p></li> <li><p><code>int64</code>: 64-bit signed integers (-2^63 to 2^63-1), default integer type in PyTorch</p></li> </ul> <pre><code><code>import torch

int8_tensor = torch.tensor([1, 2], dtype=torch.int8)     
uint8_tensor = torch.tensor([1, 2], dtype=torch.uint8)   
int16_tensor = torch.tensor([1, 2], dtype=torch.int16)   
int32_tensor = torch.tensor([1, 2], dtype=torch.int32)   
int64_tensor = torch.tensor([1, 2], dtype=torch.int64)
</code></code></pre> <h3>Datatype Conversion</h3> <p>We can convert tensors from one datatype to another using the <code>.to</code> method.</p> <pre><code><code># Converting between data types
tensor = torch.tensor([1, 2, 3])
float_tensor = tensor.to(torch.float32) # Convert from int64 to float32
int_tensor = tensor.to(torch.int32)     # Convert from float32 to int32
</code></code></pre> <h3>Common Tensor Operations</h3> <p>PyTorch provides several fundamental tensor operations essential for deep learning computations. Here are the key operations with their implementations and specific use cases.</p> <h4>1. Tensor Creation and Shape Manipulation</h4> <p>Creating tensors and understanding their shape are fundamental operations in PyTorch:</p> <pre><code><code>import torch

# Create 2D tensor
tensor2d = torch.tensor([[1, 2, 3], 
                        [4, 5, 6]])

# Check tensor shape
shape = tensor2d.shape  
# Returns: torch.Size([2, 3])
</code></code></pre> <p>For the above tensor, the shape if 2 x 3 i.e. 2 rows and 3 columns. We can change the shape of the array by maintaining the total size of the array using reshape method.</p> <h4>2. Reshaping Operations</h4> <p>PyTorch offers two methods for tensor reshaping:</p> <pre><code><code># Reshape tensor from (2,3) to (3,2)
reshaped_tensor = tensor2d.reshape(3, 2)

# Alternative using view
viewed_tensor = tensor2d.view(3, 2)
</code></code></pre> <p><strong>Technical Note</strong>: <code>.view()</code> and <code>.reshape()</code> differ in memory handling:</p> <ul> <li><p><code>.view()</code>: Requires contiguous memory layout</p></li> <li><p><code>.reshape()</code>: Works with any memory layout, performs copy if necessary</p></li> </ul> <h4>3. Matrix Operations</h4> <p>PyTorch implements efficient matrix operations essential for linear algebra computations:</p> <pre><code><code># Transpose operation
transposed = tensor2d.T

# Matrix multiplication methods
result1 = tensor2d.matmul(tensor2d.T)  # Using matmul
result2 = tensor2d @ tensor2d.T        # Using @ operator
</code></code></pre> <p>Output shapes for a 2x3 input tensor:</p> <ul> <li><p>Transpose: 3x2</p></li> <li><p>Matrix multiplication with transpose: 2x2</p></li> </ul> <p>These operations form the foundation for neural network computations and linear algebra operations in deep learning models. For an exhaustive list of tensor operations, refer to the <a href="https://pytorch.org/docs/stable/tensors.html" rel="external nofollow noopener" target="_blank">PyTorch documentation</a>.</p> <h2>Automatic Differentiation</h2> <h3>Understanding Computational Graphs</h3> <p>PyTorch builds computational graphs that track operations performed on tensors. These graphs enable automatic differentiation through the <code>autograd</code> system, making gradient computation efficient and programmatic.</p> <p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural networkâ€”we will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p> <p>Consider the following example of a single layer neural network performing logistic regression with single weight and bias.</p> <pre><code><code>import torch
import torch.nn.functional as F

# Initialize inputs and parameters
y = torch.tensor([1.0])           # Target
x1 = torch.tensor([1.1])          # Input
w1 = torch.tensor([2.2], 
                  requires_grad=True)  # Weight
b = torch.tensor([0.0], 
                 requires_grad=True)   # Bias

# Forward pass computation
z = x1 * w1 + b                   # Linear computation
a = torch.sigmoid(z)              # Activation
loss = F.binary_cross_entropy(a, y)    # Loss computation
</code></code></pre> <p>We have used the <code>torch.nn.functional</code> module from <code>torch</code> which provides many utility functions like loss functions, activations etc required to write and train deep neural networks.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!6wqh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" width="1156" height="400" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/a38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":400,"width":1156,"resizeWidth":null,"bytes":26016,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LLMs from Scratch</a></em></p> <h3>Gradient Computation with Autograd</h3> <p>To train the above model, we have to compute the gradients of loss w.r.t <code>w1</code> and <code>b</code> which will be further used to update the existing weights iteratively. This is where PyTorch makes our life easier by automatically calculating them using the <code>autograd</code> engine.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!D2gj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" width="1120" height="726" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":726,"width":1120,"resizeWidth":null,"bytes":68642,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LLMs from Scratch</a></em></p> <p>PyTorch's autograd system automatically computes gradients for all tensors with <code>requires_grad=True</code>. Here's how to compute gradients:</p> <pre><code><code>from torch.autograd import grad

# Manual gradient computation
grad_L_w1 = grad(loss, w1, retain_graph=True)
grad_L_b = grad(loss, b, retain_graph=True)

# Alternative using backward()
loss.backward()
print(w1.grad)    # Access gradient for w1
print(b.grad)     # Access gradient for b
</code></code></pre> <p><strong>Technical Note</strong>: When using <code>backward()</code>:</p> <ul> <li><p>Gradients accumulate by default</p></li> <li><p>Use <code>zero_grad()</code> before each backward pass in training loops</p></li> <li><p><code>retain_graph=True</code> allows multiple backward passes</p></li> </ul> <p>The <code>grad</code> function is used to get gradients manually and it is useful for debugging and demonstration purposes. Using the <code>backward()</code> function automatically calculates for all the tensors which has <code>requires_grad=True</code> set and gradients will be stored inside <code>.grad</code> property.</p> <h2>Building Neural Networks with PyTorch</h2> <p>Next, we focus on PyTorch as a library for implementing deep neural networks. While our previous example demonstrated a single neuron for classification, practical applications require complex architectures like transformers and ResNets that process multiple inputs through various hidden layers to produce outputs. Manually calculating and updating individual weights becomes impractical at this scale. PyTorch provides a structured approach through its neural network modules, enabling efficient implementation of sophisticated architectures.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!AV-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" width="860" height="760" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":760,"width":860,"resizeWidth":null,"bytes":90385,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LLMs from Scratch</a></em></p> <h3>Introduction to torch.nn.Module</h3> <p>The <code>torch.nn.Module</code> serves as PyTorch's foundational class for neural networks, providing a systematic way to define and manage model architectures, parameters, and computations. This base class handles essential functionalities including parameter management, device placement, and training behaviors.</p> <p>The subclass has the following components:</p> <ul> <li><p><code>__init__</code>: We define the layers of neural networks in the constructor of the subclass defined and how the layers interact during forward propagation.</p></li> <li><p><code>forward</code>: The forward method describes how the input data passes through the network and comes together as a computation graph.</p></li> </ul> <h3>Creating Custom Neural Network Architectures</h3> <p>Complex neural networks require multiple layers with specific activation functions. Here's a practical implementation of a multi-layer neural network:</p> <pre><code><code>class DeepNetwork(nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(num_inputs, 30),     # First hidden layer
            nn.ReLU(),                     # Activation function
            nn.Linear(30, 20),             # Second hidden layer
            nn.ReLU(),                     
            nn.Linear(20, num_outputs)      # Output layer
        )

    def forward(self, x):
        return self.layers(x)
</code></code></pre> <p><code>nn.Sequential</code> provides a container for stacking layers in a specific order, streamlining the forward pass implementation.</p> <h3>Model Parameters and Initialization</h3> <p>PyTorch automatically handles parameter initialization, but you can access and modify parameters:</p> <pre><code><code>model = DeepNetwork(50, 3)

# Count trainable parameters
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Trainable parameters: {num_params}")

# Access layer parameters
for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()}")

# Custom initialization
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

model.apply(init_weights)</code></code></pre> <p>Each parameter for which <code>requires_grad=True</code> counts as a trainable parameter and will be updated during training. In the above code, this referes to the weights initialized in <code>torch.nn.Linear</code> layers.</p> <h3>Forward Propagation Implementation</h3> <p>Forward propagation defines how input data flows through the network. Let's initialise random values and pass it through the model.</p> <pre><code><code># Sample forward pass
model = DeepNetwork(50, 3)
batch_size = 32
input_features = torch.randn(batch_size, 50)

with torch.no_grad():
    outputs = model(input_features)

print(f"Output shape: {outputs.shape}")</code></code></pre> <h3>Training Mode vs. Evaluation Mode</h3> <p>PyTorch models have distinct training and evaluation modes that affect certain layers' behavior:</p> <pre><code><code>model = DeepNetwork(50, 3)

# Training mode
model.train()
training_output = model(input_features)  # Layers like Dropout and BatchNorm active

print(training_output)

# Evaluation mode
model.eval()
with torch.no_grad():
    eval_output = model(input_features)  # Deterministic behavior
    print(eval_output)</code></code></pre> <p>PyTorch models operate in two distinct modes:</p> <ol> <li> <p>Training Mode (<code>model.train()</code>):</p> <ul> <li><p>Activates Dropout and BatchNorm layers</p></li> <li><p>Enables gradient computation and tracking</p></li> <li><p>Maintains computational graph for backpropagation</p></li> </ul> </li> <li> <p>Evaluation Mode (<code>model.eval()</code> with <code>torch.no_grad()</code>):</p> <ul> <li><p>Disables Dropout and freezes BatchNorm statistics</p></li> <li><p>Prevents gradient computation and tracking</p></li> <li><p>Optimizes memory usage by eliminating gradient storage</p></li> <li><p>Reduces computational overhead during inference</p></li> </ul> </li> </ol> <p>This mode management ensures efficient resource utilization while maintaining appropriate model behavior for both training and inference phases.</p> <h2>Efficient Data Handling</h2> <p>Efficient data handling is crucial for developing robust deep learning models. PyTorch provides two primary tools for data management: the <code>Dataset</code> and <code>DataLoader</code> classes.</p> <h3>1. Dataset and DataLoader Overview</h3> <p>PyTorch's data handling framework consists of</p> <ul> <li><p><code>Dataset</code>: Defines data access and preprocessing</p></li> <li><p><code>DataLoader</code>: Handles batch creation, shuffling, and parallel loading</p></li> </ul> <p>Let's implement a simple classification dataset to demonstrate these concepts:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

# Training classification data
X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])

# Testing dataset
X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])
</code></code></pre> <h3>2. Creating Custom Dataset Objects</h3> <p>Next, we create a custom dataset class, <code>SampleDataset</code>, by subclassing from PyTorchâ€™s <code>Dataset</code> parent class. It has following properties:</p> <ul> <li><p><code>__init__</code>: Initialize dataset attributes.</p></li> <li><p><code>__getitem__</code>: Define data access for individual samples</p></li> <li><p><code>__len__</code>: Return total number of samples</p></li> </ul> <pre><code><code>from torch.utils.data import Dataset

class SampleDataset(Dataset):
    def __init__(self, X, y):
    """Initialize the dataset with features and labels"""
        self.features = X
        self.labels = y

    def __getitem__(self, index):
    """Retrieve a single example and its label"""     
        one_x = self.features[index]     
        one_y = self.labels[index]       
        return one_x, one_y              

    def __len__(self):
    """Get the total number of examples in the dataset"""
        return self.labels.shape[0]     

train_ds = SampleDataset(X_train, y_train)
test_ds = SampleDataset(X_test, y_test)
</code></code></pre> <h3>3. Implementing DataLoader</h3> <p>DataLoaders handle the heavy lifting of batching, shuffling, and parallel data loading. Now we can create <code>DataLoaders</code> from the <code>SampleDataset</code> object created. This can be done as follows:</p> <pre><code><code># Create DataLoader with specific configurations
train_loader = DataLoader(
    dataset=train_ds,     # Dataset Instance
    batch_size=2,         # Number of samples per batch
    shuffle=True,         # Shuffle the training data
    num_workers=0         # Number of parallel workers
    drop_last=True.       # Drop incomplete batch
)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,        # No need to shuffle test data
    num_workers=0
)</code></code></pre> <p>Some key parameters of Dataloaders class are as follows:</p> <ul> <li><p><code>dataset</code>: The Dataset instance to load data from</p></li> <li><p><code>batch_size</code>: Number of samples per batch</p></li> <li><p><code>shuffle</code>: Whether to shuffle data between epochs</p></li> <li><p><code>num_workers</code>: Number of subprocesses for data loading</p></li> <li><p><code>drop_last</code>: Whether to drop the last incomplete batch</p></li> <li><p><code>pin_memory</code>: Pin memory for faster data transfer to GPU</p></li> </ul> <h3>Complete Example with Best Practices (Best Practice)</h3> <p>Here's a comprehensive implementation incorporating all concepts:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class SampleDataset(Dataset):
    def __init__(self, X, y, transform=None):
        self.features = X
        self.labels = y
        self.transform = transform # Input transformations if required
    
    def __getitem__(self, index):
        x = self.features[index]
        y = self.labels[index]
        
        if self.transform:
            x = self.transform(x)
            
        return x, y
    
    def __len__(self):
        return len(self.labels)

# Configuration for optimal performance
def create_data_loader(dataset, batch_size, is_training=True):
    return DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=is_training,
        num_workers=4 if is_training else 2,
        pin_memory=torch.cuda.is_available(),
        drop_last=is_training,
        persistent_workers=True
    )

# Usage example
if __name__ == "__main__":
    # Create dataset
    dataset = SampleDataset(X_train, y_train)
    
    # Create data loader
    train_loader = create_data_loader(
        dataset=dataset,
        batch_size=32,
        is_training=True
    )
    
    # Training loop example
    for epoch in range(num_epochs):
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Training operations here
            pass
</code></code></pre> <p>This implementation provides a robust foundation for handling data in PyTorch, incorporating best practices for memory management and parallel processing. Adjust the configurations based on your specific use case and available computational resources.</p> <h2>Implementing Training Loops in PyTorch</h2> <p>A PyTorch training loop consists of several key components:</p> <ul> <li><p>model initialization,</p></li> <li><p>optimizer configuration,</p></li> <li><p>loss function definition, and</p></li> <li><p>the iterative training process.</p></li> </ul> <p>Here's a structured implementation showcasing these elements.</p> <h3>Basic Training Loop Structure (Best Practice)</h3> <pre><code><code>import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    num_epochs: int,
    learning_rate: float,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
) -&gt; dict:
    
    # Initialize optimizer and loss function
    optimizer = Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    # Move model to device
    model = model.to(device)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_accuracy': []
    }
    
    # Training loop
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Move data to device
            features = features.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Accumulate loss
            train_loss += loss.item()
            
            # Optional: Print batch progress
            if batch_idx % 100 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs} | '
                      f'Batch: {batch_idx}/{len(train_loader)} | '
                      f'Loss: {loss.item():.4f}')
        
        # Calculate average training loss
        train_loss = train_loss / len(train_loader)
        history['train_loss'].append(train_loss)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(device)
                labels = labels.to(device)
                
                # Forward pass
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                # Accumulate validation metrics
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        # Calculate validation metrics
        val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * correct / total
        
        # Store validation metrics
        history['val_loss'].append(val_loss)
        history['val_accuracy'].append(val_accuracy)
        
        # Print epoch summary
        print(f'Epoch: {epoch+1}/{num_epochs} | '
              f'Train Loss: {train_loss:.4f} | '
              f'Val Loss: {val_loss:.4f} | '
              f'Val Accuracy: {val_accuracy:.2f}%')
    
    return history

# Example Usage
def main():
    # Assume we have model and data loaders defined
    model = DeepNetwork()
    
    # Training configuration
    config = {
        'num_epochs': 10,
        'learning_rate': 0.001,
    }
    
    # Train model
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=test_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
</code></code></pre> <p>The training loop does the following gradient descent as following:</p> <ul> <li><p>The training process involves passing logits to the <code>cross_entropy</code> loss function, which internally applies <code>softmax</code> for optimized performance and numerical stability.</p></li> <li><p>The <code>loss.backward()</code> call computes gradients through PyTorch's computational graph</p></li> <li><p>The <code>optimizer.step()</code> step updates the model parameters using these gradients</p></li> <li><p>The <code>optimizer.zero_grad()</code> must be called every training iteration to reset gradients, preventing unintended accumulation that could distort the optimization process.</p></li> </ul> <h2>Model Persistence in PyTorch: Saving and Loading</h2> <p>PyTorch provides efficient mechanisms for model persistence through its state dictionary system. The state dictionary (<code>state_dict</code>) maintains a mapping between layer identifiers and their corresponding parameters (weights and biases).</p> <h3>Basic Model Persistence</h3> <h4>Saving Models</h4> <p>After training the model, it is necessary to save the model weights to reuse later for further training or deployment. Save a model's learned parameters using the state dictionary:</p> <pre><code><code>import torch

# Save model parameters
torch.save(model.state_dict(), "model_parameters.pth")</code></code></pre> <h3>Loading Models</h3> <p>The <code>torch.load("model_parameters.pth")</code> function reads the file <code>"model_parameters.pth"</code> and reconstructs the Python dictionary object containing the modelâ€™s parameters while <code>model.load_state_dict()</code> applies these parameters to the model, effectively restoring its learned state from when we saved it.</p> <p>We need the instance of the model in memory to apply the saved parameters. Here, the <code>NeuralNetwork(2,</code> <code>2)</code> architecture needs to match the original saved model exactly.</p> <pre><code><code># Initialize model architecture
model = NeuralNetwork(num_inputs=2, num_outputs=2)

# Load saved parameters
model.load_state_dict(torch.load("model_parameters.pth"))
</code></code></pre> <h2>Comprehensive Model Persistence (Best Practice)</h2> <p>For production scenarios, save additional information alongside model parameters:</p> <pre><code><code># Save complete model state
checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
    'model_config': {
        'num_inputs': 2,
        'num_outputs': 2
    }
}
torch.save(checkpoint, "model_checkpoint.pth")

# Load complete model state
checkpoint = torch.load("model_checkpoint.pth")
model = NeuralNetwork(**checkpoint['model_config'])
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
</code></code></pre> <h2>Conclusion</h2> <p>PyTorch's architecture provides a robust foundation for deep learning development through its integrated components: tensor computations, automatic differentiation, and neural network modules. The framework's design enables efficient model implementation through dynamic computation graphs, GPU acceleration, and intuitive APIs for data processing and model construction.</p> <p>For continued learning and implementation guidance, refer to PyTorch's official documentation which provides comprehensive updates on best practices, optimizations, and emerging capabilities. This ensures your deep learning applications remain aligned with current framework standards and performance benchmarks.</p> <div><hr></div> <p>Thanks for reading NeuraForge: AI Unleashed! </p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. ðŸ§ âœ¨</p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p></p> </body></html>