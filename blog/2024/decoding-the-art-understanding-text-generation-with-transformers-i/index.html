<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction </h2> <p>One of the most remarkable abilities of transformer models is their capacity to generate text that closely mimics human writing. With extensive training, large language models (LLMs) like GPT-3 can develop a wide range of skills for understanding and recognizing patterns in the text they've been trained on. These skills can be harnessed through various input prompts. In this blog, let’s delve into different text generation techniques using transformer-based models and examine how various hyperparameters play a crucial role in shaping the LLM's response to a given prompt.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!W73Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!W73Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 424w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 848w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1272w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic" width="698" height="698" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/c91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":1024,"width":1024,"resizeWidth":698,"bytes":161002,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!W73Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 424w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 848w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1272w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">AI-Generated Image: Robot Writing</figcaption></figure></div> <h2><strong>Understanding Text Generation in Transformers</strong></h2> <p>The decoder component of a transformer model primarily facilitates text generation. The decoder iteratively produces tokens based on the input prompt until they reach the specified length. This process, known as <strong>conditional text generation</strong>, depends on the input prompt to shape the output sequence.</p> <p>The decoding method is central to this mechanism, which determines the selection of tokens from the vocabulary at each timestep. Various decoding techniques, each with its own set of hyperparameters, can be employed to achieve the desired output for a given prompt. In this blog, let’s explore some of these methods and their impact on text generation. </p> <h2>Exploring Text Generation Techniques</h2> <h3><strong>1. Greedy Search Decoding</strong></h3> <p>Greedy search decoding is the most basic method used in text generation. This approach is characterized by its simplicity and speed, as it makes decisions based solely on the immediate next token without considering the overall sequence.</p> <h4><strong>How Greedy Search Works</strong></h4> <ul> <li><p>It selects the token with the highest probability, or logits, at each timestep.</p></li> <li><p>This approach is straightforward and can quickly generate text.</p></li> </ul> <h4><strong>Implementation</strong></h4> <p>Greedy Search can be implemented using the transformers library by setting the <code>do_sample</code> parameter to <code>False</code> in the <code>generate()</code> function. We can also specify the maximum number of tokens generated by setting the <code>max_new_tokens</code> parameter.</p> <p>Below is an example of how to implement greedy search decoding with the <strong>GPT-2 XL</strong> model using the transformers library:</p> <pre><code><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup the model and tokenizer
checkpoint = "gpt2-xl"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

# Encode the inputs
input_text = "Once upon a time, there was a"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Generate output using Greedy Search
output = model.generate(input_ids, max_new_tokens=70, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man named John. John was a man of God. He was a man of God who loved his family and his friends. He was a man of God who loved his country and his God. He was a man of God who loved his God and his country. He was a man of God who loved his God and his country. He was a
</code></pre> <p>Analyzing the output generated by the model using greedy search decoding, we notice that while the text was successfully completed, the model repetitively produced the same sentence until reaching the maximum token limit. </p> <h4><strong>Limitations</strong></h4> <ul> <li><p>Greedy search often leads to repetitive sequences, underscoring its limitations.</p></li> <li><p>This limitation is particularly pronounced when diversity is desired in text generation.</p></li> <li><p>The issue arises from the method's emphasis on choosing the highest probability word at each step.</p></li> <li><p>As a result, it overlooks sequences that might have a higher overall probability.</p></li> </ul> <h4><strong>Use Cases</strong></h4> <p>Although the greedy search has shortcomings, it can be effective for tasks that demand <strong>deterministic and factually accurate outputs, such as arithmetic problems</strong>. Nonetheless, it is less frequently employed for tasks requiring varied or imaginative text.</p> <h3>2. Beam Search Decoding</h3> <p>Beam search introduces a refined approach to token selection in text generation. Beam search is an algorithm used to improve the quality of text generation in models like those based on transformer architecture. It balances the brute-force exactness of exhaustive search and the practical efficiency of greedy search.</p> <h4><strong>How Beam Search Works</strong></h4> <ul> <li><p>Rather than choosing the highest-probability token at each step, this method maintains the top-b most probable next tokens, where <code>b</code> represents the<em> <strong>beam width</strong></em><strong> </strong>or <em><strong>beam size</strong></em>.</p></li> <li><p>It involves evaluating all potential extensions of the current set and selecting the <code>b</code> most promising ones. This process is repeated until the maximum number of tokens is reached, ensuring a more balanced search space exploration.</p></li> </ul> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Q8xB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 424w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 848w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1272w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic" width="1456" height="876" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":876,"width":1456,"resizeWidth":null,"bytes":54906,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 424w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 848w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1272w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: <a href="https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/beam_2.jpg" rel="external nofollow noopener" target="_blank">HuggingFace</a> - Beam Search Step Visualization</figcaption></figure></div> <h4><strong>Implementation</strong></h4> <p>Beam Search can be activated with the <code>generate() </code>function by specifying the <code>num_beams </code>parameter. The more beams we choose, the better the result potentially gets. </p> <p>Below is an example of how to implement beam search decoding with the <strong>GPT-2 XL</strong> model using the transformers library:</p> <p><strong>Case I</strong>: <strong>With </strong><code>num_beams=5</code></p> <pre><code><code># Beam Search Decoding with num_beams=5
output = model.generate(input_ids, max_new_tokens=70, num_beams=5, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who lived in a small town. He had a wife and two children. One day, he went to the store to buy some milk. When he returned home, he found that his wife and children were gone. He searched for them, but they were nowhere to be found. He went to the police station to report the disappearance of his family
</code></pre> <p>Analyzing the output from beam search decoding reveals that it generated more coherent and clear text. Let’s try another example by increasing the <code>num_beams</code> parameter to 10. </p> <p><strong>Case II</strong>: <strong>With </strong><code>num_beams=10</code></p> <pre><code># Beam Search Decoding with num_beams=10

output = model.generate(input_ids, max_new_tokens=70, num_beams=10, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who had a dream.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.</code></pre> <p>From the second output, we can see that beam search also suffers from repetitive text. One way to address this is to impose an <em>n</em>-gram penalty with the <code>no_repeat_ngram_size</code> parameter that tracks which <em>n</em>-grams have been seen and sets the next token probability to zero if it would produce a previously seen <em>n</em>-gram, thereby reducing repetitions: </p> <p><strong>Case I</strong>: <strong>With </strong><code>num_beams=5 </code><strong>and</strong><code> no_repeat_ngram_size=3</code></p> <pre><code># Beam Search Decoding with num_beams=10 and no_repeat_ngram_size=3

output = model.generate(input_ids, max_new_tokens=70, num_beams=10, do_sample=False, no_repeat_ngram_size=3)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who lived in a small town. He had a wife, a son, and a daughter. One day, the man went to visit his son in the hospital. When he got there, he found out that his son had died of a heart attack. The man was devastated. He didn't know what to do, so he went to his</code></pre> <p>Well, from this output, we can observe the generated text is more coherent and clear than the previous output, with <code>num_beams=10. </code></p> <h4><strong>Limitations</strong></h4> <ul> <li><p>Beam Search suffers from issues like repetitive token generation and bias towards shorter sequences.</p></li> <li><p>The beam search algorithm requires tracking multiple beams at each step, which increases memory usage and computational complexity compared to greedy search. </p></li> </ul> <h4><strong>Use Cases</strong></h4> <p>Beam Search with <em>n</em>-gram penalty is commonly used in applications like <strong>summarization</strong> and <strong>machine translation</strong> where factual correctness is emphasised over diversity. Now, let’s explore the text generation technique where we can make the generated text more creative and diverse.</p> <h3>3. Random Sampling</h3> <p>Random sampling is a strategy employed in text generation, particularly in scenarios where diversity is prioritized over factual accuracy, such as story generation or open-domain conversations. Unlike greedy search, which consistently opts for the token with the highest probability, this method introduces an element of randomness to the selection process.</p> <h4><strong>How Random Sampling Works</strong></h4> <ul> <li><p>The model produces a probability distribution for the next token over the entire vocabulary based on the input prompt and the tokens generated.</p></li> <li><p>Instead of choosing the most probable token, a token is randomly selected from this probability distribution, allowing for a broader exploration of possible sequences. </p></li> <li><p>The degree of randomness in sampling can be adjusted using a parameter known as <code>temperature</code>. Higher <code>temperatures</code> lead to a more uniform distribution, enhancing randomness and diversity. Conversely, a lower <code>temperature</code> produces a sharper distribution, making the output more similar to the highest probability sequence.</p></li> </ul> <h4><strong>Implementation</strong></h4> <p>Random Sampling decoding technique can be activated easily in the transformer’s <code>generate()</code> function by setting the <code>do_sample</code> parameter to <code>True</code> and specify the <code>temperature</code> to control the creativity and diversity of the created text.</p> <p>Below is an example of how to implement random sampling with the <strong>GPT-2 XL</strong> model using the transformers library:</p> <p><strong>Case I</strong>: <strong>When </strong><code>temperature</code><strong> is high (&gt;1.0) </strong></p> <pre><code><code># Define the input prompt
input_text = "Once upon a time, there was a man who"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Perform Random Sampling to generate 70 tokens
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=2.0)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who liked to cook. Every so often this man visited one part of Egypt more than three different times every fifteen years… until finally they saw him leave Egypt entirely forever, his back into the north… he set himself off somewhere. And on this very lonely place would he spend fifty five years, every generation…

How strange the sound those sixty-</code></pre> <p>In the above output, we can observe that the generated text is very diverse in nature, without any formal plot in the story. This is due to setting the <code>temperature</code> parameter as 2 (i.e. high value)</p> <p><strong>Case II</strong>: <strong>When </strong><code>temperature</code><strong> is low (&lt;1.0) </strong></p> <pre><code># Random Sampling with temperature=0.7
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.7)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre> <p><strong>Code Output:</strong></p> <pre><code>Once upon a time, there was a man who made a name for himself in the world. He was handsome, charming, and popular with the ladies, but his wife was not satisfied with his success. She wanted more. She thought she knew what needed to be done, and she was determined to make him do it.</code></pre> <p>Upon reviewing the output above, we can get the gist of the generated text and ensure that it maintains a reasonable level of diversity. </p> <h4><strong>Limitations</strong></h4> <ul> <li><p>The randomness can sometimes result in nonsensical or off-topic text.</p></li> <li><p>The overall quality and coherence of the text might be lower compared to more deterministic methods.</p></li> <li><p>It cannot be used for tasks where factual correctness and deterministic outputs are expected.</p></li> </ul> <h4><strong>Use Cases</strong></h4> <p>Random sampling can be particularly beneficial in creative writing, where text diversity is essential. By introducing an element of unpredictability, it can enhance the narrative's richness and originality. Additionally, random sampling can generate varied responses in text conversations and chatbots. This variability can contribute to a more human-like feel, making users' interactions with these systems more engaging and natural.</p> <h2>Conclusion</h2> <p>Various techniques, such as top-k sampling and nucleus sampling, can enhance the coherence and creativity of the text. The next blog will discuss these techniques in more detail. The notebook for the code provided above can be accessed at this <a href="https://www.kaggle.com/code/narasimhajwalapuram/text-generation-with-transformers/notebook?scriptVersionId=168118058" rel="external nofollow noopener" target="_blank">link</a>.</p> <p>Happy Learning!</p> <div><hr></div> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! 🚀🤖</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your email…" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </body></html>