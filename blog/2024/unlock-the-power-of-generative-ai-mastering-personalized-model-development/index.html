<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>The impact of the <strong>Generative Pre-trained Transformers</strong> (GPT) series by OpenAI in the revolution of large language models has been undeniable. In this blog, we delve into fine-tuning a variant of the GPT-2, <strong>GPT-2 small</strong> variant using Hugging Face's Transformers library, a process crucial for tailoring these models to specific language tasks.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!UTP4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UTP4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 424w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 848w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1272w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic" width="1024" height="1024" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":1024,"width":1024,"resizeWidth":null,"bytes":346225,"alt":null,"title":null,"type":"image/heic","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UTP4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 424w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 848w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1272w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Image Generated by Dall-E</figcaption></figure></div> <h2>Decoding GPT-2 &amp; Transformers: The Power Duo in Language Modelling</h2> <p>The GPT-2 model is a prominent example of a decoder-based language model developed by OpenAI. Transformer models, like GPT-2, are distinguished by their ability to process words in relation to all other words in a sentence. This contrasts with earlier models that processed text in sequential order.</p> <p>Hugging Face's <strong>Transformers</strong> library is a comprehensive suite that simplifies using thousands of pre-trained models for various natural language processing tasks. It bridges these complex models and developers, providing tools for easy implementation, customization, and deployment of many open-source AI models, including GPT-2. This library is widely recognized for its user-friendly interface and extensive documentation, making cutting-edge AI accessible to novice and expert practitioners. The combination of GPT-2’s advanced capabilities and the Transformers library’s ease of use offers an unparalleled toolkit for developing sophisticated language processing applications. </p> <h2>Language Modelling Deep Dive</h2> <p>In this section, let’s dive deep into different stages of fine-tuning the GPT-2 model with a custom dataset. </p> <h3>Setting Up the Development Environment</h3> <p>Begin by preparing your development environment, a vital step for success in your project. Start with installing <strong>Python</strong>, then install Hugging Face's Transformers library, which is essential for this endeavour. Use Python's package installer, pip, for a smooth installation:</p> <pre><code>!pip install transformers
!pip install datasets
!pip install accelerate -U
!pip install apache-beam</code></pre> <p>Additionally, consider leveraging Google Colab for your project. <strong>Google Colab</strong> provides a cloud-based platform that offers free access to powerful GPUs, which can significantly accelerate the training and fine-tuning process of models like GPT-2. This makes it an ideal choice, especially if you have limited resources or require advanced computational capabilities. With these tools and platforms at your disposal, you are now equipped to dive into the fine-tuning of the GPT-2 language model. Google Colab can be accessed through this <a href="https://colab.research.google.com/" rel="external nofollow noopener" target="_blank">link</a>. </p> <h3><strong>Delving into the Wikipedia Dataset</strong></h3> <p>For fine-tuning GPT-2, the <strong><a href="https://huggingface.co/datasets/wikipedia" rel="external nofollow noopener" target="_blank">Wikipedia dataset</a></strong> from Hugging Face offers a comprehensive collection of articles, ideal for training language models. This dataset provides a diverse range of topics and writing styles, making it perfect for enhancing the linguistic understanding and generative capabilities of GPT-2. Load the dataset from the <strong>datasets</strong> library: </p> <pre><code>from datasets import load_dataset

dataset = load_dataset("wikipedia", "20220301.en")</code></pre> <p>We can view the contents of the dataset by printing the structure. </p> <pre><code>print(dataset)</code></pre> <pre><code>Output:

DatasetDict({
    train: Dataset({
        features: ['id', 'url', 'title', 'text'],
        num_rows: 6458670
    })
})</code></pre> <p>Given the vast dataset size, which includes over <strong>6 million rows</strong>, we'll focus on a subset for this tutorial. Training on the entire dataset would demand extensive computational resources.</p> <pre><code># Choose a data subset
data_subset = dataset['train'].select(range(1000))
print(data_subset)

# Split the dataset into training and validation sets
split = data_subset.train_test_split(test_size=0.1)
train_dataset = split['train']
val_dataset = split['test']</code></pre> <h3>Loading the GPT-2 Model &amp; Tokenizer</h3> <p>For this task, let’s use the <strong>small </strong>variant of the GPT-2 model. Load the GPT-2 model and its tokenizer from Hugging Face's <strong>Transformers</strong> library: </p> <pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")</code></pre> <p>Now, let’s process the Wikipedia dataset to prepare it for model training. </p> <h3>Data preprocessing: Tokenization </h3> <p>Before diving into the fine-tuning process, preparing the Wikipedia dataset appropriately is essential. This preparation involves tokenizing the text data and converting it into a format the GPT-2 model can understand and process efficiently. We use the <code>GPT2Tokenizer</code> loaded earlier to tokenize the text. This process breaks down the text into tokens or smaller pieces, aligning with the model's linguistic understanding. Once tokenized, the dataset is ready for model training, ensuring that the input data aligns with the internal workings of GPT-2. This step is crucial in ensuring effective training and fine-tuning of the model on the dataset.</p> <p>Let’s split the dataset before proceeding with model training. </p> <pre><code># Function to tokenize the text
def tokenize_function(inputs):
    return tokenizer(inputs['text'],
                    padding="max_length",
                    max_length=512,
                    truncation=True,
                    return_overflowing_tokens=True,
                    return_length=True)

# Apply the tokenization to the dataset splits
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_valid = val_dataset.map(tokenize_function, batched=True)</code></pre> <h3>Fine-tuning of GPT-2 with Wikipedia Dataset: Model training </h3> <p>In the fine-tuning process, the <strong>TrainingArguments</strong> and the <strong>Trainer</strong> classes from the transformer library play pivotal roles. We can implement these classes in code as follows:</p> <pre><code># Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    evaluation_strategy="epoch",
    save_steps=10_000,
    save_total_limit=2,
    report_to=None
)

# Custom function to compute perplexity
def compute_perplexity(eval_pred):
    logits, labels = eval_pred
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    loss_fct = torch.nn.CrossEntropyLoss()
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    return {"perplexity": torch.exp(loss)}</code></pre> <p>The more detailed explanation is provided below,</p> <ul> <li> <p><strong>TrainingArguments: </strong>We define various parameters for training our model. Some of the import parameters of this class are:</p> <ul> <li><p><strong>Output Directory (</strong><code>output_dir</code>): Specifies where to save the model.</p></li> <li><p><strong>Number of Epochs (</strong><code>num_train_epochs</code>): Sets how many times the model will see the entire dataset.</p></li> <li><p><strong>Batch Sizes (</strong><code>per_device_train_batch_size</code>, <code>per_device_eval_batch_size</code>): Determines the number of samples processed before the model is updated.</p></li> <li><p><strong>Evaluation and Save Steps (</strong><code>eval_steps</code>, <code>save_steps</code>): Defines frequency of evaluation and model saving.</p></li> <li><p><strong>Warmup Steps (</strong><code>warmup_steps</code>): Adjusts learning rate in the initial training phase.</p></li> <li><p><strong>Logging Directory (</strong><code>logging_dir</code>): Location for storing training logs.</p></li> </ul> <p></p> </li> <li> <p><strong>Trainer</strong>: The <code>Trainer</code> is a powerful class in the Transformers library that abstracts much of the training loop. Some essential parameters passed as inputs to this model are: </p> <ul> <li><p><strong>Model (</strong><code>model</code>): The pre-trained GPT-2 model to be fine-tuned.</p></li> <li><p><strong>Training Arguments (</strong><code>args</code>): The <strong>TrainingArguments</strong> instance as detailed above.</p></li> <li><p><strong>Training Dataset (</strong><code>train_dataset</code>): The subset of the Wikipedia dataset for training.</p></li> <li><p><strong>Evaluation Dataset (</strong><code>eval_dataset</code>): The subset of the Wikipedia dataset for evaluation.</p></li> <li><p><strong>Metrics Function (</strong><code>compute_perplexity</code>): Function to evaluate model performance with perplexity metric.</p></li> </ul> </li> </ul> <p>This structured approach in configuring the <code>TrainingArguments</code> and <code>Trainer</code> ensures an optimized environment for effectively fine-tuning the GPT-2 model on the chosen dataset.</p> <pre><code># Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_valid,
    compute_metrics=compute_perplexity,
)

# Training the model
trainer.train()</code></pre> <h3>Save the Model</h3> <p>Once the model is trained, we can save the model for performing further fine-tuning or inference with data. </p> <pre><code># Save the model
model.save_pretrained("./gpt2-medium-finetuned")</code></pre> <h2>Conclusion</h2> <p>In this blog, we explored how to prepare and tokenize the dataset for Causal Language Modeling tasks and perform fine-tuning on the GPT-2 pre-trained model using the HuggingFace Transformers library. Using the above code and method, we can fine-tune any model and dataset of our choice, just by replacing the model checkpoint and the dataset source. Just ensure you have enough computing available in terms of GPUs and memory.</p> <p>Thank you for reading!</p> <div><hr></div> <div class="subscription-widget-wrap-editor" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe","language":"en"}' data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"> <div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! 🚀🤖</p></div> <form class="subscription-widget-subscribe"> <input type="email" class="email-input" name="email" placeholder="Type your email…" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"> <div class="fake-input"></div> <div class="fake-button"></div> </div> </form> </div></div> </body></html>