<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>Attention mechanisms have revolutionized the field of natural language processing (NLP) in recent years, enabling models to effectively capture long-range dependencies and achieve state-of-the-art performance on a wide range of tasks. At the heart of modern language models like the Transformer and GPT series lies the self-attention mechanism, a powerful tool for relating different positions within an input sequence.</p> <p>In this blog post, we'll explore the inner workings of attention, starting from the limitations of traditional approaches and building up to the efficient multi-head attention used in today's cutting-edge models. You can follow this blog post along with this <a href="https://colab.research.google.com/drive/1fi1YFBixhPjConeVDogSxcwiPSSrvkZX?usp=sharing" rel="external nofollow noopener" target="_blank">Colab notebook</a>.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3956" height="2220" data-attrs='{"src":"https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":2220,"width":3956,"resizeWidth":null,"bytes":null,"alt":"cable network","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="cable network" title="cable network" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">Taylor Vick</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>Recap: The Problem with Modeling Long Sequences</h2> <p>Recurrent Neural Networks (RNNs) and encoder-decoder models have been widely used for processing sequential data in natural language processing tasks. However, these architectures face several challenges when dealing with long sequences:</p> <ul> <li><p><strong>Information Bottleneck Problem</strong>: RNNs and encoder-decoder models compress the entire input sequence into a fixed-size hidden state vector. As the sequence length grows, it becomes increasingly difficult to pack all the necessary information into this fixed-size representation. Important details from earlier parts of the sequence can be lost or overwritten as the hidden state is updated at each step, making it challenging to capture long-range dependencies.</p></li> <li> <p><strong>Vanishing or Exploding Gradient Problem</strong>: During training, as the gradient signal is backpropagated through time, it can either decay exponentially (vanishing) or grow exponentially (exploding).</p> <ul> <li><p>Vanishing gradients make it difficult for the model to learn long-range dependencies, as the gradient signal becomes too weak to effectively update earlier parts of the network.</p></li> <li><p>Exploding gradients can cause the model to become unstable and diverge during training.</p></li> </ul> </li> </ul> <div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!25PW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" width="375" height="134" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/b74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":134,"width":375,"resizeWidth":null,"bytes":3488,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":false,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"></picture><div></div> </div></a><figcaption class="image-caption">Source: https://github.com/sooftware/seq2seq</figcaption></figure></div> <p>In encoder-decoder models, the decoder has limited access to the context from the input sequence. The fixed-size hidden state from the encoder is the only information available to the decoder at each generation step, which can be insufficient for capturing all the relevant context. This is particularly problematic for tasks like machine translation, where understanding the entire source sequence is crucial for generating accurate translations.</p> <p>Furthermore, RNNs and encoder-decoder models process the input sequence sequentially, one token at a time. This sequential processing can be time-consuming, especially for long sequences, as the computation cannot be easily parallelized. Each hidden state update depends on the previous hidden state, creating an inherent sequential dependency that limits the ability to take advantage of modern hardware like GPUs that excel at parallel processing.</p> <p>These limitations motivated the development of attention mechanisms, which allow the model to selectively focus on different parts of the input sequence during processing. By enabling the model to access and utilize relevant information from the entire sequence, attention mechanisms can effectively capture long-range dependencies and overcome the limitations of fixed-size hidden state representations.</p> <h2>Capturing Dependencies with Attention Mechanisms</h2> <p>To address the limitations of RNNs and encoder-decoder models, researchers introduced attention mechanisms. Attention allows the model to selectively focus on different parts of the input sequence during processing, enabling it to capture long-range dependencies more effectively.</p> <p>One of the first attention mechanisms proposed was the <em><strong>Bahdanau</strong></em> attention, introduced in 2014 for neural machine translation. In this approach, the decoder can attend to relevant parts of the source sequence at each generation step, rather than relying solely on the fixed-size hidden state from the encoder. This is achieved by computing attention weights that determine the importance of each source token for the current decoding step.</p> <p>The attention mechanism works by calculating a compatibility score between the current decoder hidden state and each encoder hidden state. These scores are then normalized using a softmax function to obtain attention weights. The weighted sum of the encoder hidden states, based on the attention weights, forms the context vector that provides relevant information to the decoder at each step.</p> <p>By allowing the decoder to access and utilize information from the entire source sequence, the Bahdanau attention mechanism enables the model to capture long-range dependencies and generate more accurate translations. This approach laid the foundation for subsequent developments in attention mechanisms.</p> <p>Building upon this idea, the Transformer architecture, introduced in the influential paper <em><strong>"Attention Is All You Need" by Vaswani et al. in 2017</strong></em>, took attention to the next level with the self-attention mechanism. Self-attention extends the attention concept to capture dependencies within a single sequence, rather than just between the encoder and decoder.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!WXL9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" width="380" height="560" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":560,"width":380,"resizeWidth":null,"bytes":71606,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="external nofollow noopener" target="_blank">Transformer Architecture proposed by Vasvani et al in 2017</a></figcaption></figure></div> <p>In self-attention, each token in the input sequence attends to all other tokens in the sequence, allowing the model to capture rich, context-dependent representations. This mechanism enables the model to directly learn the relationships and dependencies between different positions in the sequence, without relying on recurrent or convolutional operations.</p> <p>The self-attention mechanism forms the core of the Transformer architecture and has revolutionized natural language processing. It has led to the development of powerful language models like BERT, GPT, and their variants, which have achieved state-of-the-art performance on a wide range of tasks, including language understanding, generation, and translation.</p> <p>By leveraging attention mechanisms, particularly self-attention, models can effectively capture long-range dependencies, handle variable-length sequences, and process information in parallel. This has greatly enhanced the ability of models to understand and generate coherent and contextually relevant language.</p> <p>In the following sections, we will dive deeper into the details of self-attention and explore its implementation in modern language models.</p> <h2>Simplified Self-Attention Mechanism</h2> <p>To gain a better understanding of how self-attention works, let's start with a simplified version of the mechanism and walk through the computation step by step. Consider an input sequence <code>X</code> of length 6, where each token is represented by a 3-dimensional embedding vector. The goal of self-attention is to compute a new representation for each token that incorporates information from the entire sequence. This is achieved by calculating attention weights between pairs of tokens and using these weights to compute weighted sums of the input embeddings.</p> <p>First, we compute the dot product between each pair of token embeddings. The dot product serves as a measure of similarity between tokens, indicating how much they should attend to each other. In PyTorch, we can compute the dot products efficiently using matrix multiplication:</p> <pre><code><code>import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

attn_scores = inputs @ inputs.T
</code></code></pre> <p>In the code above, <code>attn_scores[i][j]</code> represents the dot product between the embeddings of tokens i and j. The resulting <code>attn_scores</code> matrix captures the similarity scores between all pairs of tokens.</p> <p>Next, we apply the softmax function to each row of the <code>attn_scores</code> matrix to obtain the attention weights. The softmax function normalizes the scores, converting them into probabilities that sum up to 1. This ensures that the attention weights can be interpreted as the relative importance of each token for a given token:</p> <pre><code><code>attn_weights = torch.softmax(attn_scores, dim=-1)</code></code></pre> <p>After applying the softmax function, <code>attn_weights[i][j]</code> represents the normalized attention weight indicating how much token i attends to token j.</p> <p>Finally, we compute the self-attended representations by taking a weighted sum of the input embeddings using the attention weights:</p> <pre><code><code>context_vecs = attn_weights @ inputs</code></code></pre> <p>The resulting <code>context_vecs</code> matrix contains the self-attended representations for each token. Each row in <code>context_vecs</code>is a weighted sum of the input embeddings, where the weights are determined by the attention weights. This allows each token to incorporate information from the entire sequence, weighted by the relevance of each token.</p> <p>This simplified version of self-attention demonstrates the core idea of allowing tokens to attend to each other and compute new representations based on the entire sequence. However, in practice, the self-attention mechanism used in Transformer models includes additional components, such as trainable weight matrices and scaling factors, which we will explore in the next section.</p> <h2>Math Behind Self-Attention</h2> <p>In the previous section, we explored a simplified version of self-attention that directly used the input embeddings to compute attention scores and context vectors. However, in practice, the self-attention mechanism used in Transformer models incorporates trainable weight matrices to project the inputs into query, key, and value representations before computing the attention scores.</p> <p>In self-attention, each input vector <strong>xi</strong> is projected onto three distinct vectors: query <strong>qi</strong>, key <strong>ki</strong>, and value <strong>vi</strong>. </p> <p>These projections are performed via learnable weight matrices <strong>Wq</strong>, <strong>Wk</strong>, and <strong>Wv</strong>, resulting in:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"q_i = x_i W_Q, \\quad k_i=x_i W_K, \\quad v_i=x_i W_V","id":"OTDSYPPSNS"}' data-component-name="LatexBlockToDOM"></div> <p>These weight matrices are initialized randomly and optimized during training.</p> <p>The simplified matrix representation, where the query, key, and value matrices are computed as a single operation, is given by:</p> <div class="latex-rendered" data-attrs='{"persistentExpression":"\\text{attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V","id":"IYMDIGSHSI"}' data-component-name="LatexBlockToDOM"></div> <p>The working of the above attention calculation is explained in the next section.</p> <h2>Adding Trainable Weights to Self-Attention</h2> <p>The query, key, and value matrices (Q, K, V) are obtained by multiplying the input embedding matrix <code>X</code> with learned weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>, respectively:</p> <pre><code><code>import torch.nn as nn

d_in = 3
d_out = 2

W_query = nn.Linear(d_in, d_out)
W_key   = nn.Linear(d_in, d_out)
W_value = nn.Linear(d_in, d_out)

print(W_query)

# Calculate queries, keys and values
queries = W_query(inputs)
keys    = W_key(inputs)
values  = W_value(inputs)</code></code></pre> <p>Here, <code>d_in</code> represents the input embedding dimension, and <code>d_out</code> represents the output dimension of the projected queries, keys, and values. The weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code> are initialized randomly and learned during the training process.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8H5b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" width="418" height="641" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":641,"width":418,"resizeWidth":null,"bytes":61815,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="external nofollow noopener" target="_blank">Scaled Dot-Product Attention</a></figcaption></figure></div> <p>By projecting the inputs into separate query, key, and value spaces, the model can learn to capture different aspects of the input embeddings that are relevant for computing attention. This allows for more expressive and flexible representations compared to directly using the input embeddings.</p> <p>After obtaining the queries, keys, and values, the attention scores are computed as the scaled dot product between the queries and keys:</p> <pre><code><code>attn_scores = queries @ keys.T / keys.shape[-1]**0.5
print(attn_scores)</code></code></pre> <p>The scaling factor (the square root of the key dimension) is introduced to mitigate the effect of large magnitudes in the dot products, which can lead to extremely small gradients when passed through the <strong>softmax</strong> function. This scaling helps stabilize the training process and improve convergence.</p> <p>Once the attention scores are computed, the rest of the self-attention mechanism remains the same as in the simplified version. The attention weights are obtained by applying the <strong>softmax</strong> function to the scores, and the context vectors are computed as the weighted sum of the values using the attention weights.</p> <p>To encapsulate this computation in a more compact and reusable form, we can define a Python class that implements the self-attention mechanism with trainable weights:</p> <pre><code><code>class SelfAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()

        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

    def forward(self, x):
        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vecs = attn_weights @ values

        return context_vecs
</code></code></pre> <p>The <code>SelfAttention</code> class uses PyTorch's <code>nn.Linear</code> module to define the trainable weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>. The <code>forward</code> method performs the self-attention computation, taking the input embeddings <code>x</code> and returning the context vectors.</p> <p>By incorporating trainable weights into the self-attention mechanism, Transformer models can learn to adapt the attention computation to the specific requirements of the task at hand. This flexibility and expressiveness have contributed to the success of Transformer-based models in various natural language processing tasks.</p> <h2>Causal Attention: Masking Future Tokens</h2> <p>In certain tasks, such as language modeling or text generation, it's crucial to prevent the self-attention mechanism from accessing information from future tokens. This is where causal attention, also known as masked attention, comes into play.</p> <p>Causal attention restricts the self-attention computation to only consider the tokens up to the current position in the sequence. In other words, when computing the attention scores for a given token, only the tokens that appear before it in the sequence are considered.</p> <p>To achieve this, we modify the attention weight matrix by applying a mask that sets the upper triangular part of the matrix to negative infinity. This effectively prevents the model from attending to future tokens.</p> <p>Here's an example of how to create the mask and apply it to the attention scores:</p> <pre><code><code>import torch

def create_mask(context_length):
    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
    return mask.bool()

context_length = attn_scores.shape[0]
mask = create_mask(context_length)
attn_scores = attn_scores.masked_fill(mask, -torch.inf)
print(attn_scores)
</code></code></pre> <p>In the code above, we create a mask using PyTorch's <code>torch.triu</code> function, which sets the elements above the main diagonal to 1 and the rest to 0. We then convert the mask to a boolean tensor and use it to fill the upper triangular part of the <code>attn_scores</code> matrix with negative infinity.</p> <p><strong>By setting the masked positions to negative infinity, we ensure that the softmax function will assign zero attention weights to those positions, effectively ignoring the future tokens.</strong></p> <p>After applying the mask, we proceed with the rest of the self-attention computation as usual, applying the softmax function to obtain the attention weights and computing the context vectors.</p> <pre><code><code>attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
print(attn_weights)</code></code></pre> <p>It's worth noting that causal attention is particularly important in autoregressive models like GPT, where the model generates tokens sequentially and should only have access to the previously generated tokens at each step.</p> <h3>Adding Dropout</h3> <p>In addition to masking future tokens, we can also incorporate dropout regularization to the attention weights. Dropout helps prevent overfitting by randomly setting a fraction of the attention weights to zero during training. This encourages the model to rely on a broader set of tokens and reduces the risk of memorizing specific patterns.</p> <p>Here's an example of how to apply dropout to the attention weights:</p> <pre><code><code>dropout = nn.Dropout(p=0.1)
attn_weights = dropout(attn_weights)</code></code></pre> <p>In the code above, we create an instance of PyTorch's <code>nn.Dropout</code> module with a dropout probability of 0.1. We then apply the dropout to the <code>attn_weights</code> matrix, randomly setting 10% of the attention weights to zero and increasing the remaining values in the matrix by 10%.</p> <p>By incorporating causal attention and dropout regularization, we can effectively mask future tokens and improve the generalization ability of our self-attention-based models.</p> <p>To encapsulate the causal attention mechanism, we can define a <code>CausalAttention</code> class that inherits from the <code>SelfAttention</code> class and adds the masking and dropout functionality:</p> <pre><code><code>import torch.nn as nn

class CausalAttention(nn.Module):
    """
    Implements causal attention with dropout and masking.

    Args:
        d_in (int): Input embedding dimension.
        d_out (int): Output embedding dimension.
        context_length (int): Length of the context (number of tokens).
        dropout (float): Dropout rate. Default is 0.1.
        qkv_bias (bool): If True, adds a learnable bias to the Q, K, V projections. Default is False.
    """
    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):
        super().__init__()

        # Linear layers for K, Q, V projections
        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
            'mask',
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        """
        Forward pass for causal attention.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_tokens, d_in).

        Returns:
            torch.Tensor: Context vectors of shape (batch_size, num_tokens, d_out).
        """

        b, num_tokens, d_in = x.shape

        # Compute keys, queries, and values
        keys = self.W_keys(x)
        query = self.W_query(x)
        values = self.W_value(x)

        # Calculate attention scores
        att_scores = query @ keys.transpose(1, 2)  # Transpose to get (batch_size, num_tokens, num_tokens)

        # Apply mask to prevent attending to future tokens
        att_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)

        # Compute attention weights
        attn_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)

        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights)

        # Compute context vectors
        context_vec = attn_weights @ values

        return context_vec</code></code></pre> <p>The <code>CausalAttention</code> class takes additional arguments <code>context_length</code> and <code>dropout</code> to specify the maximum sequence length and the dropout probability, respectively. It also registers the mask as a buffer to ensure it is properly moved to the appropriate device along with the model.</p> <p>In the <code>forward</code> method, we apply the mask to the attention scores using <code>masked_fill</code>, ensuring that future tokens are ignored. We then apply the <strong>softmax</strong> function, perform dropout regularization, and compute the context vectors as before.</p> <p>By using the <code>CausalAttention</code> class, we can easily incorporate causal attention and dropout regularization into our self-attention-based models, enabling them to handle tasks that require masking future tokens.</p> <h2>Multi-Head Attention</h2> <p>Multi-head attention is an extension of the self-attention mechanism that allows the model to attend to different parts of the input sequence in multiple ways simultaneously. Instead of performing a single attention operation, multi-head attention splits the input embeddings into multiple smaller matrices (heads) and applies self-attention to each head independently. The results from all heads are then concatenated and linearly transformed to produce the final output.</p> <p>The motivation behind multi-head attention is to enable the model to capture different types of relationships and dependencies within the input sequence. Each head can focus on different aspects of the input, allowing the model to learn a more diverse and nuanced representation.</p> <p>Here's a step-by-step breakdown of the multi-head attention process:</p> <ul> <li><p><strong>Splitting the Input Embeddings</strong>: The input embeddings are split into multiple smaller matrices, each representing a different head. The number of heads is a hyperparameter that can be tuned based on the specific task and model architecture. If the input embeddings have dimension <code>d_out</code> and there are <code>num_heads</code> heads, each head will have a dimension of <code>d_head = d_out // num_heads</code>.</p></li> <li><p><strong>Applying Self-Attention to Each Head</strong>: For each head, the input embeddings are projected into query, key, and value matrices using separate linear transformations. The self-attention mechanism is then applied to each head independently, computing the attention scores, attention weights, and context vectors for each head.</p></li> <li><p><strong>Concatenating the Head Outputs</strong>: The context vectors from all heads are concatenated along the embedding dimension to form a single matrix. This concatenated matrix has a dimension of <code>d_model</code>, which is the same as the original input embeddings.</p></li> <li><p><strong>Linear Transformation</strong>: The concatenated matrix is passed through a final linear transformation to produce the output of the multi-head attention mechanism. This linear transformation allows the model to combine and mix the information from different heads.</p></li> </ul> <pre><code><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, num_heads, context_length, dropout):
        super().__init__()
        
        assert (d_out % num_heads == 0), "d_out must be divisible by num_heads" 
        
        self.d_in = d_in
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        # Linear layers for Q, K, V projections
        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

		# Linear layer to combine head outputs
        self.out_proj = nn.Linear(d_out, d_out)

		# Dropout layer
        self.dropout = nn.Dropout(dropout)

		# Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
		    "mask", 
			torch.triu(torch.ones(context_length, context_length), diagonal=1)
		)
        
    def forward(self, x):
    
        batch_size, num_tokens, d_in = x.shape

		# Compute keys, queries, and values
        keys = self.W_key(x)      # (batch_size, num_tokens, d_out)
        queries = self.W_query(x) # (batch_size, num_tokens, d_out)
        values = self.W_value(x)  # (batch_size, num_tokens, d_out)


		# Reshape to (batch_size, num_tokens, num_heads, head_dim)
        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)

		# Transpose to (batch_size, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

		# Calculate attention scores
        attn_scores = queries @ keys.transpose(2, 3) 
        # Shape: (batch_size, num_heads, num_tokens, num_tokens)

		# Apply mask to prevent attending to future tokens
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)

		# Compute attention weights
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        
        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights) 


		# Compute context vectors
		context_vec = attn_weights @ values 
		# Shape: (batch_size, num_heads, num_tokens, head_dim)

        # Reshape and combine heads
        context_vec = context_vec.transpose(1, 2) # (batch_size, num_tokens, num_heads, head_dim)
        context_vec = context_vec.contiguous().view(batch_size, num_tokens, -1)
        # (batch_size, num_tokens, d_out)
        
        # Apply final linear projection
		context_vec = self.out_proj(context_vec) # (batch_size, num_tokens, d_out)

		return context_vec
</code></code></pre> <p>The <code>__init__</code> method initializes the necessary parameters and modules for multi-head attention:</p> <ul> <li><p>It takes the input dimension <code>d_in</code>, output dimension <code>d_out</code>, number of heads <code>num_heads</code>, context length <code>context_length</code>, and dropout probability <code>dropout</code>.</p></li> <li><p>It asserts that <code>d_out</code> is divisible by <code>num_heads</code> to ensure proper splitting of dimensions.</p></li> <li><p>It initializes the linear transformations for the query, key, and value matrices (<code>self.W_query</code>, <code>self.W_key</code>, <code>self.W_value</code>) and an additional output projection matrix (<code>self.out_proj</code>).</p></li> <li><p>It also registers the causal mask as a buffer using <code>self.register_buffer()</code>.</p></li> </ul> <p>The <code>forward</code> method performs the multi-head attention computation:</p> <ol> <li><p>It applies the linear transformations to the input <code>x</code> to obtain the query, key, and value matrices (<code>queries</code>, <code>keys</code>, <code>values</code>).</p></li> <li><p>It splits the matrices into multiple heads by reshaping and transposing the tensors. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>, where <code>head_dim</code> is <code>d_out // num_heads</code>.</p></li> <li><p>It computes the attention scores by performing matrix multiplication between the queries and keys using the <code>@</code>operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, num_tokens)</code>.</p></li> <li><p>It applies the causal mask to the attention scores using <code>masked_fill_()</code>, setting the upper triangular part to negative infinity. This ensures that each token can only attend to the tokens that appear before it in the sequence.</p></li> <li><p>It applies the softmax function to the masked attention scores to obtain the attention weights. The scaling factor <code>keys.shape[-1]**0.5</code> is used to stabilize the gradients.</p></li> <li><p>It computes the context vectors by multiplying the attention weights with the values using the <code>@</code> operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>.</p></li> <li><p>It transposes and reshapes the context vectors to <code>(batch_size, num_tokens, d_out)</code> to combine the outputs from all heads.</p></li> <li><p>It applies the output projection matrix (<code>self.out_proj</code>) to the combined context vectors to obtain the final output.</p></li> </ol> <p>The <code>MultiHeadAttention</code> class efficiently implements multi-head attention by performing the computations in a single pass. It takes advantage of tensor operations and reshaping to parallelize the computations across multiple heads.</p> <p>By using this efficient implementation, the model can capture different types of relationships and dependencies within the input sequence, allowing it to learn more expressive and nuanced representations for various natural language processing tasks.</p> <p>The <code>MultiHeadAttention</code> class can be used as a building block in larger models, such as the Transformer architecture, to leverage the power of multi-head attention in a computationally efficient manner.</p> <h2>The GPT Architecture: Harnessing the Power of Multi-Head Attention</h2> <p>The GPT (Generative Pre-trained Transformer) architecture, which includes models like GPT-2 and GPT-3, has revolutionized the field of natural language processing. At the core of the GPT architecture lies the multi-head attention mechanism, which enables the model to capture rich linguistic patterns and generate coherent and contextually relevant text.</p> <p>For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600. The embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out).</p> <p>Here's a code snippet showcasing the initialization of the multi-head attention module in the GPT-2 architecture:</p> <pre><code><code>torch.manual_seed(123)

# Sample inputs for num_heads = 12, d_in = d_out = 768, context_length = 1024
batch = torch.rand(2, 1024, 768) 
# Two inputs with 1024 tokens each; each token has embedding dimension 768.

print(batch.shape)

batch_size, context_length, d_in = batch.shape
d_out = 768
num_heads = 12

mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, 0.0)
context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
</code></code></pre> <h2>Conclusion</h2> <p>In this blog post, we have explored the concept of attention mechanisms and their significant impact on natural language processing tasks. Starting with a simplified version of self-attention and progressing to the more advanced and efficient multi-head attention, we have seen how attention allows models to selectively focus on relevant parts of the input sequence and capture long-range dependencies effectively. Self-attention enables each token to attend to every other token in the sequence, facilitating the learning of rich, context-dependent representations. Multi-head attention takes this a step further by allowing models to capture different types of relationships and dependencies simultaneously, enhancing their expressive power and ability to understand and generate natural language.</p> <p>Congrats on sticking with the blog and understanding the importance of attention in coding the GPT-style models from scratch. Implementing attention mechanisms, especially multi-head attention, is crucial for building the Transformer architecture. By delving into the details and implementing it efficiently, you’ve gained valuable insights into the core component that powers many top-notch NLP models.</p> <div><hr></div> <p>Thanks for reading NeuraForge: AI Unleashed!</p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. 🧠✨</p> <p>Connect with me on <a href="https://www.linkedin.com/in/narasimhakarthik/" rel="external nofollow noopener" target="_blank">LinkedIn</a>. </p> <p></p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p></p> <p></p> </body></html>