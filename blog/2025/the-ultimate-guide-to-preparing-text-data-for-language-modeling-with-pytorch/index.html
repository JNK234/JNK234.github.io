<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2>Introduction</h2> <p>When working with large language models (LLMs), one of the most crucial steps is preparing the textual data in a format that these models can understand and learn from. This process involves converting raw text into numerical vectors, known as embeddings, as LLMs cannot directly process plain text.</p> <p>In this post, we'll take a deep dive into the techniques and best practices for text preprocessing and embedding generation using PyTorch, a popular deep learning framework. We'll cover everything from basic tokenization to implementing advanced algorithms like Byte Pair Encoding (BPE), creating efficient data sampling techniques, and building embedding layers from scratch. By the end, you'll have a solid understanding of how to prepare text data for training powerful language models. To explore the concepts further and see the code in action, check out the accompanying Colab notebook <a href="https://colab.research.google.com/drive/1n6UjZRTRP0yRdcvJLHK2CRfNL8Gj_-IL?usp=sharing" rel="external nofollow noopener" target="_blank">here</a> and follow along with the step-by-step examples.</p> <p>Let's get started!</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"></source><img src="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="5472" height="3648" data-attrs='{"src":"https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":3648,"width":5472,"resizeWidth":null,"bytes":null,"alt":"open book lot","title":null,"type":"image/jpg","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="open book lot" title="open book lot" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Photo by <a href="true">Patrick Tomasso</a> on <a href="https://unsplash.com" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure></div> <h2>Understanding Embeddings: The Bridge Between Text and Mathematics</h2> <p>Before we dive into the technical implementation details, let's understand what embeddings are and why they're crucial for language models. Think of embeddings as a way to translate words into numbers â€“ but not just any numbers. They're carefully crafted numerical representations that capture the meaning, relationships, and context of words in a way that computers can process.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!m1KP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" width="1100" height="566" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/c041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":566,"width":1100,"resizeWidth":null,"bytes":113900,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":false,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">Embedding representations</a></em></figcaption></figure></div> <h3>What Are Embeddings and Why Do We Need Them?</h3> <p>At their core, embeddings are dense vectors (arrays of numbers) that represent words or tokens in a continuous vector space. When you feed the word "cat" to a computer, you can't just use the letters "c-a-t" - computers need numbers to perform calculations. An embedding transforms "cat" into a vector like [0.2, -0.5, 0.8, ...], where each number helps represent different aspects of the word's meaning.</p> <p>What makes embeddings powerful is their ability to capture semantic relationships. Words with similar meanings end up having similar numerical representations. For example, the embeddings for "cat" and "kitten" would be more similar to each other than to the embedding for "submarine". This similarity can be measured mathematically, allowing models to understand relationships between words.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!UPsT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" width="1456" height="819" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/f0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":819,"width":1456,"resizeWidth":null,"bytes":269534,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: <a href="https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings" rel="external nofollow noopener" target="_blank">Representation of embedding vectors in 2D</a></figcaption></figure></div> <p>Modern embedding systems typically represent words in high-dimensional spaces. For example:</p> <ul> <li><p>GPT-2 uses 768-dimensional embeddings for its smallest model</p></li> <li><p>GPT-3's largest model uses 12,288-dimensional embeddings</p></li> <li><p>BERT-base uses 768-dimensional embeddings</p></li> </ul> <p>The real power of embeddings comes from their ability to learn from data. During model training, these embeddings are automatically adjusted to capture relationships present in the training data, adapting to specific domains and discovering nuanced patterns that might not be obvious to human designers.</p> <h2>Text Tokenization and Preprocessing Techniques</h2> <p>The first step in preparing text for LLMs is <strong>tokenization</strong> - breaking down raw text into smaller units called tokens. Tokens can be individual words, subwords, or even characters. The goal is to create a finite set of meaningful units that the model can learn from.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!rt5h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" width="862" height="529" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":529,"width":862,"resizeWidth":null,"bytes":109042,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LLMs from Scratch</a></em></figcaption></figure></div> <p>However, raw text often contains noise and inconsistencies that can hinder the tokenization process. These include:</p> <ul> <li><p>Inconsistent casing (e.g., "Hello" vs "hello")</p></li> <li><p>Punctuation attached to words (e.g., "world!")</p></li> <li><p>Special characters and contractions (e.g., "don't", "U.S.A.")</p></li> <li><p>Unknown or rare words</p></li> </ul> <p>To handle these issues and perform effective tokenization, we can use a combination of text preprocessing techniques and regular expressions in Python. Here's an example code snippet that demonstrates this:</p> <pre><code><code>import re

UNK = '&lt;unk&gt;'  # Token for unknown words
EOS = '&lt;eos&gt;'  # Token for end of text

def tokenize(text, known_words):
    # Lowercase the text
    text = text.lower()

    # Split on whitespace and punctuation using regular expressions
    tokens = re.findall(r"\w+|[^\w\s]", text)

    # Replace unknown words with &lt;unk&gt; token
    tokens = [t if t in known_words else UNK for t in tokens]

    # Append &lt;eos&gt; token to the end of the text
    tokens.append(EOS)

    return tokens

# Example usage
text = "Hello, world! This is a sample sentence."
known_words = {'this', 'is', 'a', 'sample', 'sentence'}

print(tokenize(text, known_words))</code></code></pre> <pre><code>['&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', 'this', 'is', 'a', 'sample', 'sentence', '&lt;unk&gt;', '&lt;eos&gt;']</code></pre> <p>Let's break down the tokenization process step by step:</p> <ol> <li><p>First, we convert the entire text to lowercase using <code>text.lower()</code>. This ensures consistent casing across all words.</p></li> <li><p>Next, we use a regular expression <code>r"\w+|[^\w\s]"</code> to split the text on whitespace and punctuation. The regex pattern <code>\w+</code> matches one or more word characters , while <code>[^\w\s]</code> matches any single character that is not a word character or whitespace. This effectively separates words and punctuation into individual tokens.</p></li> <li><p>We then replace any unknown words (i.e., words not in the <code>known_words</code> set) with a special <code>&lt;unk&gt;</code> token. This helps the model handle out-of-vocabulary words gracefully during training and inference.</p></li> <li><p>Finally, we append an <code>&lt;eos&gt;</code> token to the end of the tokenized text to mark the end of the sequence. This is useful for the model to learn when a text or document ends.</p></li> </ol> <h2>Understanding and Implementing Byte Pair Encoding</h2> <p>While the tokenization approach we discussed so far works well for many cases, it has some limitations. One major drawback is the handling of unknown or rare words. Replacing all uncommon words with a generic <code>&lt;unk&gt;</code> token can lead to loss of information and hinder the model's ability to understand the nuances of the text.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Z7y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" width="602" height="279" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":279,"width":602,"resizeWidth":null,"bytes":40598,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LLMs from Scratch</a></em>...</figcaption></figure></div> <p>This is where Byte Pair Encoding (BPE) comes into play. BPE is a <strong>subword tokenization algorithm</strong> that iteratively builds a vocabulary of subword units based on their frequency in the training corpus. It starts with individual characters and progressively merges them into larger subword units until a desired vocabulary size is reached. This allows BPE to effectively handle out-of-vocabulary words by representing them as combinations of subword units.</p> <p>Let's walk through a step-by-step example to better understand how BPE constructs its vocabulary. Imagine we have the following list of words:</p> <pre><code><code>['low', 'lower', 'newest', 'widest']</code></code></pre> <ul><li><p>Step 1: Initialization</p></li></ul> <p>BPE begins by splitting each word into individual characters and appending a special end-of-word symbol, typically denoted by <code>&lt;/w&gt;</code>, to mark the end of each word. This initial segmentation looks like this:</p> <pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre> <ul><li><p>Step 2: Frequency Counting</p></li></ul> <p>Next, BPE counts the frequency of each character pair in the corpus. In this example, the most frequent pair is <code>e</code> followed by <code>&lt;/w&gt;</code>, as it appears in two words: <code>lower</code> and <code>newest</code>:</p> <ul><li><p>Step 3: Merging</p></li></ul> <p>BPE merges the most frequent pair into a new subword unit. In our example, <code>e&lt;/w&gt;</code> becomes a single unit i.e. considered as single token in vocabulary:</p> <pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre> <ul><li><p>Step 4: Iteration</p></li></ul> <p>The process of frequency counting and merging is repeated iteratively. In the next iteration, the most frequent pair is <code>es</code> followed by <code>&lt;/w&gt;</code>, so they get merged:</p> <pre><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w es&lt;/w&gt;', 'w i d es&lt;/w&gt;']</code></pre> <p>This iterative process continues until one of two conditions is met:</p> <ol> <li><p>A desired vocabulary size is reached (e.g., 10,000 subword units).</p></li> <li><p>No more frequent pairs are found (i.e., all possible merges have been performed).</p></li> </ol> <p>The resulting set of subword units, along with their frequencies, forms the final BPE vocabulary. To further illustrate how BPE handles out-of-vocabulary words, let's consider an example. Suppose we have a BPE vocabulary that includes the subword units <code>low</code>, <code>est</code>, and <code>&lt;/w&gt;</code>, but not the word <code>lowest</code>. When encountering <code>lowest</code>, BPE would break it down into the known subword units:</p> <pre><code>['low', 'est', '&lt;/w&gt;']</code></pre> <p>By representing <code>lowest</code> as a combination of subword units, BPE enables the model to process and generate words it hasn't seen during training.</p> <h3> <br>Byte Pair Encoding in Python</h3> <p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library. <code>tiktoken</code> is an existing Python open source library (<a href="https://github.com/openai/tiktoken" rel="external nofollow noopener" target="_blank">https://github.com/openai/tiktoken</a>), which implements the BPE algorithm very efficiently based on source code in Rust. It can be installed as follows:</p> <p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library:</p> <pre><code>pip install tiktoken</code></pre> <pre><code><code>import tiktoken

# Load the BPE tokenizer 
bpe_tokenizer = tiktoken.get_encoding("gpt2")

text = "This is an example of byte pair encoding! xhsbfubs"

# Tokenize the text using BPE
tokens = bpe_tokenizer.encode(text) 
decoded = bpe_tokenizer.decode(tokens) 

print(f"Encoded tokens: {tokens}") 
print(f"Decoded text: {decoded}")

# Encoded tokens: [1212, 318, 281, 1672, 286, 18022, 5166, 21004, 0, #  # 2124, 11994, 19881, 23161] 
# Decoded text: This is an example of byte pair encoding! xhsbfubs
</code></code></pre> <p>The <code>tiktoken</code> library provides an efficient implementation of the BPE algorithm used by OpenAI's GPT models. We first load the BPE tokenizer with <code>tiktoken.get_encoding("gpt2")</code>, which gives us access to the same tokenizer used by the GPT-2 model.</p> <p>We then encode our text using <code>bpe_tokenizer.encode(text)</code>, which applies the BPE algorithm and returns a list of token IDs. These IDs correspond to the subwords in the BPE vocabulary.</p> <p>Finally, we can decode the token IDs back into the original text using <code>bpe_tokenizer.decode(tokens)</code>. This demonstrates that BPE can effectively tokenize and reconstruct the text without losing information.</p> <p>The real power of BPE lies in its ability to handle out-of-vocabulary words. Since it breaks down words into subwords, even if a word is not explicitly present in the vocabulary, it can still be represented by a combination of subwords. This allows the model to understand and generate words it hasn't seen during training. By understanding and implementing Byte Pair Encoding, you can take your text preprocessing to the next level and build more powerful and versatile language models.</p> <h2>Creating and Managing Sampling Windows</h2> <p>Now that we have our text data tokenized into a sequence of token IDs, the next step is to prepare it for training our language model. But how exactly do we feed this data to the model?</p> <p>To answer that, let's first understand how language models like GPT learn. During training, the model tries to predict the next token in a sequence given the tokens that come before it. For example, if the input is "The cat sat on the", the model learns to predict the next most likely token, such as "mat" or "couch".</p> <p>To facilitate this learning process, we need to create input-target pairs from our tokenized text. The input will be a sequence of tokens, and the target will be the next token that follows this sequence. We can generate these pairs using a technique called <strong>sampling windows</strong>. The sampling window is popularly also known as <strong>context length</strong>.</p> <p>Imagine our tokenized text as a long ribbon. We take a small window of a fixed size (say, 50 tokens) and slide it over the ribbon. At each step, the tokens inside the window become our input, and the token immediately following the window becomes the target. We keep sliding the window until we reach the end of the ribbon.</p> <p>Here's a visual representation:</p> <pre><code><code>[The, cat, sat, on, the, mat, ., &lt;eos&gt;]
 |   window 1    |
      |   window 2    |
           |   window 3    |</code></code></pre> <p>In window 1, the input is <code>[The, cat, sat, on, the]</code> and the target is <code>mat</code>. In window 2, the input is <code>[cat, sat, on, the, mat]</code> and the target is <code>.</code>. And so on.</p> <p>By creating these sampling windows, we break down our long text into manageable sequences that the model can learn from. The size of the window is a hyperparameter that we can tune. A larger window allows the model to learn from more context, but it also increases the computational complexity.</p> <p>Now, let's see how we can implement this in Python. We'll use PyTorch's <code>Dataset</code> and <code>DataLoader</code> classes to create an efficient data pipeline.</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, tokens, window_size):
        # Store the tokenized text
        self.tokens = tokens  
        # Store the size of the sampling window
        self.window_size = window_size  

    def __len__(self):
        # Return the total number of sampling windows
        return len(self.tokens) - self.window_size

    def __getitem__(self, idx):
        # Get the input-target pair for the given index
        input_seq = self.tokens[idx:idx+self.window_size]  # Input sequence
        target_seq = self.tokens[idx+1:idx+self.window_size+1]  # Target sequence (shifted by 1)
        return torch.tensor(input_seq), torch.tensor(target_seq)

# Example usage
tokens = [1212, 318, 281, 1672, 286, 2419, 683, 26254, 0] # Tokenized text
dataset = TextDataset(tokens, window_size=5)  # Create a TextDataset with window size of 5
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # Create a DataLoader with batch size of 2 and shuffling enabled

for inputs, targets in dataloader:
    print(inputs)  # Print the input sequences
    print(targets)  # Print the corresponding target sequences
    break  # Break after the first batch (for demonstration purposes)
</code></code></pre> <pre><code>tensor([[ 1672, 286, 2419, 683, 26254], 
		[ 1212, 318, 281, 1672, 286]]) 

tensor([[ 286, 2419, 683, 26254, 0], 
		[ 318, 281, 1672, 286, 2419]])</code></pre> <p>Let's break this down step by step:</p> <ol> <li><p>We define a custom <code>TextDataset</code> class that inherits from PyTorch's <code>Dataset</code> class. This class takes the tokenized text and the window size as input.</p></li> <li><p>The <code>__len__</code> method returns the total number of sampling windows we can create from the tokenized text. We subtract the window size to avoid going out of bounds.</p></li> <li><p>The <code>__getitem__</code> method is the heart of the dataset. It takes an index <code>idx</code> and returns the input-target pair for the corresponding sampling window. The input is <code>tokens[idx:idx+window_size]</code> and the target is <code>tokens[idx+1:idx+window_size+1]</code>, i.e., the input sequence shifted by one token.</p></li> <li><p>We then create an instance of the <code>TextDataset</code> with our tokenized text and a window size of 5.</p></li> <li><p>We wrap the dataset in a <code>DataLoader</code>, which allows us to batch the data and shuffle it for training. Here, we use a batch size of 2.</p></li> <li><p>Finally, we loop over the dataloader to get batches of input-target pairs. Each input is a tensor of shape <code>(batch_size, window_size)</code>, and each target is a tensor of shape <code>(batch_size, window_size)</code>.</p></li> </ol> <p>Now consider the following code which provides the best practice for creating datasets and dataloaders:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class GPTDataset(Dataset):
    def __init__(self, text, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []
        
        # Tokenize entire text
        token_ids = tokenizer.encode(text)
        
        # Create overlapping sequences
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1:i + max_length + 1]
            
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))
            
    def __len__(self):
        return len(self.input_ids)
        
    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader(text, batch_size=4, max_length=256, stride=128):
    """Create an efficient data loader for training"""
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDataset(text, tokenizer, max_length, stride)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    return dataloader
</code></code></pre> <h2>Building Token Embeddings from Scratch</h2> <p>So far, we've seen how to preprocess text data and convert it into sequences of token IDs using techniques like tokenization and Byte Pair Encoding. The next crucial step is to transform these discrete token IDs into continuous vector representations, known as <strong>embeddings</strong>.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8wV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" data-component-name="Image2ToDOM" rel="external nofollow noopener"><div class="image2-inset"> <picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw"></source><img src="https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" width="1372" height="966" data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":966,"width":1372,"resizeWidth":null,"bytes":41167,"alt":null,"title":null,"type":"image/png","href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"> <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div> <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div> </div></div> </div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png</figcaption></figure></div> <p>Embeddings are dense, low-dimensional vectors that capture semantic and syntactic information about the tokens. By representing tokens as embeddings, we enable the language model to learn meaningful relationships and patterns in the text data.</p> <p>In PyTorch, we can create embeddings using the <code>nn.Embedding</code> layer. This layer maps each token ID to a corresponding vector of a specified size.</p> <p>Here's an example of how to create an embedding layer in PyTorch:</p> <pre><code><code>import torch
import torch.nn as nn

vocab_size = 10000  # Size of the vocabulary (number of unique tokens)
embed_size = 128  # Dimensionality of the embedding vectors

embedding_layer = nn.Embedding(vocab_size, embed_size)
</code></code></pre> <p>In this code snippet, we define an embedding layer with a vocabulary size of 10,000 and an embedding size of 128. This means that each token ID will be mapped to a 128-dimensional vector.</p> <p>To use the embedding layer, we simply pass the token IDs through it:</p> <pre><code><code>token_ids = torch.tensor([1, 2, 3, 4])  # Example token IDs
embeddings = embedding_layer(token_ids)

print(embeddings.shape)  

# Output: torch.Size([4, 128])
</code></code></pre> <p>Here, we pass a tensor of token IDs through the embedding layer, and it returns the corresponding embeddings. The resulting <code>embeddings</code> tensor has a shape of <code>(4, 128)</code>, indicating that we have 4 tokens, each represented by a 128-dimensional vector.</p> <p>But how does the embedding layer know what values to assign to each token's embedding vector? Initially, the embedding layer is <strong>randomly</strong> initialized. During the training process, the language model learns to adjust these embeddings based on the patterns and relationships in the text data.</p> <p>However, the token embeddings each word independently, regardless of its position. This is where positional embeddings come in â€“ they help the model understand where each word appears in the sequence.</p> <h3>Adding Positional Embeddings</h3> <p>The self-attention mechanism in transformer models is inherently position-agnostic. When looking at token embeddings alone, the model has no way to know whether "cat" appears at the beginning, middle, or end of the sentence. Positional embeddings solve this by adding position-specific information to each token embedding.</p> <p>Think of it this way: if token embeddings tell us "what" the word is, positional embeddings tell us "where" it appears. When we combine them, the model gets both pieces of information simultaneously.</p> <h4>Implementing Positional Embeddings</h4> <p>Let's implement a complete embedding system that combines both token and positional embeddings. We choose <code>max_sequence_length</code> based on how long our input sequences might be:</p> <pre><code><code># Define max_sequence_length as 512
max_sequence_lengthÂ =Â 512Â 

# Create positional embedding layer
position_embeddingÂ =Â nn.Embedding(max_sequence_length,Â embed_size)
</code></code></pre> <p>Now, generate position indices for our sequence:</p> <pre><code><code># If our token_ids has length 4, we need positions [0, 1, 2, 3]Â 
positionsÂ =Â torch.arange(len(token_ids))Â 
print(f"Position indices:Â {positions}")

# Output: Position indices: tensor([0, 1, 2, 3])</code></code></pre> <p>Now get the embeddings for these positions or indices:</p> <pre><code><code># Get embeddings from position_embeddings layer
position_embeddingsÂ =Â position_embedding(positions)Â 
print(f"Position embedding shape:Â {position_embeddings.shape}")

# Output: Position embedding shape: torch.Size([4, 128])</code></code></pre> <p>Now combine both token embeddings and positional embeddings:</p> <pre><code><code>combined_embeddings = embeddings + position_embeddings
print(f"Combined embedding shape: {combined_embeddings.shape}")

# Output: Combined embedding shape: torch.Size([4, 128])</code></code></pre> <h3>Implementing Embeddings (Best Practise)</h3> <p>Let's implement a complete embedding system that combines both token and positional embeddings:</p> <pre><code><code># Best practices implementation
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_sequence_length):
        super().__init__()
        
        # Initialize embeddings with proper scaling
        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.position_embedding = nn.Embedding(max_sequence_length, embedding_dim)
        
        # Initialize weights using normal distribution
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.position_embedding.weight, std=0.02)
        
    def forward(self, token_ids):
        # Apply scaling to token embeddings
        token_embeddings = self.token_embedding(token_ids) * self.scale
        
        # Create and cache position indices
        if not hasattr(self, '_position_ids'):
            self._position_ids = torch.arange(
                token_ids.size(1), 
                device=token_ids.device
            )
        
        # Add positional embeddings
        return token_embeddings + self.position_embedding(self._position_ids)</code></code></pre> <p>Let's break down how this works:</p> <ol> <li><p><strong>Token Embeddings</strong>: Each word gets transformed into a dense vector through the <code>token_embedding</code> layer, just as we discussed earlier.</p></li> <li><p><strong>Position Numbers</strong>: We create a sequence of position indices (0, 1, 2, ...) for each position in our input sequence.</p></li> <li><p><strong>Position Embeddings</strong>: These indices get transformed into position-specific vectors through the <code>position_embedding</code> layer.</p></li> <li><p><strong>Combination</strong>: We add the token and positional embeddings together. This addition operation preserves both the meaning of the word (from token embeddings) and its position (from positional embeddings).</p></li> </ol> <h2>Wrapping Up</h2> <p>In this comprehensive guide, we've explored the fundamental building blocks of text preprocessing for language modeling. We started by diving into tokenization techniques, learning how to break down raw text into meaningful units while handling challenges like punctuation, casing, and special characters. Next, we discovered the power of Byte Pair Encoding (BPE) for creating subword vocabularies that effectively handle rare and unknown words. We then learned how to construct efficient sampling windows to prepare tokenized text for training, and finally, we built token embeddings from scratch using PyTorch, incorporating positional information to capture word order and context.</p> <p>Remember, the techniques and concepts we've discussed are not just theoretical - they have practical applications in a wide range of natural language processing tasks, such as language translation, text summarization, sentiment analysis, and more. By mastering these fundamentals, you'll be equipped to tackle complex language modeling challenges and build impressive AI systems.</p> <div><hr></div> <p>Thanks for reading NeuraForge: AI Unleashed!</p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. ðŸ§ âœ¨</p> <p></p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/subscribe?","text":"Subscribe now","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?" rel="external nofollow noopener" target="_blank"><span>Subscribe now</span></a></p> <p class="button-wrapper" data-attrs='{"url":"https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share","text":"Share","action":null,"class":null}' data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="external nofollow noopener" target="_blank"><span>Share</span></a></p> <p></p> <h1></h1> <p></p> <p></p> <p></p> </body></html>