<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Training an unknown label detecting classifier using fastai</h3> <h4>Unknow labels detection: Detecting out of domain data i.e. classifying images only it is trained for.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*b40tp_HwIBKNDleBihKUlA.jpeg"><figcaption>Photo by <a href="https://unsplash.com/@seanpollock?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Sean Pollock</a> on <a href="https://unsplash.com/s/photos/work?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <h3>Motivation</h3> <p>I have discussed in my previous blog post how we can train an out-of-domain input detecting image classifier using a multi-label classification approach. In this blog post let’s discuss how to train a regular multi-class classifier but make it some more intelligent i.e giving it the ability to detect data that it doesn’t know about. We are using the fast.ai library to create and train the model.</p> <h3>Explore Dataset</h3> <p>Let’s approach the problem by using the PETs dataset. This dataset has 37 categories of different pet breeds with nearly 200 images for each class. More details about this dataset can be found <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Let’s get started by installing and importing the required libraries.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/77d0e843ee1955240821222503e0a366/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/77d0e843ee1955240821222503e0a366/href</a></iframe> <p>Now let’s download the PETs dataset,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b57ae0601407b155e4cddf0c8f3c722f/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/b57ae0601407b155e4cddf0c8f3c722f/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZGn4tl7-_oEKg_I80hj-cQ.png"><figcaption>Output</figcaption></figure> <p>Since we have downloaded the dataset, let’s build a datablock object and dataloaders.</p> <h3>Getting the data ready</h3> <p>DataBlock is a high-level API that is used to easily build and load the data i.e. create dataloaders that can directly be served to model. This makes data loading a lot easier and more customizable. Now, let’s create a datablock object according to our dataset.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c1bebe37d2149201c8f40ac58b78f2ff/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/c1bebe37d2149201c8f40ac58b78f2ff/href</a></iframe> <p>Now since the datablock object is ready, let’s see in some detail what it is doing,</p> <ul> <li> <strong>blocks:</strong> This parameter defines the input and target types of the data that we are providing to the model. In the above code, <strong>ImageBlock</strong> represents the input type as an image and <strong>MultiCategoryBlock </strong>represents the output type as a one-hot-encoded array whereas the conventional <strong>CategoryBlock </strong>which is used for image classification uses ordinal encoding of targets.</li> <li> <strong>get_items:</strong> This parameter defines how to get the inputs. We use the fastai’s util function <strong>get_image_files </strong>which takes a <strong>path</strong> <strong>to a folder </strong>as a parameter and returns paths to all the images present in the given path. Since we have defined the input block as ImageBlock, datablock will be able to convert the image paths into PIL Image.</li> <li> <strong>get_y: </strong>This parameter defines how to get the targets. We have seen an example of the image path which we have downloaded. Each image file name has a pattern i.e. ‘breed_name’_XXX.jpg where X=0–9. To get the breed name we can use <strong>regex</strong> to extract it from the path string and convert it to a list. We are using fastai’s <strong>Pipeline</strong> API to create a function pipeline.</li> <li> <strong>splitter:</strong> This parameter defines how the train-validation split is being done. We are using <strong>RandomSplitter </strong>to<strong> </strong>perform random splitting of the dataset with 20% for validation (default). We are setting random seed as 42 to ensure we get the same validation set for every run.</li> <li> <strong>item_tfms:</strong> This parameter defines the transforms to be applied to each item. We are using the <strong>Resize </strong>transform with <strong>size=460</strong> that resizes each image to size 460.</li> <li> <strong>batch_tfms:</strong> This parameter defines the transforms to be applied to each batch of data. Since this is being applied on batches of data, the operation runs on GPU (if available) to make it faster. In the above code, we are using the fastai’s <strong>aug_tranforms </strong>augmentation function which applies basic data augmentations like flip, change contrast, brightness, resize etc.</li> </ul> <p>When the <strong>size</strong> and <strong>min_scale </strong>arguments are given, the <strong>aug_transform</strong> function randomly crops the image to the given size by retaining at least some minimum amount of image data every epoch, which is specified by the <strong>min_scale</strong> argument.</p> <p>This datablock object accepts the <strong>path</strong> to the directory containing images/data as a parameter while loading dataloaders. So now let’s create dataloaders and display some image samples,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/aa483c8af1254c33faed24455f8adf6d/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/aa483c8af1254c33faed24455f8adf6d/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mbJ56qdfS8dr9sGuR4Obig.png"><figcaption>Output</figcaption></figure> <p>Let’s also look at some of the targets,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d73be6db03197763a8901c68c22102f4/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/d73be6db03197763a8901c68c22102f4/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HoPKuF8TL71IIfLJqt5LZQ.png"><figcaption>Output</figcaption></figure> <p>We can see that the target is a one-hot-encoded array with a total of 37 classes. You can check all the labels by dls.vocab .</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tNPzuwVyO5fzwOt4PD0XJA.png"><figcaption>List of all the 37 classes</figcaption></figure> <p>Now let’s use a pre-trained model and fine-tune it with our data.</p> <h3>Model Training</h3> <p>Let’s create a learner object using the <strong>resnet50</strong> pre-trained model. Since we are using the multi-label classification method to detect unknown labels, we need to change the loss function as well. So we should use the binary cross-entropy loss function or fastai’s BCEWithLogitsLossFlat() loss function with the default threshold value. We should use <strong>accuracy_multi </strong>with a higher threshold as a metric to ensure only the label with the highest probability is chosen.</p> <p>To reduce the model size and training time, we are using the half-precision training method by converting all the weights in the model to 16-bit floats. For achieving this to_fp16() is used.</p> <p>Now let’s look at the code,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bee6e52079f68eb2d29705daee86cc09/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/bee6e52079f68eb2d29705daee86cc09/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/994/1*KVi-4HExlUzo8HdXUz7MDQ.png"><figcaption>Output- Learning rate finder</figcaption></figure> <p>Since we have found the optimal learning rate to use, let’s fine-tune the model for 3 epochs.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ae3459f0d6913501ba1453761bc7cb57/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/ae3459f0d6913501ba1453761bc7cb57/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/826/1*GyWnWHWwDT7RCGSi1pELYw.png"><figcaption>Output-Model training</figcaption></figure> <p>We can see that within 3 epochs of training, our model has got 99% accuracy… This is a pretty impressive result, thanks to the pre-trained resnet50 model. Let’s also look at the loss plot,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5b0c20ae1fd6d5d26140c07d86d2c8c8/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/5b0c20ae1fd6d5d26140c07d86d2c8c8/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/956/1*nGBHibFw1JwaAnChnPIXfg.png"><figcaption>Output-Loss Plot</figcaption></figure> <p>Okay, we can see that the training and validation loss has reduced gradually and the model is not overfitting.</p> <p>Now let’s proceed to model inference, where we can test the unknown label detection in action…</p> <h3>Inference</h3> <p>Before inference, let’s update the threshold of our BCEWithLogitsLossFlat() loss function to 0.95. Since the loss function we are using has the <strong>sigmoid</strong> activation, we should increase the threshold to enable the detection of labels that the model is highly confident about.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f050eaf1eb433c12c0ec9c0c6f2e5e69/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/f050eaf1eb433c12c0ec9c0c6f2e5e69/href</a></iframe> <p>Now let’s check some results,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/08e9243d9a42ff98a3bc6948d8c61b38/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/08e9243d9a42ff98a3bc6948d8c61b38/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gFBCCJQWHQJxwVsMPDV1vA.png"><figcaption>Output: Inference Results</figcaption></figure> <p>We can see from the above results that our model is doing good on the validation set.</p> <p>Now let’s do the model inference manually i.e. download some images from the internet and check if the results are matching...</p> <p>To test the positive case, I have downloaded an image of ‘Bombay cat’ from google images to the notebook location and created a PIL Image object,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/96594a0910b0408bf2683b3d1e4d827f/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/96594a0910b0408bf2683b3d1e4d827f/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/664/1*H1R7QKAM91zzI-t_lG5bMw.png"><figcaption>Output: Bombay Cat Image.</figcaption></figure> <p>Now let’s predict what our model will predict,</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f25b135a6c6fbda99c38d31ada57e865/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/f25b135a6c6fbda99c38d31ada57e865/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*axpKzczsBOQhj2RnxEWGfQ.png"><figcaption>Output for prediction</figcaption></figure> <p>Yay... Our model correctly predicted that the class is ‘Bombay’. We can also see that the probability for this class is 0.99… which is pretty high.</p> <p>Now let’s try to find predictions for some out of domain data. I have downloaded an image of the Eiffel tower from google images and used it for prediction…</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8763b828ec4c83d60c606c5789a0482c/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/8763b828ec4c83d60c606c5789a0482c/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6SvPhuWEeLGWjmQttSCZjw.png"><figcaption>Output: Eiffle Tower Image</figcaption></figure> <p>Now if we try to find predictions for this image, our model should return negative for all the classes. Let’s find the prediction and check…</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/63abd2604bedd1969603298921444b6b/href" rel="external nofollow noopener" target="_blank">https://medium.com/media/63abd2604bedd1969603298921444b6b/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jwH3R8EILcVNxP04hd8GXg.png"><figcaption>Output: Prediction for tower image</figcaption></figure> <p>Great!! we can see that our model was able to understand that this image doesn’t belong to any dog or cat breed it was trained for i.e. 37 breeds of pets and returned <strong>False</strong> for all the available classes.</p> <h3>Conclusion</h3> <p>So from the above results, we can see that using the multi-label classification technique to train a multi-class classification model is the best way to train an intelligent image classifier.</p> <p>Thank you</p> <h4>Sources:</h4> <ol> <li> <em>Deep Learning for Coders with fastai and PyTorch, </em>Book by Howard &amp; Gugger.</li> <li><a href="http://walkwithfastai.com" rel="external nofollow noopener" target="_blank"><em>Walk with fastai</em></a></li> </ol> <h4>You can connect with me on LinkedIn <a href="https://www.linkedin.com/in/narasimhakarthik/" rel="external nofollow noopener" target="_blank">here</a>. GitHub link for the notebook can be accessed <a href="https://github.com/JNK234/100-days-of-deep-learning/blob/main/Day%205/Detecting_Unknown_labels.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</h4> <p><a href="https://medium.com/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb" rel="external nofollow noopener" target="_blank">Mlearning.ai Submission Suggestions</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=be705c001fb2" width="1" height="1" alt=""></p> </body></html>