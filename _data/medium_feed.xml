<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Narasimha Karthik Jwalapuram on Medium]]></title>
        <description><![CDATA[Stories by Narasimha Karthik Jwalapuram on Medium]]></description>
        <link>https://medium.com/@narasimhakarthik?source=rss-c115716f79b5------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*8UlHWY8tu8lsATqsULFTNQ.jpeg</url>
            <title>Stories by Narasimha Karthik Jwalapuram on Medium</title>
            <link>https://medium.com/@narasimhakarthik?source=rss-c115716f79b5------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sun, 07 Sep 2025 16:24:22 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@narasimhakarthik/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[PyTorch in Practice: Essential Building Blocks for Modern Deep Learning]]></title>
            <link>https://narasimhakarthik.medium.com/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning-06b66a651f1f?source=rss-c115716f79b5------2</link>
            <guid isPermaLink="false">https://medium.com/p/06b66a651f1f</guid>
            <category><![CDATA[pytorch]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[deep-learning]]></category>
            <dc:creator><![CDATA[Narasimha Karthik Jwalapuram]]></dc:creator>
            <pubDate>Thu, 02 Jan 2025 03:49:50 GMT</pubDate>
            <atom:updated>2025-01-02T03:49:50.378Z</atom:updated>
            <content:encoded><![CDATA[<h4>From Tensors to Neural Networks: Understanding Core Components</h4><h3>Introduction</h3><p>As deep learning continues to advance artificial intelligence applications, PyTorch has established itself as a fundamental framework powering everything from computer vision systems to large language models. Originally developed by Meta’s AI Research lab, PyTorch combines Python’s flexibility with deep learning capabilities through a powerful, intuitive interface.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MjgGftC7-d5x2mD7" /><figcaption>Photo by <a href="https://neuraforge.substack.com/p/true">Iker Urteaga</a> on <a href="https://unsplash.com/">Unsplash</a></figcaption></figure><h4>Core Components of PyTorch</h4><p>PyTorch’s architecture rests on three key components that work together to enable efficient deep learning development:</p><ol><li><strong>Dynamic Tensor Library</strong></li></ol><ul><li>Extends NumPy’s array programming capabilities</li><li>Provides seamless CPU and GPU acceleration</li><li>Implements efficient mathematical operations for deep learning computations</li></ul><p>2. <strong>Automatic Differentiation Engine (Autograd)</strong></p><ul><li>Computes gradients automatically through computational graphs</li><li>Manages backpropagation for neural network training</li></ul><p>3. <strong>Deep Learning Framework</strong></p><ul><li>Delivers modular neural network components</li><li>Implements optimized loss functions and optimizers</li></ul><h3>Getting Started with PyTorch</h3><h4>Installation and Setup</h4><p>PyTorch can be installed directly using pip, Python’s package installer:</p><pre>pip install torch</pre><p>However, for optimal performance, it’s recommended to install the version specifically compatible with your system’s hardware. Visit <a href="https://pytorch.org/">pytorch.org</a> to get the appropriate installation command based on your:</p><ul><li>Operating system</li><li>Package manager preference (pip/conda)</li><li>CUDA version (for GPU support)</li><li>Python version</li></ul><h4>GPU Support and Compatibility</h4><p>PyTorch seamlessly integrates with NVIDIA GPUs through CUDA. To verify GPU availability in your environment:</p><pre><br># Check GPU availability<br>gpu_available = torch.cuda.is_available()<br>print(f&quot;GPU Available: {gpu_available}&quot;)<br><br># Get GPU device count if available<br>if gpu_available:<br>    print(f&quot;Number of GPUs: {torch.cuda.device_count()}&quot;)</pre><p>If a GPU is detected, you can move tensors and models to GPU memory using:</p><pre># Create a tensor<br>tensor = torch.tensor([1.0, 2.0, 3.0])<br><br># Move to GPU if available<br>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;<br>tensor = tensor.to(device)</pre><h4>Apple Silicon Support</h4><p>For users with Apple M1/M2/M3 chips, PyTorch provides acceleration through the Metal Performance Shaders (MPS) backend. Verify MPS availability:</p><pre><br># Check MPS (Metal Performance Shaders) availability<br>mps_available = torch.backends.mps.is_available()<br>print(f&quot;MPS Available: {mps_available}&quot;)<br><br># If MPS is available, you can use it as device<br>if mps_available:<br>    device = torch.device(&quot;mps&quot;)<br>    # Move tensors/models to MPS device<br>    tensor = tensor.to(device)</pre><p>For ease of usage, I recommend using <a href="https://colab.research.google.com/">Google Colab</a> i.e. a popular jupyter notebook–like environment, which provides time-limited access to GPUs.</p><h3>Understanding Tensors</h3><h4>What Are Tensors?</h4><p>Tensors are mathematical objects that generalize vectors and matrices to higher dimensions. In PyTorch, tensors serve as fundamental data containers that hold and process multi-dimensional arrays of numerical values. These containers enable efficient computation and automatic differentiation, making them essential for deep learning operations. PyTorch tensors are similar to Numpy arrays in basic sense.</p><h4>Scalers, Vectors, Matrices and Tensors</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/0*HVGh8DKM7kouoN-m.png" /></figure><p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorch’s `Tensor` class using the `torch.tensor` function as shown in the following listing.</p><pre>import torch<br><br># Scalar (0-dimensional tensor)<br>scalar = torch.tensor(1)     <br><br># Vector (1-dimensional tensor)<br>vector = torch.tensor([1, 2, 3])    <br><br># Matrix (2-dimensional tensor)<br>matrix = torch.tensor([[1, 2], <br>                      [3, 4]])     <br><br># 3-dimensional tensor<br>tensor3d = torch.tensor([[[1, 2], [3, 4]], <br>                        [[5, 6], [7, 8]]])</pre><p>Each tensor type maintains its specific dimensionality, accessible through the <strong>.shape</strong> attribute:</p><pre>print(f&quot;Scalar shape: {scalar.shape}&quot;)      # torch.Size([])<br>print(f&quot;Vector shape: {vector.shape}&quot;)      # torch.Size([3])<br>print(f&quot;Matrix shape: {matrix.shape}&quot;)      # torch.Size([2, 2])<br>print(f&quot;3D tensor shape: {tensor3d.shape}&quot;) # torch.Size([2, 2, 2])</pre><h4>Tensor Data Types and Precision</h4><p>PyTorch supports various data types with different precision levels, optimized for different computational needs:</p><p>Some of the common torch datatypes available with torch are float32, float64, float16, bfloat16, int8, uint8, int16, int32, int64.</p><p>The choice of precision impacts both memory usage and computational efficiency:</p><ul><li>float32: Standard for most deep learning tasks</li><li>float16: Reduced precision, useful for memory optimization</li><li>bfloat16: Brain Floating Point, balances precision and range</li></ul><h4>Floating Data Types</h4><p>PyTorch supports various floating-point precisions for tensors, each serving different computational needs:</p><ul><li>torch.float32 (default): 32-bit precision offering 6-9 decimal places, optimal for most deep learning tasks</li><li>torch.float64: 64-bit double precision with 15-17 decimal places, suitable for high-precision numerical computations</li><li>torch.float16: 16-bit half precision with 3-4 decimal places, useful for memory-efficient operations</li><li>torch.bfloat16: Brain floating point format with 2-3 decimal precision, balancing range and precision</li></ul><pre>import torch<br><br>float32_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  <br>float64_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)  <br>float16_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16)  <br>bfloat16_tensor = torch.tensor([1.0, 2.0], dtype=torch.bfloat16)</pre><h4>Integer Types</h4><p>PyTorch supports various integer data types, each with specific memory allocations and value ranges:</p><ul><li>int8: 8-bit signed integers (-128 to 127)</li><li>uint8: 8-bit unsigned integers (0 to 255)</li><li>int16: 16-bit signed integers (-32768 to 32767)</li><li>int32: 32-bit signed integers (-2^31 to 2^31-1)</li><li>int64: 64-bit signed integers (-2^63 to 2^63-1), default integer type in PyTorch</li></ul><pre>int8_tensor = torch.tensor([1, 2], dtype=torch.int8)     <br>uint8_tensor = torch.tensor([1, 2], dtype=torch.uint8)   <br>int16_tensor = torch.tensor([1, 2], dtype=torch.int16)   <br>int32_tensor = torch.tensor([1, 2], dtype=torch.int32)   <br>int64_tensor = torch.tensor([1, 2], dtype=torch.int64)</pre><h4>Datatype Conversion</h4><p>We can convert tensors from one datatype to another using the .to method.</p><pre># Converting between data types<br>tensor = torch.tensor([1, 2, 3])<br><br>float_tensor = tensor.to(torch.float32) # Convert from int64 to float32<br>int_tensor = tensor.to(torch.int32)     # Convert from float32 to int32</pre><h3>Common Tensor Operations</h3><p>PyTorch provides several fundamental tensor operations essential for deep learning computations. Here are the key operations with their implementations and specific use cases.</p><h4>1. Tensor Creation and Shape Manipulation</h4><p>Creating tensors and understanding their shape are fundamental operations in PyTorch:</p><pre>import torch<br><br># Create 2D tensor<br>tensor2d = torch.tensor([[1, 2, 3], <br>                        [4, 5, 6]])<br><br># Check tensor shape<br>shape = tensor2d.shape  <br># Returns: torch.Size([2, 3])</pre><p>For the above tensor, the shape if 2 x 3 i.e. 2 rows and 3 columns. We can change the shape of the array by maintaining the total size of the array using reshape method.</p><h4>2. Reshaping Operations</h4><p>PyTorch offers two methods for tensor reshaping:</p><pre># Reshape tensor from (2,3) to (3,2)<br>reshaped_tensor = tensor2d.reshape(3, 2)<br><br># Alternative using view<br>viewed_tensor = tensor2d.view(3, 2)</pre><p><strong>Technical Note</strong>: .view() and .reshape() differ in memory handling:</p><ul><li>.view(): Requires contiguous memory layout</li><li>.reshape(): Works with any memory layout, performs copy if necessary</li></ul><h4>3. Matrix Operations</h4><p>PyTorch implements efficient matrix operations essential for linear algebra computations:</p><pre># Transpose operation<br>transposed = tensor2d.T<br><br># Matrix multiplication methods<br>result1 = tensor2d.matmul(tensor2d.T)  # Using matmul<br>result2 = tensor2d @ tensor2d.T        # Using @ operator</pre><p>Output shapes for a 2x3 input tensor:</p><ul><li>Transpose: 3x2</li><li>Matrix multiplication with transpose: 2x2</li></ul><p>These operations form the foundation for neural network computations and linear algebra operations in deep learning models. For an exhaustive list of tensor operations, refer to the <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>.</p><h3>Automatic Differentiation</h3><h4>Understanding Computational Graphs</h4><p>PyTorch builds computational graphs that track operations performed on tensors. These graphs enable automatic differentiation through the autogradsystem, making gradient computation efficient and programmatic.</p><p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network — we will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p><p>Consider the following example of a single layer neural network performing logistic regression with single weight and bias.</p><pre>import torch<br>import torch.nn.functional as F<br><br># Initialize inputs and parameters<br>y = torch.tensor([1.0])           # Target<br>x1 = torch.tensor([1.1])          # Input<br>w1 = torch.tensor([2.2], <br>                  requires_grad=True)  # Weight<br>b = torch.tensor([0.0], <br>                 requires_grad=True)   # Bias<br><br># Forward pass computation<br>z = x1 * w1 + b                   # Linear computation<br>a = torch.sigmoid(z)              # Activation<br>loss = F.binary_cross_entropy(a, y)    # Loss computation</pre><p>We have used the torch.nn.functional module from torch which provides many utility functions like loss functions, activations etc required to write and train deep neural networks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*e_agaazNkZeE48Ie" /></figure><p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs from Scratch</em></a></p><h4>Gradient Computation with Autograd</h4><p>To train the above model, we have to compute the gradients of loss w.r.t w1 and bwhich will be further used to update the existing weights iteratively. This is where PyTorch makes our life easier by automatically calculating them using the autograd engine.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cA24CuRSLTFFncr7" /></figure><p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs from Scratch</em></a></p><p>PyTorch’s autograd system automatically computes gradients for all tensors with requires_grad=True. Here&#39;s how to compute gradients:</p><pre>from torch.autograd import grad<br><br># Manual gradient computation<br>grad_L_w1 = grad(loss, w1, retain_graph=True)<br>grad_L_b = grad(loss, b, retain_graph=True)<br><br># Alternative using backward()<br>loss.backward()<br>print(w1.grad)    # Access gradient for w1<br>print(b.grad)     # Access gradient for b</pre><p><strong>Technical Note</strong>: When using backward():</p><ul><li>Gradients accumulate by default</li><li>Use zero_grad() before each backward pass in training loops</li><li>retain_graph=True allows multiple backward passes</li></ul><p>The grad function is used to get gradients manually and it is useful for debugging and demonstration purposes. Using the backward() function automatically calculates for all the tensors which has requires_grad=True set and gradients will be stored inside .grad property.</p><h3>Building Neural Networks with PyTorch</h3><p>Next, we focus on PyTorch as a library for implementing deep neural networks. While our previous example demonstrated a single neuron for classification, practical applications require complex architectures like transformers and ResNets that process multiple inputs through various hidden layers to produce outputs. Manually calculating and updating individual weights becomes impractical at this scale. PyTorch provides a structured approach through its neural network modules, enabling efficient implementation of sophisticated architectures.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/860/0*USeS0rqcvEQiAYsC" /></figure><p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs from Scratch</em></a></p><h4>Introduction to torch.nn.Module</h4><p>The torch.nn.Module serves as PyTorch&#39;s foundational class for neural networks, providing a systematic way to define and manage model architectures, parameters, and computations. This base class handles essential functionalities including parameter management, device placement, and training behaviors.</p><p>The subclass has the following components:</p><ul><li>__init__: We define the layers of neural networks in the constructor of the subclass defined and how the layers interact during forward propagation.</li><li>forward: The forward method describes how the input data passes through the network and comes together as a computation graph.</li></ul><h4>Creating Custom Neural Network Architectures</h4><p>Complex neural networks require multiple layers with specific activation functions. Here’s a practical implementation of a multi-layer neural network:</p><pre>class DeepNetwork(nn.Module):<br>    def __init__(self, num_inputs, num_outputs):<br>        super().__init__()<br>        <br>        self.layers = nn.Sequential(<br>            nn.Linear(num_inputs, 30),     # First hidden layer<br>            nn.ReLU(),                     # Activation function<br>            nn.Linear(30, 20),             # Second hidden layer<br>            nn.ReLU(),                     <br>            nn.Linear(20, num_outputs)      # Output layer<br>        )<br><br>    def forward(self, x):<br>        return self.layers(x)</pre><p>nn.Sequential provides a container for stacking layers in a specific order, streamlining the forward pass implementation.</p><h4>Model Parameters and Initialization</h4><p>PyTorch automatically handles parameter initialization, but you can access and modify parameters:</p><pre>model = DeepNetwork(50, 3)<br><br># Count trainable parameters<br>num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)<br>print(f&quot;Trainable parameters: {num_params}&quot;)<br><br># Access layer parameters<br>for name, param in model.named_parameters():<br>    print(f&quot;Layer: {name} | Size: {param.size()}&quot;)<br><br># Custom initialization<br>def init_weights(m):<br>    if isinstance(m, nn.Linear):<br>        torch.nn.init.xavier_uniform_(m.weight)<br>        m.bias.data.fill_(0.01)<br><br>model.apply(init_weights)</pre><p>Each parameter for which requires_grad=True counts as a trainable parameter and will be updated during training. In the above code, this referes to the weights initialized in torch.nn.Linear layers.</p><h4>Forward Propagation Implementation</h4><p>Forward propagation defines how input data flows through the network. Let’s initialise random values and pass it through the model.</p><pre># Sample forward pass<br>model = DeepNetwork(50, 3)<br>batch_size = 32<br>input_features = torch.randn(batch_size, 50)<br><br>with torch.no_grad():<br>    outputs = model(input_features)<br><br>print(f&quot;Output shape: {outputs.shape}&quot;)</pre><h4>Training Mode vs. Evaluation Mode</h4><p>PyTorch models have distinct training and evaluation modes that affect certain layers’ behavior:</p><pre>model = DeepNetwork(50, 3)<br><br># Training mode<br>model.train()<br>training_output = model(input_features)  <br># Layers like Dropout and BatchNorm active<br>print(training_output)<br><br># Evaluation mode<br>model.eval()<br>with torch.no_grad():<br>    eval_output = model(input_features)  # Deterministic behavior<br>    print(eval_output)</pre><p>PyTorch models operate in two distinct modes:</p><ol><li>Training Mode (model.train()):</li></ol><ul><li>Activates Dropout and BatchNorm layers</li><li>Enables gradient computation and tracking</li><li>Maintains computational graph for backpropagation</li></ul><p>2. Evaluation Mode (model.eval() with torch.no_grad()):</p><ul><li>Disables Dropout and freezes BatchNorm statistics</li><li>Prevents gradient computation and tracking</li><li>Optimizes memory usage by eliminating gradient storage</li><li>Reduces computational overhead during inference</li></ul><p>This mode management ensures efficient resource utilization while maintaining appropriate model behavior for both training and inference phases.</p><h3>Efficient Data Handling</h3><p>Efficient data handling is crucial for developing robust deep learning models. PyTorch provides two primary tools for data management: the Dataset and DataLoader classes.</p><h4>1. Dataset and DataLoader Overview</h4><p>PyTorch’s data handling framework consists of</p><ul><li>Dataset: Defines data access and preprocessing</li><li>DataLoader: Handles batch creation, shuffling, and parallel loading</li></ul><p>Let’s implement a simple classification dataset to demonstrate these concepts:</p><pre>import torch<br>from torch.utils.data import Dataset, DataLoader<br><br># Training classification data<br>X_train = torch.tensor([<br>    [-1.2, 3.1],<br>    [-0.9, 2.9],<br>    [-0.5, 2.6],<br>    [2.3, -1.1],<br>    [2.7, -1.5]<br>])<br><br>y_train = torch.tensor([0, 0, 0, 1, 1])<br><br># Testing dataset<br>X_test = torch.tensor([<br>    [-0.8, 2.8],<br>    [2.6, -1.6],<br>])<br><br>y_test = torch.tensor([0, 1])</pre><h4>2. Creating Custom Dataset Objects</h4><p>Next, we create a custom dataset class, SampleDataset, by subclassing from PyTorch’s Dataset parent class. It has following properties:</p><ul><li>__init__: Initialize dataset attributes.</li><li>__getitem__: Define data access for individual samples</li><li>__len__: Return total number of samples</li></ul><pre>from torch.utils.data import Dataset<br><br>class SampleDataset(Dataset):<br>    def __init__(self, X, y):<br>    &quot;&quot;&quot;Initialize the dataset with features and labels&quot;&quot;&quot;<br>        self.features = X<br>        self.labels = y<br>    def __getitem__(self, index):<br>    &quot;&quot;&quot;Retrieve a single example and its label&quot;&quot;&quot;     <br>        one_x = self.features[index]     <br>        one_y = self.labels[index]       <br>        return one_x, one_y              <br>    def __len__(self):<br>    &quot;&quot;&quot;Get the total number of examples in the dataset&quot;&quot;&quot;<br>        return self.labels.shape[0]     <br><br>train_ds = SampleDataset(X_train, y_train)<br>test_ds = SampleDataset(X_test, y_test)</pre><h4>3. Implementing DataLoader</h4><p>DataLoaders handle the heavy lifting of batching, shuffling, and parallel data loading. Now we can create DataLoaders from the SampleDataset object created. This can be done as follows:</p><pre># Create DataLoader with specific configurations<br>train_loader = DataLoader(<br>    dataset=train_ds,     # Dataset Instance<br>    batch_size=2,         # Number of samples per batch<br>    shuffle=True,         # Shuffle the training data<br>    num_workers=0         # Number of parallel workers<br>    drop_last=True.       # Drop incomplete batch<br>)<br><br>test_loader = DataLoader(<br>    dataset=test_ds,<br>    batch_size=2,<br>    shuffle=False,        # No need to shuffle test data<br>    num_workers=0<br>)</pre><p>Some key parameters of Dataloaders class are as follows:</p><ul><li>dataset: The Dataset instance to load data from</li><li>batch_size: Number of samples per batch</li><li>shuffle: Whether to shuffle data between epochs</li><li>num_workers: Number of subprocesses for data loading</li><li>drop_last: Whether to drop the last incomplete batch</li><li>pin_memory: Pin memory for faster data transfer to GPU</li></ul><h4>Complete Example with Best Practices (Best Practice)</h4><p>Here’s a comprehensive implementation incorporating all concepts:</p><pre>import torch<br>from torch.utils.data import Dataset, DataLoader<br><br>class SampleDataset(Dataset):<br>    def __init__(self, X, y, transform=None):<br>        self.features = X<br>        self.labels = y<br>        self.transform = transform # Input transformations if required<br>    <br>    def __getitem__(self, index):<br>        x = self.features[index]<br>        y = self.labels[index]<br>        <br>        if self.transform:<br>            x = self.transform(x)<br>            <br>        return x, y<br>    <br>    def __len__(self):<br>        return len(self.labels)<br><br># Configuration for optimal performance<br>def create_data_loader(dataset, batch_size, is_training=True):<br>    return DataLoader(<br>        dataset=dataset,<br>        batch_size=batch_size,<br>        shuffle=is_training,<br>        num_workers=4 if is_training else 2,<br>        pin_memory=torch.cuda.is_available(),<br>        drop_last=is_training,<br>        persistent_workers=True<br>    )<br><br><br># Usage example<br>if __name__ == &quot;__main__&quot;:<br>    # Create dataset<br>    dataset = SampleDataset(X_train, y_train)<br>    <br>    # Create data loader<br>    train_loader = create_data_loader(<br>        dataset=dataset,<br>        batch_size=32,<br>        is_training=True<br>    )<br>    <br>    # Training loop example<br>    for epoch in range(num_epochs):<br>        for batch_idx, (features, labels) in enumerate(train_loader):<br>            # Training operations here<br>            pass</pre><p>This implementation provides a robust foundation for handling data in PyTorch, incorporating best practices for memory management and parallel processing. Adjust the configurations based on your specific use case and available computational resources.</p><h3>Implementing Training Loops in PyTorch</h3><p>A PyTorch training loop consists of several key components:</p><ul><li>model initialization,</li><li>optimizer configuration,</li><li>loss function definition, and</li><li>the iterative training process.</li></ul><p>Here’s a structured implementation showcasing these elements.</p><h4>Basic Training Loop Structure (Best Practice)</h4><pre>import torch<br>import torch.nn as nn<br>from torch.optim import Adam<br>from torch.utils.data import DataLoader<br><br>def train_model(<br>    model: nn.Module,<br>    train_loader: DataLoader,<br>    val_loader: DataLoader,<br>    num_epochs: int,<br>    learning_rate: float,<br>    device: str = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;<br>) -&gt; dict:<br>    &quot;&quot;&quot;<br>    Trains a PyTorch neural network model using the provided data loaders.<br>    <br>    This function implements a standard training loop with validation after each epoch.<br>    It supports both CPU and GPU training and tracks various metrics during training.<br>    <br>    Args:<br>        model (nn.Module): The PyTorch model to train<br>        train_loader (DataLoader): DataLoader for training data<br>        val_loader (DataLoader): DataLoader for validation data<br>        num_epochs (int): Number of training epochs<br>        learning_rate (float): Learning rate for the optimizer<br>        device (str): Device to train on (&#39;cuda&#39; or &#39;cpu&#39;), defaults to GPU if available<br>        <br>    Returns:<br>        dict: Training history containing:<br>            - train_loss: List of training losses per epoch<br>            - val_loss: List of validation losses per epoch<br>            - val_accuracy: List of validation accuracies per epoch<br>    &quot;&quot;&quot;<br>    <br>    # Initialize Adam optimizer and Cross Entropy Loss criterion<br>    optimizer = Adam(model.parameters(), lr=learning_rate)<br>    criterion = nn.CrossEntropyLoss()<br>    <br>    # Move model to appropriate device (GPU/CPU)<br>    model = model.to(device)<br>    <br>    # Dictionary to store training metrics<br>    history = {<br>        &#39;train_loss&#39;: [],<br>        &#39;val_loss&#39;: [],<br>        &#39;val_accuracy&#39;: []<br>    }<br>    <br>    # Main training loop over epochs<br>    for epoch in range(num_epochs):<br>        # Set model to training mode (enables dropout, batch norm, etc.)<br>        model.train()<br>        train_loss = 0.0<br>        <br>        # Iterate over training batches<br>        for batch_idx, (features, labels) in enumerate(train_loader):<br>            # Transfer batch data to appropriate device<br>            features = features.to(device)<br>            labels = labels.to(device)<br>            <br>            # Forward propagation<br>            outputs = model(features)<br>            loss = criterion(outputs, labels)<br>            <br>            # Backward propagation and optimization<br>            optimizer.zero_grad()  # Clear previous gradients<br>            loss.backward()        # Compute gradients<br>            optimizer.step()       # Update model parameters<br>            <br>            # Accumulate batch loss<br>            train_loss += loss.item()<br>            <br>            # Print progress every 100 batches<br>            if batch_idx % 100 == 0:<br>                print(f&#39;Epoch: {epoch+1}/{num_epochs} | &#39;<br>                      f&#39;Batch: {batch_idx}/{len(train_loader)} | &#39;<br>                      f&#39;Loss: {loss.item():.4f}&#39;)<br>        <br>        # Calculate average training loss for the epoch<br>        train_loss = train_loss / len(train_loader)<br>        history[&#39;train_loss&#39;].append(train_loss)<br>        <br>        # Validation phase<br>        model.eval()  # Set model to evaluation mode<br>        val_loss = 0.0<br>        correct = 0<br>        total = 0<br>        <br>        # Disable gradient computation for validation<br>        with torch.no_grad():<br>            for features, labels in val_loader:<br>                features = features.to(device)<br>                labels = labels.to(device)<br>                <br>                # Forward pass only<br>                outputs = model(features)<br>                loss = criterion(outputs, labels)<br>                <br>                # Accumulate validation metrics<br>                val_loss += loss.item()<br>                _, predicted = torch.max(outputs.data, 1)  # Get predicted class<br>                total += labels.size(0)                    # Count total samples<br>                correct += (predicted == labels).sum().item()  # Count correct predictions<br>        <br>        # Calculate validation metrics<br>        val_loss = val_loss / len(val_loader)<br>        val_accuracy = 100 * correct / total<br>        <br>        # Store validation metrics<br>        history[&#39;val_loss&#39;].append(val_loss)<br>        history[&#39;val_accuracy&#39;].append(val_accuracy)<br>        <br>        # Print epoch summary<br>        print(f&#39;Epoch: {epoch+1}/{num_epochs} | &#39;<br>              f&#39;Train Loss: {train_loss:.4f} | &#39;<br>              f&#39;Val Loss: {val_loss:.4f} | &#39;<br>              f&#39;Val Accuracy: {val_accuracy:.2f}%&#39;)<br>    <br>    return history<br><br><br># Example Usage<br>def main():<br>    &quot;&quot;&quot;<br>    Example implementation of the training pipeline.<br>    <br>    This function demonstrates how to use the train_model function<br>    with a custom model and configuration.<br>    &quot;&quot;&quot;<br>    # Initialize model (assumed to be defined elsewhere)<br>    model = DeepNetwork()<br>    <br>    # Define training hyperparameters<br>    config = {<br>        &#39;num_epochs&#39;: 10,     # Number of training epochs<br>        &#39;learning_rate&#39;: 0.001,  # Learning rate for optimization<br>    }<br>    <br>    # Train the model with specified configuration<br>    history = train_model(<br>        model=model,<br>        train_loader=train_loader,  # Assumed to be defined elsewhere<br>        val_loader=test_loader,     # Assumed to be defined elsewhere<br>        num_epochs=config[&#39;num_epochs&#39;],<br>        learning_rate=config[&#39;learning_rate&#39;]<br>    )</pre><p>The training loop does the following gradient descent as following:</p><ul><li>The training process involves passing logits to the cross_entropy loss function, which internally applies softmax for optimized performance and numerical stability.</li><li>The loss.backward() call computes gradients through PyTorch&#39;s computational graph</li><li>The optimizer.step() step updates the model parameters using these gradients</li><li>The optimizer.zero_grad() must be called every training iteration to reset gradients, preventing unintended accumulation that could distort the optimization process.</li></ul><h3>Model Persistence in PyTorch: Saving and Loading</h3><p>PyTorch provides efficient mechanisms for model persistence through its state dictionary system. The state dictionary (state_dict) maintains a mapping between layer identifiers and their corresponding parameters (weights and biases).</p><h3>Basic Model Persistence</h3><h4>Saving Models</h4><p>After training the model, it is necessary to save the model weights to reuse later for further training or deployment. Save a model’s learned parameters using the state dictionary:</p><pre>import torch<br><br># Save model parameters<br>torch.save(model.state_dict(), &quot;model_parameters.pth&quot;)</pre><h4>Loading Models</h4><p>The torch.load(&quot;model_parameters.pth&quot;) function reads the file &quot;model_parameters.pth&quot; and reconstructs the Python dictionary object containing the model’s parameters while model.load_state_dict() applies these parameters to the model, effectively restoring its learned state from when we saved it.</p><p>We need the instance of the model in memory to apply the saved parameters. Here, the NeuralNetwork(2, 2) architecture needs to match the original saved model exactly.</p><pre># Initialize model architecture<br>model = NeuralNetwork(num_inputs=2, num_outputs=2)<br><br># Load saved parameters<br>model.load_state_dict(torch.load(&quot;model_parameters.pth&quot;))</pre><h3>Comprehensive Model Persistence (Best Practice)</h3><p>For production scenarios, save additional information alongside model parameters:</p><pre># Save complete model state<br>checkpoint = {<br>    &#39;model_state_dict&#39;: model.state_dict(),<br>    &#39;optimizer_state_dict&#39;: optimizer.state_dict(),<br>    &#39;epoch&#39;: epoch,<br>    &#39;loss&#39;: loss,<br>    &#39;model_config&#39;: {<br>        &#39;num_inputs&#39;: 2,<br>        &#39;num_outputs&#39;: 2<br>    }<br>}<br>torch.save(checkpoint, &quot;model_checkpoint.pth&quot;)<br><br><br># Load complete model state<br>checkpoint = torch.load(&quot;model_checkpoint.pth&quot;)<br>model = NeuralNetwork(**checkpoint[&#39;model_config&#39;])<br>model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])<br>optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])<br>epoch = checkpoint[&#39;epoch&#39;]<br>loss = checkpoint[&#39;loss&#39;]</pre><h3>Conclusion</h3><p>PyTorch’s architecture provides a robust foundation for deep learning development through its integrated components: tensor computations, automatic differentiation, and neural network modules. The framework’s design enables efficient model implementation through dynamic computation graphs, GPU acceleration, and intuitive APIs for data processing and model construction.</p><p>For continued learning and implementation guidance, refer to PyTorch’s official documentation which provides comprehensive updates on best practices, optimizations, and emerging capabilities. This ensures your deep learning applications remain aligned with current framework standards and performance benchmarks.</p><p>Want more AI/ML deep dives? Subscribe <a href="https://neuraforge.substack.com">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=06b66a651f1f" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Here’s how you should train an intelligent classifier model..]]></title>
            <link>https://narasimhakarthik.medium.com/heres-how-you-should-train-an-intelligent-classifier-model-be705c001fb2?source=rss-c115716f79b5------2</link>
            <guid isPermaLink="false">https://medium.com/p/be705c001fb2</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[computer-vision]]></category>
            <dc:creator><![CDATA[Narasimha Karthik Jwalapuram]]></dc:creator>
            <pubDate>Thu, 02 Dec 2021 03:16:00 GMT</pubDate>
            <atom:updated>2021-12-02T08:27:17.357Z</atom:updated>
            <content:encoded><![CDATA[<h3>Training an unknown label detecting classifier using fastai</h3><h4>Unknow labels detection: Detecting out of domain data i.e. classifying images only it is trained for.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*b40tp_HwIBKNDleBihKUlA.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@seanpollock?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Sean Pollock</a> on <a href="https://unsplash.com/s/photos/work?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><h3>Motivation</h3><p>I have discussed in my previous blog post how we can train an out-of-domain input detecting image classifier using a multi-label classification approach. In this blog post let’s discuss how to train a regular multi-class classifier but make it some more intelligent i.e giving it the ability to detect data that it doesn’t know about. We are using the fast.ai library to create and train the model.</p><h3>Explore Dataset</h3><p>Let’s approach the problem by using the PETs dataset. This dataset has 37 categories of different pet breeds with nearly 200 images for each class. More details about this dataset can be found <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">here</a>.</p><p>Let’s get started by installing and importing the required libraries.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/77d0e843ee1955240821222503e0a366/href">https://medium.com/media/77d0e843ee1955240821222503e0a366/href</a></iframe><p>Now let’s download the PETs dataset,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b57ae0601407b155e4cddf0c8f3c722f/href">https://medium.com/media/b57ae0601407b155e4cddf0c8f3c722f/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZGn4tl7-_oEKg_I80hj-cQ.png" /><figcaption>Output</figcaption></figure><p>Since we have downloaded the dataset, let’s build a datablock object and dataloaders.</p><h3>Getting the data ready</h3><p>DataBlock is a high-level API that is used to easily build and load the data i.e. create dataloaders that can directly be served to model. This makes data loading a lot easier and more customizable. Now, let’s create a datablock object according to our dataset.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c1bebe37d2149201c8f40ac58b78f2ff/href">https://medium.com/media/c1bebe37d2149201c8f40ac58b78f2ff/href</a></iframe><p>Now since the datablock object is ready, let’s see in some detail what it is doing,</p><ul><li><strong>blocks:</strong> This parameter defines the input and target types of the data that we are providing to the model. In the above code, <strong>ImageBlock</strong> represents the input type as an image and <strong>MultiCategoryBlock </strong>represents the output type as a one-hot-encoded array whereas the conventional <strong>CategoryBlock </strong>which is used for image classification uses ordinal encoding of targets.</li><li><strong>get_items:</strong> This parameter defines how to get the inputs. We use the fastai’s util function <strong>get_image_files </strong>which takes a <strong>path</strong> <strong>to a folder </strong>as a parameter and returns paths to all the images present in the given path. Since we have defined the input block as ImageBlock, datablock will be able to convert the image paths into PIL Image.</li><li><strong>get_y: </strong>This parameter defines how to get the targets. We have seen an example of the image path which we have downloaded. Each image file name has a pattern i.e. ‘breed_name’_XXX.jpg where X=0–9. To get the breed name we can use <strong>regex</strong> to extract it from the path string and convert it to a list. We are using fastai’s <strong>Pipeline</strong> API to create a function pipeline.</li><li><strong>splitter:</strong> This parameter defines how the train-validation split is being done. We are using <strong>RandomSplitter </strong>to<strong> </strong>perform random splitting of the dataset with 20% for validation (default). We are setting random seed as 42 to ensure we get the same validation set for every run.</li><li><strong>item_tfms:</strong> This parameter defines the transforms to be applied to each item. We are using the <strong>Resize </strong>transform with <strong>size=460</strong> that resizes each image to size 460.</li><li><strong>batch_tfms:</strong> This parameter defines the transforms to be applied to each batch of data. Since this is being applied on batches of data, the operation runs on GPU (if available) to make it faster. In the above code, we are using the fastai’s <strong>aug_tranforms </strong>augmentation function which applies basic data augmentations like flip, change contrast, brightness, resize etc.</li></ul><p>When the <strong>size</strong> and <strong>min_scale </strong>arguments are given, the <strong>aug_transform</strong> function randomly crops the image to the given size by retaining at least some minimum amount of image data every epoch, which is specified by the <strong>min_scale</strong> argument.</p><p>This datablock object accepts the <strong>path</strong> to the directory containing images/data as a parameter while loading dataloaders. So now let’s create dataloaders and display some image samples,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/aa483c8af1254c33faed24455f8adf6d/href">https://medium.com/media/aa483c8af1254c33faed24455f8adf6d/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mbJ56qdfS8dr9sGuR4Obig.png" /><figcaption>Output</figcaption></figure><p>Let’s also look at some of the targets,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d73be6db03197763a8901c68c22102f4/href">https://medium.com/media/d73be6db03197763a8901c68c22102f4/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HoPKuF8TL71IIfLJqt5LZQ.png" /><figcaption>Output</figcaption></figure><p>We can see that the target is a one-hot-encoded array with a total of 37 classes. You can check all the labels by dls.vocab .</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tNPzuwVyO5fzwOt4PD0XJA.png" /><figcaption>List of all the 37 classes</figcaption></figure><p>Now let’s use a pre-trained model and fine-tune it with our data.</p><h3>Model Training</h3><p>Let’s create a learner object using the <strong>resnet50</strong> pre-trained model. Since we are using the multi-label classification method to detect unknown labels, we need to change the loss function as well. So we should use the binary cross-entropy loss function or fastai’s BCEWithLogitsLossFlat() loss function with the default threshold value. We should use <strong>accuracy_multi </strong>with a higher threshold as a metric to ensure only the label with the highest probability is chosen.</p><p>To reduce the model size and training time, we are using the half-precision training method by converting all the weights in the model to 16-bit floats. For achieving this to_fp16() is used.</p><p>Now let’s look at the code,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bee6e52079f68eb2d29705daee86cc09/href">https://medium.com/media/bee6e52079f68eb2d29705daee86cc09/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/994/1*KVi-4HExlUzo8HdXUz7MDQ.png" /><figcaption>Output- Learning rate finder</figcaption></figure><p>Since we have found the optimal learning rate to use, let’s fine-tune the model for 3 epochs.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ae3459f0d6913501ba1453761bc7cb57/href">https://medium.com/media/ae3459f0d6913501ba1453761bc7cb57/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/826/1*GyWnWHWwDT7RCGSi1pELYw.png" /><figcaption>Output-Model training</figcaption></figure><p>We can see that within 3 epochs of training, our model has got 99% accuracy… This is a pretty impressive result, thanks to the pre-trained resnet50 model. Let’s also look at the loss plot,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5b0c20ae1fd6d5d26140c07d86d2c8c8/href">https://medium.com/media/5b0c20ae1fd6d5d26140c07d86d2c8c8/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/956/1*nGBHibFw1JwaAnChnPIXfg.png" /><figcaption>Output-Loss Plot</figcaption></figure><p>Okay, we can see that the training and validation loss has reduced gradually and the model is not overfitting.</p><p>Now let’s proceed to model inference, where we can test the unknown label detection in action…</p><h3>Inference</h3><p>Before inference, let’s update the threshold of our BCEWithLogitsLossFlat() loss function to 0.95. Since the loss function we are using has the <strong>sigmoid</strong> activation, we should increase the threshold to enable the detection of labels that the model is highly confident about.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f050eaf1eb433c12c0ec9c0c6f2e5e69/href">https://medium.com/media/f050eaf1eb433c12c0ec9c0c6f2e5e69/href</a></iframe><p>Now let’s check some results,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/08e9243d9a42ff98a3bc6948d8c61b38/href">https://medium.com/media/08e9243d9a42ff98a3bc6948d8c61b38/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gFBCCJQWHQJxwVsMPDV1vA.png" /><figcaption>Output: Inference Results</figcaption></figure><p>We can see from the above results that our model is doing good on the validation set.</p><p>Now let’s do the model inference manually i.e. download some images from the internet and check if the results are matching...</p><p>To test the positive case, I have downloaded an image of ‘Bombay cat’ from google images to the notebook location and created a PIL Image object,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/96594a0910b0408bf2683b3d1e4d827f/href">https://medium.com/media/96594a0910b0408bf2683b3d1e4d827f/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/664/1*H1R7QKAM91zzI-t_lG5bMw.png" /><figcaption>Output: Bombay Cat Image.</figcaption></figure><p>Now let’s predict what our model will predict,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f25b135a6c6fbda99c38d31ada57e865/href">https://medium.com/media/f25b135a6c6fbda99c38d31ada57e865/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*axpKzczsBOQhj2RnxEWGfQ.png" /><figcaption>Output for prediction</figcaption></figure><p>Yay... Our model correctly predicted that the class is ‘Bombay’. We can also see that the probability for this class is 0.99… which is pretty high.</p><p>Now let’s try to find predictions for some out of domain data. I have downloaded an image of the Eiffel tower from google images and used it for prediction…</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8763b828ec4c83d60c606c5789a0482c/href">https://medium.com/media/8763b828ec4c83d60c606c5789a0482c/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6SvPhuWEeLGWjmQttSCZjw.png" /><figcaption>Output: Eiffle Tower Image</figcaption></figure><p>Now if we try to find predictions for this image, our model should return negative for all the classes. Let’s find the prediction and check…</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/63abd2604bedd1969603298921444b6b/href">https://medium.com/media/63abd2604bedd1969603298921444b6b/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jwH3R8EILcVNxP04hd8GXg.png" /><figcaption>Output: Prediction for tower image</figcaption></figure><p>Great!! we can see that our model was able to understand that this image doesn’t belong to any dog or cat breed it was trained for i.e. 37 breeds of pets and returned <strong>False</strong> for all the available classes.</p><h3>Conclusion</h3><p>So from the above results, we can see that using the multi-label classification technique to train a multi-class classification model is the best way to train an intelligent image classifier.</p><p>Thank you</p><h4>Sources:</h4><ol><li><em>Deep Learning for Coders with fastai and PyTorch, </em>Book by Howard &amp; Gugger.</li><li><a href="http://walkwithfastai.com"><em>Walk with fastai</em></a></li></ol><h4>You can connect with me on LinkedIn <a href="https://www.linkedin.com/in/narasimhakarthik/">here</a>. GitHub link for the notebook can be accessed <a href="https://github.com/JNK234/100-days-of-deep-learning/blob/main/Day%205/Detecting_Unknown_labels.ipynb">here</a>.</h4><p><a href="https://medium.com/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">Mlearning.ai Submission Suggestions</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=be705c001fb2" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why Multi-label classification should be used instead of conventional classifiers.]]></title>
            <link>https://narasimhakarthik.medium.com/approaching-multi-label-image-classification-using-fastai-515a4fd52c8c?source=rss-c115716f79b5------2</link>
            <guid isPermaLink="false">https://medium.com/p/515a4fd52c8c</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[computer-vision]]></category>
            <category><![CDATA[fastai]]></category>
            <category><![CDATA[pytorch]]></category>
            <dc:creator><![CDATA[Narasimha Karthik Jwalapuram]]></dc:creator>
            <pubDate>Mon, 29 Nov 2021 02:05:46 GMT</pubDate>
            <atom:updated>2021-11-29T08:59:29.334Z</atom:updated>
            <content:encoded><![CDATA[<h3>Approaching Multi-label image classification using fastai</h3><h4>Why Multi-label classification should be used instead of conventional classification techniques.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CwNf11nyV0r98T96CrFeIw.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@pjpavel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Pavel Pjatakov</a> on <a href="https://unsplash.com/t/interiors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><h3>Motivation</h3><p>We have seen some single label classifiers i.e. models which try to assign a single label to the image provided. This can be a cat vs dog classifier or handwritten digits classifier, but at the end of the day, the single-label classifier tries to assign a label to the image provided. This type of classifier does the job of providing an output label perfectly for the given input sample.</p><p>But this might become an issue when we use the above model in production settings i.e. we have deployed a web app with a dog vs cat classifier and it is trained up to a good amount of accuracy. But if some users start uploading images of buildings or any non-cat or non-dog images, our model doesn’t understand that the input image is not a dog or cat. It provides the prediction of the image based on the max probability of labels it is trained on. So the model can only tell that the image <strong>might be a dog or cat</strong> and cannot tell <strong>it is not a dog or cat</strong>.</p><p>Another issue with single-label classifiers is that they can only assign a single label to the image. But if our image consists of multiple objects, this model will classify according to the most dominant object present in the image and ignore others. Hence this doesn’t lead to accurate classification.</p><p>The solution for these issues can be addressed by using <strong>Multi-Label Classification.</strong></p><h3>Introduction to Multi-Label Classification</h3><p>Multi-label classification can be used to assign one or more labels to each input sample (image, text, audio…). This solves the drawbacks of single-label classifier in the following ways:</p><ul><li>We can train the multi-label classifier to identify images that don’t belong to any category by just having some images that don’t have any label in the training data. In this way, a multi-label classifier will classify any out-of-domain data correctly.</li><li>Multi-label classification is a technique used to assign more than one label to a given input sample. The data can be a story/movie with multiple genres or a single image having different objects in it.</li></ul><p>Let’s see in action on how a multi-label dataset looks like and how training a multi-label classifier is different from a single-label classifier.</p><h3>Explore Dataset</h3><p>For better understanding let’s use the PASCAL dataset, which has more than one kind of classified object per image. Before that first, let’s install and import fastai,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/13bbc40dd441bcda1e36eea2143a5d3b/href">https://medium.com/media/13bbc40dd441bcda1e36eea2143a5d3b/href</a></iframe><p>Now let’s download the dataset using the untar_data function,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6ebd820d002c848fda07e4a1961e3fa5/href">https://medium.com/media/6ebd820d002c848fda07e4a1961e3fa5/href</a></iframe><p>In the downloaded path, we can see that we have train and test folders containing images along with a train.csv file telling us what labels to use for each image. Let’s inspect it using pandas,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/331a74e38570e0536aa24cfde6e79bcd/href">https://medium.com/media/331a74e38570e0536aa24cfde6e79bcd/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*fhkRT9ed015CmQ66KPcHFA.png" /></figure><p>In the train data frame, we can see that it has 3 columns with filename, labels and is_valid.</p><p>labels column consisting of one or more categories is separated by a space-delimited string. is_valid is used to determine whether it is used for validation or training.</p><h3>Getting the data ready</h3><p>Now let’s create a datablock object to load the data and create dataloaders. DataBlock is a high-level API that makes data preprocessing and loading easier. Let’s see how to construct one for our application.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2cd070000e8d41198e56c03a30a745e9/href">https://medium.com/media/2cd070000e8d41198e56c03a30a745e9/href</a></iframe><p>Ok now let’s explore each argument passed to the datablock object and see what it does.</p><ul><li><strong>blocks:</strong> This defines the independent and dependent variable/data or more precisely the type of inputs and the targets which we are providing to our model. In this case, we have images as inputs and multiple categories associated with it, hence ImageBlock and MultiCategoryBlock.</li></ul><p>The fastai library automatically detects multiple categories and performs one-hot encoding depending on the number of targets available. (But when CategoryBlock is used, ordinal encoding of categories is done i.e for single-label classification)</p><ul><li><strong>get_x: </strong>This defines how to get the inputs i.e. images in our case. In the above code, we have defined a function <strong>get_x </strong>that returns the path object to the corresponding file name. The ImageBlock reads the path object and then performs necessary processing to convert it to a PIL Image object.</li><li><strong>get_y: </strong>This<strong> </strong>defines how to get the targets i.e. multiple categories for each image. In the <strong>get_y</strong> function above, we are returning the list of labels associated with that particular image that is obtained by splitting the labels column.</li><li><strong>splitter: </strong>This defines how we are performing the train-validation split of our dataset. In the <strong>splitter </strong>function, we are using splitting the data frame based on the ‘is_valid’ column and then returning the list of all the indices as training and validation sets.</li><li><strong>item_tfms: </strong>This defines the transforms that should be applied to every sample in the dataset. Here we are using RandomResizedCrop transform which randomly resizes the image to size 128 by keeping at least 35% of the image every time it applies the transform.</li></ul><p>Now let’s create dataloaders and visualize our dataset,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6be748b24149c7dd8b767b73b82fd538/href">https://medium.com/media/6be748b24149c7dd8b767b73b82fd538/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ciugl0Xe5hnPhGtS6vZstA.png" /></figure><p>Since now we have the dataloaders ready, we can now create a model.</p><h3>Model Training</h3><p>Now let’s create a cnn_learner object using the resnet50 pretrained model. But before that let’s understand important differences in the training methods.</p><h4><strong>Loss Function</strong></h4><p>First, let’s discuss some basics of single-label classification. In a binary classification model, the task is to classify any given input into one of two classes. So the loss can be found by applying the sigmoid activation function and passing it through BinaryCrossEntropy loss function. The sigmoid is responsible for converting all values into a range of [0–1].</p><p>Since we have all the logits in the range [0–1], we can use the threshold of 0.5 and determine the class of the input sample.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HXCBO-Wx5XhuY_OwMl0Phw.png" /><figcaption>Source: Sigmoid Activation Function (<a href="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</a>)</figcaption></figure><p>Similarly in a multi-label classification model, the task is to classify given input into one of three or more classes. Here the loss is obtained by first applying the softmax activation function and then passing it through CrossEntropy loss function.</p><p>But in a multi-label classification model, the task is to classify given input into more than one class i.e. assigning multiple labels. The targets are represented as a one-hot-encoded array. This seems similar to the binary classification task but with the only difference of having multiple instances of it i.e. binary classification of the input image is done for every label available.</p><p>So for this task, we use the BinaryCrossEntropy loss function along with the sigmoid activation function. This is due to the fact that multi-label classification is similar to binary classification, except PyTorch applies the activation and loss to one-hot encoded targets using its <strong>element-wise operation</strong>.</p><h4>Metrics</h4><p>Fastai provides a metric called accuracy_multi for mult-label classification. After we receive the output logits from the model, sigmoid activation is applied to convert all values into a range of [0–1]. Then the accuracy_multi function is applied by specifying a <strong>threshold</strong> value i.e. all the probabilities above that thresh will be true else it will be false. In this way, we get the predictions of the model.</p><p>The accuracy score is computed by comparing predictions with targets as usual to the normal accuracy calculation. The important thing to keep in mind is to properly determine the value of the threshold. If we are selecting a threshold that is too high, we will only select the objects about which the model is very confident. If we are selecting a threshold that’s too low, we will often be failing to select correctly labelled objects.</p><p>Now let’s dive into code and see how to train the model.</p><h4>Create a Learner</h4><p>Let’s create a learner object using cnn_learner and resnet50 model.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3eb6dd0ece5125f687db36de587e4fb4/href">https://medium.com/media/3eb6dd0ece5125f687db36de587e4fb4/href</a></iframe><p>The partial function accepts a function and keyword parameter of that function and returns a new function with the updated keyword parameter. In the above example, we are using partial to update the <strong>thresh</strong> value of the default <strong>accuracy_multi</strong> metric function from 0.5 to 0.2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/708/1*5sD5EmClM16faXlwBOva8Q.png" /></figure><p>We can see that when we keep the threshold as 0.2, we are achieving accuracy up to 95 %. Let’s see what happens if we change the thresh value.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ec8fab3c354e72bb87de1221403f5d6b/href">https://medium.com/media/ec8fab3c354e72bb87de1221403f5d6b/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*38plxoCSFIwS3WDdhWI1Zw.png" /><figcaption>Loss, accuracy when thresh = 0.1</figcaption></figure><p>In the above code, we have changed the metric of our model to accuracy_multi with thresh=0.1. When we do <strong>learn.validate() </strong>the learner object calculates the loss and accuracy on our validation data. Since we are only changing the metric, there is no need to train the model. From the above output, we can see that when thresh=0.1, we are getting an accuracy of 93% which is less compared to our previous result. Let’s now check for a higher thresh value.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/aa9954fcc0df3cf51d4c5cd75623c2ff/href">https://medium.com/media/aa9954fcc0df3cf51d4c5cd75623c2ff/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*djAmsWm7ah7K4FANDa1EhQ.png" /><figcaption>Loss, accuracy when thresh=0.99</figcaption></figure><p>When we are having a higher threshold value i.e. thresh = 0.99, we can see that the accuracy is 94%. So we can conclude that picking a thresh value is dependent on the data we are using and not a fixed value to be used always.</p><p>So it is recommended to experiment and check for different thresh values and decide on a value that gives better accuracy.</p><h3>Conclusion</h3><p>Therefore we have seen how the multi-label classifier works and how it can be used to avoid some problems faced by conventional single-label-classifier. So it is always recommended to train a multi-label classifier even for normal/single-label classification tasks for more robust model behaviour.</p><p>Thank you</p><h4><strong>GitHub link for Notebook </strong><a href="https://github.com/JNK234/100-days-of-deep-learning/blob/main/Day%203/Multi_Label_classification.ipynb"><strong>here</strong></a><strong>. LinkedIn profile </strong><a href="https://www.linkedin.com/in/narasimhakarthik/"><strong>here</strong></a><strong>.</strong></h4><p><a href="https://medium.com/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">Mlearning.ai Submission Suggestions</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=515a4fd52c8c" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Approaching Data-centric AI using Fast.ai]]></title>
            <link>https://narasimhakarthik.medium.com/approaching-data-centric-ai-using-fast-ai-6099c3d3e0e1?source=rss-c115716f79b5------2</link>
            <guid isPermaLink="false">https://medium.com/p/6099c3d3e0e1</guid>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[blogging]]></category>
            <category><![CDATA[fastai]]></category>
            <category><![CDATA[computer-vision]]></category>
            <dc:creator><![CDATA[Narasimha Karthik Jwalapuram]]></dc:creator>
            <pubDate>Sat, 06 Nov 2021 06:25:02 GMT</pubDate>
            <atom:updated>2021-11-09T14:25:38.241Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1_Do5p1u9-VIke44y5xdTQ.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@hjrc33?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Héctor J. Rivas</a> on <a href="https://unsplash.com/s/photos/images?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><p>This blog post is part of the 100-days of Deep Learning challenge. I have started this challenge by reading the book <strong><em>“Deep Learning for Coders with fastai &amp; PyTorch”</em></strong> by Jeremy Howard and Sylvain Gugger, to learn about the fastai library and its applications in deep learning.</p><p>Fastai library is a deep learning library that adds higher-level functionalities on top of PyTorch. So this is a perfect choice of the library for quick prototyping and model building on different datasets as well as utilising the flexibility and speed of PyTorch.</p><p>In this blog post, let us discuss the data-centric approach of training deep learning models. Data-centric AI is based on the concept of systematically enhancing the datasets over the course of model development to improve the model metrics. This approach is usually overlooked and data collection and cleaning has not been the most favourite task.</p><blockquote>“The model and the code for many applications are basically a solved problem. Now that the models have advanced to a certain point, we got to make the data work as well” — Andrew Ng</blockquote><p>Let’s train an image classifier pipeline from scratch i.e. from gathering data to training the model using the fastai library thereby exploring the potential of the data-centric model training approach.</p><h3>Gathering the Data</h3><p>Let’s train a bear detector i.e. it will discriminate between three types of bears: grizzly, black and teddy bear. To obtain the dataset, let’s use the <strong>jmd_imagescraper </strong>library. This is an image scraping library for creating datasets. It uses the DuckDuckGo for image scraping, hence you can verify the images being downloaded by searching in DuckDuckGo.</p><p>Let’s install the required libraries,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/715142e118c431359a66fa53b99276b3/href">https://medium.com/media/715142e118c431359a66fa53b99276b3/href</a></iframe><p>Now let’s import the fastai and jmd_imagescraper library,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0fe22a684a2647ab76093c4577bd5d9e/href">https://medium.com/media/0fe22a684a2647ab76093c4577bd5d9e/href</a></iframe><p>We have to download the images corresponding to 3 classes i.e. grizzly, black and teddy bear. To download the dataset, we have to pass the path to download, name of the folder, search string and number of results to be downloaded as arguments to download the dataset.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c709b8f367c7642479ccece1f20a9987/href">https://medium.com/media/c709b8f367c7642479ccece1f20a9987/href</a></iframe><p>You can check the images downloaded in the Data folder. Since we have downloaded the data from the internet, there are chances that it might be corrupted, so let’s verify and delete the corrupted one.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9034b090f7e4eb83d007ce79a149d4a2/href">https://medium.com/media/9034b090f7e4eb83d007ce79a149d4a2/href</a></iframe><p>Fastai provides a variety of util functions which makes it easy to work with. The get_image_files is a fastai function that returns an L object containing the paths to all images. The verify_images function is used to check for corrupted image files in the list.</p><p>Since now we have our dataset ready let’s proceed to prepare data for model training.</p><h3>From Data to DataLoaders</h3><p>In PyTorch, DataLoader is a class that takes the dataset and returns an iterable which can be passed to the model for training in batches. Similarly, the DataLoaders class in fastai takes DataLoader objects which we pass and makes them available for training and validation.</p><p>To convert our downloaded data into DataLoaders in fastai, the most efficient way is to use the DataBlock API. Using this API, we can easily customize and control every stage in preparing the dataloaders. Let’s define the DataBlock object for the above dataset.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/fedfd4ca94e002ac296a3fffc10c54e0/href">https://medium.com/media/fedfd4ca94e002ac296a3fffc10c54e0/href</a></iframe><p>Let’s look at each of the arguments and how it is used,</p><ul><li><strong>blocks</strong>: This defines the types of independent and dependent variables that we are providing to model i.e in our case we are providing an image and a category hence <em>ImageBlock</em> and <em>CategoryBlock</em>.</li><li><strong>get_items</strong>: This defines how to get the list of items. <em>get_image_files</em> is a fastai function that accepts the path as an argument and returns the list of all images in that path recursively.</li><li><strong>get_y</strong>: This defines how to identify the dependent variables/labels from items retrieved using the <em>get_items </em>function<em>. </em><em>parent_label </em>is a fastai function that simply returns the name of the folder a file is in.</li><li><strong>splitter</strong>: Since we have got our data and labels, we need to specify how to split the data using this argument. <em>RandomSplitter</em> splits the dataset into training and validation sets randomly but uses a seed to make sure that the model gets the same data for training and validation for every epoch.</li><li><strong>item_tfms</strong>: As we are providing multiple image samples as a batch at a time, we need to resize the images. Hence we use the <em>Resize() </em>method on each image as an argument to item_tfms.</li></ul><p>As of now, we have defined the basic datablock object. Now let’s use it to create dataloaders from the dataset.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1d59e1316f1bd934d3e8d331c0ddf159/href">https://medium.com/media/1d59e1316f1bd934d3e8d331c0ddf159/href</a></iframe><p>Using the <em>show_batch() </em>method we can visualize a few samples present in the dataset. We can expect a similar output as follows,</p><figure><img alt="List of images in dataset along with the labels." src="https://cdn-images-1.medium.com/max/1024/1*Yeiq1S_eAVSWiK2oFshnNg.png" /></figure><p>We can see that we have easily created dataloaders and from raw data without much hassle using DataBlock API.</p><p>Now we have the data ready in the format required for model training. Before we delve into training, let’s explore another important part of model pipeline data augmentation.</p><h3>Data Augmentation</h3><p>Data augmentation refers to creating random variations of samples such that they appear different but do not change the meaning of the image. This is one of the ways to ensure that model is not memorizing the input images but learning about the patterns in the data. Let’s look at some of the most commonly used augmentation techniques.</p><ul><li>Using <em>RandomResizedCrop</em> instead of <em>Resize</em> as item transform because this transforms not only size but also the orientation and scale of the image randomly. Let’s look at how it is done,</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/56882b1e91685ccbbc1a347931d8a3db/href">https://medium.com/media/56882b1e91685ccbbc1a347931d8a3db/href</a></iframe><p>We can create a new datablock object using the <em>new </em>method on the existing datablock and passing the new item and batch transform. The RandomResizedCrop method accepts the <em>min_scale </em>argument which determines how much of the image to select at a minimum each time. Finally, by setting unique=True in <em>show_batch</em> function we can have the same image repeated with different versions of <em>RandomResizedCrop </em>transforms<em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ji9hElDpyYdKwqrk6pmfiQ.png" /></figure><ul><li>Using <em>aug_transforms </em>function on a batch of images. This is a fastai function that applies multiple augmentations like flipping, changing brightness, contrast etc. This can be applied to a batch of images as all the images will be of the same size.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4eafcf8dae513861714af6620eb77c2a/href">https://medium.com/media/4eafcf8dae513861714af6620eb77c2a/href</a></iframe><p>You can see a similar output as follows,</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iCoUWPQ1DWcdfD6XxkFWiw.png" /></figure><p>Now since we have seen some ways to perform data augmentation let’s combine the methods and get the final dataloaders.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bab26f83e4ad8fb976c4d180485b2392/href">https://medium.com/media/bab26f83e4ad8fb976c4d180485b2392/href</a></iframe><p>Let’s dive into creating and training the model…</p><h3>Training Your Model</h3><p>Now coming to the fun part… Since we don’t have a lot of data for our problem, training a model with high accuracy will be difficult without running into the problem of over-fitting. Unless we are using a technique called <strong>transfer learning</strong>.</p><p>Transfer learning is a training method where we use a pre-trained model ( i.e. a model that has been trained previously on different tasks ) and fine-tune with our dataset. This method has proved to achieve amazing results even with less training data and by taking less time. Now let’s see how it is done in fastai,</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5ccdf0fdb6b705657f730f118778116/href">https://medium.com/media/f5ccdf0fdb6b705657f730f118778116/href</a></iframe><p>After training for 4 epochs we can see that the accuracy of the model is closer to 95% and that is pretty impressive.</p><p>Now let’s see the confusion matrix to make more sense of the model performance.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/376a565dbc9afe6cf491576dbd95424d/href">https://medium.com/media/376a565dbc9afe6cf491576dbd95424d/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/1*B_hcX9eVXq5FMwwJLpvjMg.png" /></figure><p>From the confusion matrix, we can observe that our model was able to correctly classify almost all images.</p><p>In the below table, we can see the images that the model has misclassified with maximum loss. We can see that the first image belongs to the black Bear class but our model has predicted it as Grizzly Bear.</p><p>But if we observe the bear image, it resembles more like a grizzly bear than a black bear. So there has been some mess up while labelling the image. (Since these are downloaded directly from DuckDuckGo without our interference)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GrWYocBzqooDWupdJM7nsA.png" /></figure><h3>Data Cleaning through Model</h3><p>Now since our model is trained, let’s now use the data-centric approach to improve the model. This can be done by examining the data used for training and validation i.e. check if images are labelled correctly and then update it with the correct label.</p><p>Intuitively, data cleaning is done before model training. But as we have seen above, the model helps us to identify issues in the data. So to perform data cleaning, fastai provides a handy GUI that allows us to choose a category or delete samples from both training and validation datasets.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5bfee894877da32c42568dedb129e8f0/href">https://medium.com/media/5bfee894877da32c42568dedb129e8f0/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nSXyKdIlSuqSX9Nxs9x36w.png" /></figure><p>We will get the above GUI which displays images with the highest loss in order (i.e. what model think it is not right). Now we can choose the category of the image in the dropdown menu and then perform changes using the following code.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ee01673eacb0ee8040f492c3460e038b/href">https://medium.com/media/ee01673eacb0ee8040f492c3460e038b/href</a></iframe><p>Now we can clean the data easily and again train the model. We will be able to see that the model performance will have improved far better (almost 99~100% accuracy).</p><p>The Data-Centric Approach of model training and transfer learning helps us to train models on real-world datasets and achieve state-of-the-art results. Hence there is a need to shift our focus from model-centric AI to data-centric AI and explore the potential of MLOps.</p><p>Thank you !!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6099c3d3e0e1" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Getting Started with 100 Days of Deep Learning…]]></title>
            <link>https://narasimhakarthik.medium.com/getting-started-with-100-days-of-deep-learning-e92b98c6b959?source=rss-c115716f79b5------2</link>
            <guid isPermaLink="false">https://medium.com/p/e92b98c6b959</guid>
            <category><![CDATA[learning]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[100daychallenge]]></category>
            <category><![CDATA[challenge]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Narasimha Karthik Jwalapuram]]></dc:creator>
            <pubDate>Wed, 20 Oct 2021 15:31:28 GMT</pubDate>
            <atom:updated>2021-10-20T15:31:28.547Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TZCWfdsfcBIxMGtFphzzVg.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/@andrewtneel">Andrew Neel</a> on <a href="https://unsplash.com/">Unsplash</a></figcaption></figure><p>Finally, I am accepting the 100-day Deep Learning challenge…</p><p>After a lot of procrastination, I have decided to pledge that I will be taking up the “100-days of Deep Learning Challenge” from 21st Oct 2021. The main intention behind this challenge is to dedicate at least 1 hour every day for the next 100 days to learning/coding concepts in deep learning.</p><p>I will be sharing my progress via:</p><ul><li>Github: <a href="https://github.com/JNK234/100-days-of-deep-learning">https://github.com/JNK234/100-days-of-deep-learning</a></li><li>Twitter: <a href="https://twitter.com/Narasimhakarth4">https://twitter.com/Narasimhakarth4</a></li><li>Medium: <a href="https://medium.com/@jwalapuramnarasimha">https://medium.com/@jwalapuramnarasimha</a></li></ul><h3><strong>Motivation</strong></h3><p>By doing this challenge, I want to explore and learn about deep learning and its applications. My primary motivation is the extreme optimism about how deep learning can revolutionize the future and how artificial intelligence can change the way humans live.</p><p>My aim is to participate in various Kaggle competitions and implement research papers through this challenge. I hope that by the end of 100 days, I will have a rich portfolio of projects in major domains and I will try to base these projects as a solution for some real-life situations. I will try to document my learning as blog posts and share as many resources and materials that I will use in this journey.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e92b98c6b959" width="1" height="1" alt="">]]></content:encoded>
        </item>
    </channel>
</rss>