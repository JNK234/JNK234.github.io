<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0"><channel><title><![CDATA[NeuraForge: AI Unleashed]]></title><description><![CDATA[NeuraForge: AI Unleashed is a newsletter that dives deep into the world of advanced AI. It covers everything from big language models to reinforcement learning, giving you a solid understanding of these cutting-edge technologies.]]></description><link>https://neuraforge.substack.com</link><image><url>https://substackcdn.com/image/fetch/$s_!_OC8!,w_256,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F160b8c42-5833-45ef-b232-18e5198bc273_200x200.png</url><title>NeuraForge: AI Unleashed</title><link>https://neuraforge.substack.com</link></image><generator>Substack</generator><lastBuildDate>Sun, 07 Sep 2025 05:35:02 GMT</lastBuildDate><atom:link href="https://neuraforge.substack.com/feed" rel="self" type="application/rss+xml"/><copyright><![CDATA[Narasimha Karthik J]]></copyright><language><![CDATA[en]]></language><webMaster><![CDATA[neuraforge@substack.com]]></webMaster><itunes:owner><itunes:email><![CDATA[neuraforge@substack.com]]></itunes:email><itunes:name><![CDATA[Narasimha Karthik J]]></itunes:name></itunes:owner><itunes:author><![CDATA[Narasimha Karthik J]]></itunes:author><googleplay:owner><![CDATA[neuraforge@substack.com]]></googleplay:owner><googleplay:email><![CDATA[neuraforge@substack.com]]></googleplay:email><googleplay:author><![CDATA[Narasimha Karthik J]]></googleplay:author><itunes:block><![CDATA[Yes]]></itunes:block><item><title><![CDATA[Reinforcement Learning Essentials: MDPs & Optimal Control]]></title><description><![CDATA[A comprehensive guide to understanding Markov Decision Processes, Policy Iteration, Value Iteration, and achieving optimal behavior in RL.]]></description><link>https://neuraforge.substack.com/p/reinforcement-learning-essentials-f81</link><guid isPermaLink="false">https://neuraforge.substack.com/p/reinforcement-learning-essentials-f81</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Sat, 09 Aug 2025 13:01:47 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<p>Reinforcement Learning (RL) stands as a powerful paradigm for creating intelligent agents capable of autonomous decision-making in complex, dynamic environments. This comprehensive guide delves into the foundational concepts of RL, starting with the essential agent-environment interaction and progressing to the mathematical rigor of Markov Decision Processes (MDPs). We will explore how agents define strategies through policies and quantify future rewards with value functions, culminating in a detailed look at classic control algorithms like Policy and Value Iteration that drive optimal behavior. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" width="5434" height="3623" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3623,&quot;width&quot;:5434,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;silhouette of road signage during golden hour&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="silhouette of road signage during golden hour" title="silhouette of road signage during golden hour" srcset="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@soymeraki">Javier Allegue Barros</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h3>The Agent-Environment Interaction Loop</h3><p>At the heart of any Reinforcement Learning system lies a fundamental, continuous interaction loop between two primary entities: the <strong>Agent</strong> and its <strong>Environment</strong>. The agent serves as the learner and decision-maker, actively selecting and executing an action at each step. This chosen action directly influences the environment, causing it to transition from its current configuration to a new state. Crucially, in response to the agent's action and the resulting state change, the environment provides vital feedback back to the agent, thereby completing the cycle. This iterative process of observation, action, and feedback is the bedrock upon which the agent's learning process is built.</p><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h3>Understanding States, Actions, and Rewards</h3><p>The feedback provided by the environment to the agent manifests primarily in two critical forms: <strong>States</strong> and <strong>Rewards</strong>.</p><ul><li><p>A <strong>State</strong> precisely describes the current configuration or condition of the environment. It provides the agent with all the necessary contextual information to make an informed decision at that particular moment. The state encapsulates the relevant aspects of the environment that the agent perceives.</p></li><li><p>An <strong>Action</strong> represents the specific operation, decision, or control signal that the agent performs within a given state. These actions are the agent's means of influencing the environment and progressing through its task.</p></li><li><p>Following an action, the environment issues a <strong>Reward</strong>. This is a scalar numerical value that quantifies the immediate desirability or undesirability of the agent's action and the resulting transition to a new state. A positive reward indicates a favorable outcome, while a negative reward (often termed a penalty) signifies an unfavorable one.</p></li></ul><p>The overarching goal of the agent is not merely to maximize immediate rewards, but rather to maximize the <em>cumulative reward</em> over the long term. Through this iterative cycle of observing states, performing actions, and receiving rewards, the agent progressively learns which sequences of actions lead to the most favorable long-term outcomes, thereby discovering an optimal policy for navigating its environment.</p><h2>II. From Multi-Arm Bandits to Contextual Challenges</h2><p><strong>Sequential decision-making under uncertainty</strong> is a fundamental aspect of Reinforcement Learning. The Multi-Arm Bandit (MAB) problem serves as an accessible entry point to this domain, illustrating core concepts before progressing to more complex frameworks that address more intricate challenges.</p><h3>Introduction to Multi-Arm Bandit (MAB) problems</h3><p>The Multi-Arm Bandit (MAB) problem models a decision-maker faced with a finite set of choices, often referred to as "arms," each yielding a reward drawn from an unknown probability distribution. Analogous to a gambler selecting from multiple slot machines, where each machine (arm) has a distinct, unknown payout probability, the objective is to maximize the total cumulative rewards over a series of independent decisions.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!43pB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!43pB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 424w, https://substackcdn.com/image/fetch/$s_!43pB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 848w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg" width="280" height="280" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:280,&quot;width&quot;:280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16991,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!43pB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 424w, https://substackcdn.com/image/fetch/$s_!43pB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 848w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Slot Machine or Bandir Machines</figcaption></figure></div><p>The central challenge in MAB is the <strong>exploration-exploitation trade-off</strong>. Exploitation involves consistently choosing the arm that has historically provided the highest average reward, aiming for immediate gains based on current knowledge. Conversely, exploration entails trying less-chosen or untried arms to gather more information about their true reward distributions, with the potential of discovering a more lucrative long-term payout. This dilemma is inherent in scenarios where optimizing immediate reward competes with acquiring knowledge for future benefit. A key characteristic of the MAB problem is the <strong>assumption of a single, independent decision at each time step,</strong> where the <strong>environment does not change based on past actions</strong>, and there is no sequence of interconnected states. The reward received from pulling an arm does not influence the availability or characteristics of other arms or future states.</p><h3>Understanding Contextual Multi-Arm Bandit (C-MAB)</h3><p>Building upon the MAB framework, the Contextual Multi-Arm Bandit (C-MAB) problem introduces "context" or "state information" into the decision process. Unlike traditional MABs where rewards are independent of external factors, in a C-MAB, the optimal action (arm selection) is contingent on the current situation or observable features of the environment. For instance, when recommending an article to a user, the most suitable article (arm) may vary based on the user's browsing history, demographics, or the current time of day&#8212;this constitutes the context. The agent observes this context <em>before</em> making a decision at each step.</p><p>This integration of state information renders the problem more realistic and powerful, as decisions are no longer isolated but are informed by the environment's current state. The objective in a C-MAB is to <strong>learn a policy that maps observed contexts to optimal actions</strong>, thereby maximizing cumulative reward. This fundamental difference means that while MAB assumes a static optimal arm (or set of arms) regardless of the situation, C-MAB acknowledges that the best action depends dynamically on the current context.</p><h3>Limitations of Bandits for sequential decision-making</h3><p>While MAB and C-MAB problems are valuable for understanding the exploration-exploitation trade-off and the role of context in decision-making, they present significant simplifications regarding temporal dynamics. They primarily focus on single-step or short-horizon decisions where the immediate reward is the primary concern. Bandit frameworks are insufficient for scenarios where an agent's actions have long-term consequences, and the environment dynamically evolves over time as a result of those actions.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!TZf6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!TZf6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 424w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 848w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1272w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png" width="1344" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1344,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34429,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!TZf6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 424w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 848w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1272w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://images.app.goo.gl/AaAu4kvQt4pqDvSk8</figcaption></figure></div><p>Bandit problems have certain limitations in their modeling approach:</p><ul><li><p>Lack of Influence on Future States: Actions taken in the present do not impact future states or decision opportunities.</p></li><li><p>No State Transition or Delayed Reward: Traditional bandit formulations do not incorporate the concepts of state transitions or delayed rewards.</p></li><li><p>Static Environment: The environment&#8217;s state remains unchanged regardless of the agent&#8217;s actions, and future rewards are not influenced by past actions.</p></li></ul><p>For scenarios that demand foresight, planning across interconnected states, and evaluating the long-term effects of current decisions, a more robust mathematical framework is necessary. This is where full Reinforcement Learning comes into play, offering comprehensive tools to address these complexities.</p><h2>III. Markov Decision Processes (MDPs): The Foundation for Sequential Decisions</h2><p>Markov Decision Processes (MDPs) provide a formal mathematical framework for modeling sequential decision-making problems. An MDP is typically characterized by a tuple (S, A, P, R, &#947;, H) , where each element plays a critical role in defining the problem.  </p><p>The Discount Factor (&#947;) is a key parameter in reinforcement learning that quantifies the present value of future rewards. It has a value ranging from 0 to 1 (inclusive) and plays a crucial role in balancing immediate gratification against long-term objectives.</p><ul><li><p>High &#947; (closer to 1): Emphasizes long-term rewards, encouraging the agent to consider future consequences more heavily.</p></li><li><p>Low &#947; (closer to 0): Makes the agent more "myopic," prioritizing immediate rewards over future ones.</p></li></ul><h3>Defining an MDP: States (S), Actions (A), Reward function (R), Transition matrix (P)</h3><p>The fundamental components of an MDP define the environment and the agent's interaction within it:</p><ul><li><p><strong>States (S)</strong> represent all possible configurations or observable conditions of the environment at any given time. While often considered finite for theoretical simplicity and tractability, state spaces can be countably infinite or even continuous in practical applications. In such cases, techniques like function approximation become essential to manage the immense or infinite number of states.</p></li><li><p><strong>Actions (A)</strong> constitute the set of choices available to the agent when in a particular state. These actions are the agent's means of influencing the environment's evolution, leading to new states and potentially new rewards.</p></li><li><p>The <strong>Transition Probability (P)</strong> defines the dynamics of the environment, capturing its inherent uncertainty. Specifically, <em><strong>P(s&#8217;|s, a)</strong></em> denotes the probability of transitioning to a next state s&#8217; from the current state s after the agent takes action a. This probabilistic nature is fundamental to modeling real-world environments where outcomes are not always deterministic.</p></li><li><p>The <strong>Reward Function (R)</strong> provides immediate scalar feedback to the agent, indicating the desirability of a specific state-action-next-state transition.  <em><strong>R(s,a,s&#8217;) </strong></em>assigns a numerical value representing the immediate reward received by the agent for taking action a in state s and subsequently transitioning to state s&#8217;. While rewards can also be stochastic, the primary source of uncertainty in MDPs typically stems from the probabilistic state transitions.</p></li></ul><h3>The concept of Horizon (H) in MDPs</h3><p>The <strong>Horizon (H)</strong> specifies the total number of steps or time stages in an episode of the decision-making process. The horizon can be <strong>finite</strong>, as seen in games with a fixed number of turns, or infinite, which is common in continuous control tasks where the process continues indefinitely until a terminal condition is met or a steady state is achieved.</p><h3>Understanding the Markov Property</h3><p>A critical characteristic that underpins the power and analytical tractability of MDPs is the <strong>Markov Property</strong>. This property asserts that the <strong>future state and the reward depend </strong><em><strong>only</strong></em><strong> on the current state and the action taken</strong>, and are <strong>conditionally independent of all previous states and actions</strong>. Formally, this can be expressed as:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, \\dots) = P(S_{t+1}|S_t, A_t)&quot;,&quot;id&quot;:&quot;VMKOMEFNQF&quot;}" data-component-name="LatexBlockToDOM"></div><p>This simplification is profoundly significant because it allows the agent to make optimal decisions based solely on the current state, without needing to recall or process the entire history of interactions. It implies that the current state encapsulates all necessary information from the past to predict the future. For many well-defined MDPs, the optimal policy itself is also Markov, meaning the optimal action in any state depends only on that state. However, it's important to note that for more complex scenarios, such as Partially Observable Markov Decision Processes (POMDPs) where the current state is not fully observable, or Multi-Agent Reinforcement Learning (MARL) where interactions are complex, policies may need to consider historical observations to infer a more complete understanding of the environment.</p><p>This formalization of states, actions, transitions, and rewards, rigorously underpinned by the Markov Property, provides a robust mathematical foundation for understanding how agents learn to navigate and optimize their behavior in dynamic and uncertain environments. It enables the development of algorithms that can effectively derive optimal policies for sequential decision-making.</p><h2>IV. Key Concepts: Environment Settings, Policy, and Value Functions</h2><p>Reinforcement Learning (RL) agents operate within dynamic environments, learning optimal strategies through interaction. Understanding the fundamental concepts of these environments, the agent's strategy (policy), and how future rewards are estimated (value functions) is crucial for grasping how RL systems function. This section delves into these core components, including the critical role of the discount factor.</p><h3>Different Environment Settings: Known (Planning), Unknown (Simulator, Online, Offline)</h3><p>In Reinforcement Learning, the <strong>environment</strong> is everything an agent interacts with, providing states, rewards, and determining the consequences of actions. The nature of this environment significantly influences the learning approach. Environments can broadly be categorized based on whether the agent has a complete model of the world or not.</p><p>When an agent operates in a <strong>known environment</strong>, it possesses a complete model of the environment's dynamics&#8212;meaning it knows exactly how actions will transition it between states and what rewards will be received. This allows for <strong>planning</strong>, where the agent can compute optimal strategies offline without direct interaction, often leveraging techniques like dynamic programming.</p><p>Conversely, in <strong>unknown environments</strong>, the agent does not have a complete model of the world. Learning in such settings requires the agent to interact with the environment to gather information. These unknown environments can manifest in several forms:</p><ul><li><p><strong>Simulator environments</strong> provide a digital replica of a real-world system. While the underlying model might be complex or unknown to the agent, the simulator allows for repeated, safe, and often accelerated interaction to collect data. The agent learns from this simulated experience, which can then be transferred or adapted to the real world.</p></li><li><p><strong>Online environments</strong> represent direct interaction with the real world. The agent performs actions and receives immediate feedback (new states and rewards) in real-time. Learning in an online setting is continuous and directly influenced by live experience, often requiring careful exploration strategies to discover optimal behaviors without causing significant negative consequences.</p></li><li><p><strong>Offline environments</strong> involve learning from a fixed dataset of previously collected interactions without any further interaction with the environment itself. The agent analyzes historical data to derive a policy. This approach is valuable when real-world interaction is costly, dangerous, or time-consuming, but it presents challenges in terms of data coverage and avoiding extrapolation beyond the observed data.</p></li></ul><h3>Policy (&#960;): The Agent's Strategy </h3><p>A <strong>policy</strong> (&#960;) defines the agent's behavior, acting as its strategy for choosing actions in any given state. It is the core of an RL agent, mapping observed states to actions. The ultimate goal of many RL algorithms is to discover an optimal policy that maximizes the agent's cumulative reward over time. Policies can be characterized in several ways:</p><ul><li><p>A <strong>deterministic policy</strong> dictates a single, specific action for each state. If the agent is in a particular state, a deterministic policy will always prescribe the exact same action to be taken.</p></li><li><p>In contrast, a <strong>stochastic policy</strong> outputs a probability distribution over possible actions for each state. This means that for a given state, there might be multiple actions the agent could take, each with a certain probability. Stochastic policies are often useful for exploration, allowing the agent to try different actions and discover better strategies, especially in environments with inherent uncertainty.</p></li><li><p>A <strong>Markov policy</strong> (or memoryless policy) depends solely on the current state. It assumes that the current state provides all necessary information to make an optimal decision, satisfying the Markov property where future states depend only on the current state and action, not the entire history. Most standard RL algorithms assume a Markov policy.</p></li><li><p>A <strong>general policy</strong>, on the other hand, might depend on the entire history of states and actions encountered up to the current moment. While more complex, such policies can be necessary in environments where the Markov property does not hold, and past observations provide crucial context for future decisions.</p></li></ul><h3>Value Function: Quantifying Future Rewards </h3><p>Value functions are central to Reinforcement Learning, serving to estimate the "goodness" of states or state-action pairs in terms of future rewards. They provide a quantitative measure of how much cumulative reward an agent can expect to receive from a given point forward, following a specific policy.</p><p>The foundation of value functions is the <strong>Return (Gt)</strong>, which represents the total discounted sum of future rewards from a particular time step t. It aggregates all rewards obtained from t onwards, with future rewards being progressively discounted.</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{H-1} r_{t+H-1}&quot;,&quot;id&quot;:&quot;HYTMFCXBKW&quot;}" data-component-name="LatexBlockToDOM"></div><p>Building upon the concept of return, two primary types of value functions are used:</p><ul><li><p>The <strong>state-value function (V(s))</strong>, often denoted as the V-value, provides an estimate of the expected return an agent can anticipate if it starts in a particular state s and then follows a given policy &#960; thereafter. In essence, V(s) tells us <strong>"how good it is to be in state s"</strong> under a specific strategy.</p></li></ul><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V^\\pi_h(s) = E^\\pi \\left[ \\sum_{h'=h}^H r_{h'}(S_{h'}, A_{h'}) \\mid S_h = s \\right]&quot;,&quot;id&quot;:&quot;POHMMDIDGU&quot;}" data-component-name="LatexBlockToDOM"></div><ul><li><p>The <strong>state-action value function (Q(s,a))</strong>, known as the Q-value, estimates the expected return if the agent takes a specific action a in a particular state s, and then follows policy &#960; for all subsequent actions. </p></li></ul><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q^\\pi_h(s,a) = E^\\pi \\left[ \\sum_{h'=h}^H r_{h'}(S_{h'}, A_{h'}) \\mid S_h = s, A_h = a \\right]&quot;,&quot;id&quot;:&quot;UGHWGIJJXV&quot;}" data-component-name="LatexBlockToDOM"></div><p>The Q-value answers the question, "how good it is to take action a in state s ?" This function is particularly useful for decision-making, as an agent can choose the action with the highest Q-value in any given state to maximize its expected future reward.</p><p>Value functions are critical because they allow RL algorithms to evaluate different policies and learn which actions lead to long-term success, even if immediate rewards are small or negative.</p><h2>V. The Objective of Reinforcement Learning: Finding Optimal Policies</h2><p>With the fundamental concepts of environments, policies, and value functions established, the central objective of Reinforcement Learning becomes clear and actionable. The ultimate goal is to discover an <strong>optimal policy (&#960;*)</strong> that strategically directs the agent's actions, ensuring it maximizes its expected cumulative reward over the long term within a given environment.</p><h3>Defining the Optimal Policy (&#960;*)</h3><p>An optimal policy, denoted as &#960;*, signifies the best possible strategy an agent can employ. This policy dictates action choices that consistently lead to the highest attainable expected cumulative reward. Formally, this objective often translates to maximizing the value of the initial state, represented as V, assuming a fixed starting state . </p><p>In more generalized scenarios where the initial state is not predetermined, the objective expands to maximizing the expected value across a distribution of potential initial states, . For many theoretical analyses and practical applications, a fixed initial state assumption simplifies the problem without loss of generality for the core concepts.</p><h3>The Bellman Optimality Equations: Characterizing Optimal Value Functions</h3><p>Identifying an optimal policy necessitates a precise method to characterize what 'optimal' truly means in terms of value. This is precisely where the <strong>Bellman Optimality Equations</strong> become indispensable. These equations provide a recursive relationship that optimal value functions must inherently satisfy, serving as the fundamental theoretical bedrock for finding optimal policies. They rigorously define the value of a state, or a state-action pair, under an optimal policy as the maximum possible expected return. This maximum is achieved by considering the immediate reward from an action and the optimal values of all possible successor states. </p><h2>Policy Iteration for Optimal Policies</h2><p>The objective of Reinforcement Learning is to discover an optimal policy&#8212;a strategy that maximizes long-term rewards. Dynamic programming provides powerful iterative methods to achieve this goal. Policy Iteration (PI) stands as a foundational algorithm for solving Markov Decision Processes (MDPs), systematically converging to an optimal policy for finite MDPs.</p><p>Policy Iteration operates through a robust, cyclical process comprising two core phases: <strong>policy evaluation and policy improvement.</strong> The algorithm begins with any arbitrary policy, &#960;, and systematically refines it through these alternating steps until the optimal strategy is precisely identified. This iterative refinement is key to its power.</p><h3>Step 1: Policy Evaluation</h3><p>This initial phase focuses on understanding the current policy's effectiveness. In Policy Evaluation, the value function V_&#960;(s) for the <em>current</em> policy, &#960;, is accurately computed for every possible state s. This value quantifies the expected cumulative return an agent can anticipate from starting in state s and subsequently adhering strictly to policy &#960;. In essence, Policy Evaluation meticulously measures the "goodness" or long-term desirability of each state under the existing policy. For finite MDPs, this involves a thorough assessment to determine the precise state-value function for the given policy.</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V^\\pi_k(s) = \\sum_{a} \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^\\pi_{k-1}(s') \\right]&quot;,&quot;id&quot;:&quot;YTCBEKATPA&quot;}" data-component-name="LatexBlockToDOM"></div><p></p><h3>Step 2: Policy Improvement</h3><p>With a complete understanding of the current policy's value function <em>V_&#960;(s)</em>, the Policy Improvement phase updates the policy itself. A <em>new</em> policy, <em>&#960;'</em>, is constructed to be "greedy" with respect to the recently evaluated value function. This is done by first computing the Q-value function for current policy &#960; which helps quantify the value of taking a specific a in state s and then following &#960;i:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q^{\\pi_i}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^{\\pi_i}(s')&quot;,&quot;id&quot;:&quot;RJKPRUQCXM&quot;}" data-component-name="LatexBlockToDOM"></div><p> The new policy <em>&#960;'(s)</em> is chosen by taking the argmax i.e. the action that maximizes  this Q-value for each state s</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\pi_{i+1}(s) = \\text{argmax}_{a \\in A} Q^{\\pi_i}(s,a) \\quad \\forall s \\in S&quot;,&quot;id&quot;:&quot;ZAPEZBCBXT&quot;}" data-component-name="LatexBlockToDOM"></div><p>This means that for every state <em>s</em>, the improved policy <em>&#960;'(s)</em> selects the action a that promises the highest expected return. This decision considers both the immediate reward gained by taking action <em>a</em> in state <em>s</em> and the anticipated value of the subsequent state <strong>s'</strong> according to the just-computed <em>V_&#960;(s')</em>. This strategic update guarantees that the revised policy is inherently better, or at least as good as, the previous one, leveraging the comprehensive evaluation from the prior step.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!P-7I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!P-7I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 424w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 848w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1272w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp" width="801" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:801,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8976,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!P-7I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 424w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 848w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1272w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: Policy Iteration - https://www.geeksforgeeks.org/data-science/what-is-the-difference-between-value-iteration-and-policy-iteration/</figcaption></figure></div><h3>Understanding Monotonic Improvement in Policy Iteration</h3><p>The powerful cycle of policy evaluation and policy improvement continues iteratively, driving the algorithm towards optimality. A fundamental and crucial characteristic of Policy Iteration is its guarantee of <em><strong>monotonic improvement</strong></em><strong>.</strong> This means that each successive iteration is guaranteed to yield a policy that performs at least as well as, and frequently strictly better than, its predecessor in terms of expected long-term return. This consistent, non-decreasing enhancement of the policy's value at every step is a cornerstone of Policy Iteration's reliability and effectiveness. </p><p>Crucially, for finite state-action spaces, Policy Iteration is mathematically guaranteed to converge to an optimal policy within a finite number of steps. This explicit process of maintaining and continually refining the policy throughout its execution solidifies Policy Iteration as a robust and reliable method for discovering optimal control strategies.</p><h2>Value Iteration for Optimal Policies</h2><p>Value Iteration (VI) offers a distinct and powerful approach to solving Markov Decision Processes (MDPs) by directly computing the optimal value function, V*(s). Unlike Policy Iteration, VI does not explicitly maintain or iteratively improve a policy throughout its process. Instead, it focuses solely on refining the value estimates for each state until they converge to their optimal values, from which the optimal policy can then be readily derived.</p><p>The algorithm begins by initializing an arbitrary value function, commonly setting V&#8320;(s) = 0 for all states s. In each subsequent iteration k+1, the value of every state s is updated by considering the maximum expected return achievable from that state across all possible actions. This critical update rule is encapsulated by the <strong>Bellman Optimality Equation</strong>, which serves as the core iterative principle:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V_{k+1}(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V_k(s') \\right]&quot;,&quot;id&quot;:&quot;IBZPNBDGRC&quot;}" data-component-name="LatexBlockToDOM"></div><p>In this equation, R(s,a) represents the immediate reward received for taking action a in state s. The term &#947; &#931;_{s'} P(s'|s,a)V_k(s') accounts for the discounted expected value of the next state, s', weighted by its transition probability P(s'|s,a) and evaluated using the current value function V_k. The entire expression inside the maximization, R(s,a) + &#947; &#931;_{s'} P(s'|s,a)V_k(s'), effectively represents the Q-value of taking action a in state s and then proceeding optimally according to the current value function V_k. By selecting the action that maximizes this Q-value, Value Iteration iteratively builds up the optimal value function by considering increasingly longer planning horizons.</p><p>The guaranteed convergence of Value Iteration to a unique optimal value function is attributed to a fundamental mathematical property of the Bellman optimal operator: it is a <strong>contraction mapping</strong>. When the discount factor &#947; is less than 1, applying this operator repeatedly reduces the "distance" between successive value functions. This contraction property ensures that the sequence of value functions, V&#8320;, V&#8321;, V&#8322;, ..., will converge to a single, unique fixed point, which is precisely the optimal value function V*(s). This theoretical soundness is crucial for Value Iteration, firmly establishing that the algorithm will arrive at the correct V*(s), without requiring formal proofs of this property during its conceptual application.</p><p>Once the value function V*(s) has converged to a stable state, the optimal policy &#960;*(s) can be extracted directly. For each state s, the optimal policy simply dictates selecting the <strong>action a that maximizes the expected return</strong>, utilizing the converged optimal value function to evaluate future states:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\pi^*(s) = \\text{argmax}_{a} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^*(s') \\right]&quot;,&quot;id&quot;:&quot;DELNIUDSMZ&quot;}" data-component-name="LatexBlockToDOM"></div><p>It is an important distinction that while the value function itself monotonically converges to the optimal one in Value Iteration, the policy extracted at intermediate steps does not necessarily guarantee monotonic improvement. This contrasts with Policy Iteration, where policy improvement is a guaranteed step in each iteration.</p><h2>VI. Key Challenges in Reinforcement Learning</h2><p>Reinforcement Learning (RL) faces several significant challenges, which are central to the study of its mathematical foundations and practical applications:</p><p><strong>1. Function Approximation</strong></p><p>Classical Reinforcement Learning models <strong>often rely on tabular methods</strong>, which assume a finite number of states with stored values and policies. However, real-world applications like large-scale video games, autonomous driving, or complex robotic systems involve millions, billions, or even continuous state and action spaces. Directly representing or tabulating values and policies for such vast spaces is computationally intractable and memory-prohibitive. Function approximation, using <strong>parameterized functions like neural networks or linear models,</strong> approximates the value function (Q-values or state-values) or the policy directly. This allows RL algorithms to generalize from limited states to unseen ones, enabling scalability to environments with immense or continuous state-action spaces. The challenge then shifts from storing every state-action pair to learning and optimizing the parameters of these complex functions.</p><p><strong>2. Partial Observability (POMDPs)</strong></p><p>In many practical settings, an agent does not have complete or perfect information about the environment's true underlying state. Instead, it receives only limited, noisy, or incomplete observations. This condition introduces the significant challenge of <strong>partial observability,</strong> where the agent must infer the underlying true state from its incomplete observation history. For example, a robot navigating a cluttered room might only perceive its immediate vicinity through its sensors, rather than possessing a complete, global map of the entire room layout. <strong>Partially Observable Markov Decision Processes (POMDPs)</strong> are the formal mathematical framework used to model and study such scenarios. In POMDPs, agents cannot simply react to the current observation; they are required to maintain a belief distribution over possible true states, continuously updating this belief based on new observations and past actions. This belief state, rather than the true state, then guides the agent's decision-making, significantly increasing the complexity of policy derivation and learning.</p><p><strong>3. Multi-Agent RL (MARL)</strong></p><p>Multi-Agent Reinforcement Learning (MARL) addresses the complexity of RL when multiple agents interact in a shared environment. Agents can collaborate to achieve a common goal (e.g., robots) or compete (e.g., players in a game). Each agent&#8217;s perspective introduces dynamic and non-stationary environments, as the optimal policy depends on the evolving policies of other agents. This non-stationarity violates the core Markovian assumption in single-agent RL, where the environment&#8217;s dynamics are static. MARL research often draws upon game-theoretical perspectives to design algorithms and establish theoretical guarantees for these intricate multi-agent systems. Simple Markov policies may be insufficient; more sophisticated general policies that incorporate the full history of observations and actions are necessary to account for complex interdependencies and evolving behaviors.</p><p><strong>4. Sample and Computational Efficiency</strong></p><p>Two critical practical concerns in deploying Reinforcement Learning systems are sample efficiency and computational efficiency. Modern RL algorithms require millions to hundreds of millions of interactions with the environment to learn effective policies, which is prohibitively expensive, time-consuming, and potentially unsafe in real-world applications like training autonomous vehicles or operating physical robots. Reducing sample complexity is a primary goal in RL research, while computational efficiency addresses the substantial processing power required for training advanced RL models, especially those using deep neural networks. Iterative algorithms, based on principles from dynamic programming, mitigate high computational costs by avoiding expensive operations like large matrix inversions, especially in environments with vast state spaces. Balancing data requirements with practical constraints remains a significant hurdle for real-world RL deployment.</p><h2>Conclusion</h2><p>This guide explains the core principles of Reinforcement Learning, from agent-environment interactions and Markov Decision Processes to powerful control algorithms like Policy and Value Iteration. It explores how policies, value functions, and the discount factor define optimal strategies in complex state spaces through simulation. Despite challenges like function approximation, partial observability, and multi-agent interactions, understanding these foundational principles is crucial for designing intelligent systems that learn and adapt in dynamic environments.</p><div><hr></div><p>Thank you so much for taking the time to read through my thoughts. This newsletter is a small space where I share my learnings and explorations in RL, Generative AI, and beyond as I continue to study and grow. If you found value here, I&#8217;d be honored if you subscribed and shared it with friends who might enjoy it too. Your feedback means the world to me, and I genuinely welcome both your kind words and constructive critiques.</p><p>With heartfelt gratitude,<br>Thank you for being part of Neuraforge!<br>Narasimha Karthik J</p><p></p><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Beyond Supervised Learning: Unlocking AI's Potential with Reinforcement Learning]]></title><description><![CDATA[Understand the fundamental shift RL offers, its core components, real-world applications, and the advanced challenges driving the future of intelligent systems.]]></description><link>https://neuraforge.substack.com/p/beyond-supervised-learning-unlocking</link><guid isPermaLink="false">https://neuraforge.substack.com/p/beyond-supervised-learning-unlocking</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 14 Jul 2025 12:02:02 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Imagine AI systems that learn not from pre-labeled data, but through iterative trial and error, autonomously mastering complex tasks by interacting with their environment. This is the essence of Reinforcement Learning (RL), a transformative paradigm driving the next wave of artificial intelligence. This deep dive will unravel the core principles of RL, explore how its powerful merger with deep neural networks has unlocked unprecedented capabilities, and showcase its diverse applications across industries, ultimately touching upon the frontier challenges shaping its future.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" width="4000" height="6000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:6000,&quot;width&quot;:4000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;black and white robot illustration&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="black and white robot illustration" title="black and white robot illustration" srcset="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Xu Haiwei</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Understanding Reinforcement Learning: A Paradigm Shift in AI</h2><p>Artificial Intelligence (AI) and Machine Learning (ML) are rapidly transforming various domains. While Supervised Learning has demonstrated remarkable success in tasks requiring extensive labeled datasets, a distinct paradigm, Reinforcement Learning (RL), provides a compelling alternative. RL empowers intelligent agents to acquire optimal behaviors through iterative trial and error within complex, dynamic environments. This section establishes the foundational concepts of Reinforcement Learning, highlighting its unique approach and differentiating it from traditional machine learning paradigms.</p><h3>Supervised Learning vs. Reinforcement Learning: Key Differences and Limitations</h3><p>Supervised Learning operates on the principle of learning from labeled examples. A model is trained to map input features to known output labels, minimizing a predefined error function. This approach excels in tasks such as image classification, natural language processing, and regression where ample ground truth data is available, allowing the model to learn a direct mapping between inputs and desired outputs.</p><p>In contrast, Reinforcement Learning does not rely on pre-existing labeled datasets. Instead, an RL agent learns by interacting directly with an environment. The agent receives scalar feedback in the form of rewards or penalties for its actions, aiming to maximize a <em>cumulative reward</em> signal over time. This paradigm is particularly suited for sequential decision-making problems, where the optimal action depends on the current state and crucially influences future states and subsequent rewards. Limitations of supervised learning often arise in scenarios requiring strategic decision-making, where the "correct" answer for every possible state-action pair is not explicitly provided, or where the optimal behavior emerges from a sequence of interactions rather than isolated mappings. RL directly addresses these challenges by enabling autonomous learning through experience.</p><h3>The Core Components of Reinforcement Learning</h3><p>An RL system comprises several fundamental elements working in concert to facilitate learning and decision-making:</p><ul><li><p><strong>Agent:</strong> The learner or decision-maker. The agent observes the environment's state and selects actions based on its learned policy.</p></li><li><p><strong>Environment:</strong> The external world with which the agent interacts. It encompasses the rules of interaction, the possible states the agent can be in, and the reward signals provided in response to the agent's actions.</p></li><li><p><strong>State:</strong> A complete, or sufficiently informative, description of the environment at a given moment. The agent's decision-making process is fundamentally based on its perception or representation of the current state.</p></li><li><p><strong>Action:</strong> A specific move or decision made by the agent that influences the environment's state. Actions are the means by which the agent interacts with and changes its surroundings.</p></li><li><p><strong>Reward:</strong> A scalar feedback signal provided by the environment to the agent after an action. This signal quantifies the immediate desirability of the agent's action in that particular state. The agent's primary objective is to maximize its total accumulated reward over the long term.</p></li><li><p><strong>Policy:</strong> The agent's strategy or behavior function. It defines how the agent maps observed states to actions. An optimal policy dictates the best action to take in any given state to maximize long-term cumulative reward.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ToXq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ToXq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 424w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 848w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1272w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png" width="1456" height="803" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:803,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:770936,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/168227160?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ToXq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 424w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 848w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1272w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p>Consider a robotic arm learning to pick up an object. The robotic arm itself is the <strong>agent</strong>. The physical world, including the object's position, the table, and gravity, constitutes the <strong>environment</strong>. The arm's joint angles, the gripper's status, and the object's coordinates represent the current <strong>state</strong>. Moving a joint or opening/closing the gripper are <strong>actions</strong>. A positive <strong>reward</strong> is received when the object is successfully grasped, while a negative reward might be given if the object is dropped or if the arm collides with an obstacle. The arm's learned strategy for moving its joints and operating its gripper to consistently grasp objects is its <strong>policy</strong>.</p><h3>The Credit Assignment Problem: Learning from Delayed Rewards</h3><p>A central challenge in Reinforcement Learning is the <em>credit assignment problem</em>. Unlike supervised learning, where feedback (the correct label or value) is immediate for each prediction, RL agents often receive rewards that are delayed and sparse. An action taken at a specific time step might only contribute to a significant positive (or negative) reward much later in the sequence of interactions. Determining which past actions were truly responsible for a future outcome becomes a non-trivial task.</p><p>The agent's goal is to maximize the <em>cumulative reward</em> over an entire episode or its lifetime, not just immediate rewards. This necessitates that the agent learns the long-term consequences of its actions, effectively assigning credit or blame to actions that contributed to these delayed outcomes. Overcoming the credit assignment problem is fundamental to developing effective RL policies, enabling agents to learn complex, optimal behaviors even when feedback is sparse and arrives long after the causative actions.</p><h2>The Rise of Deep Reinforcement Learning: Combining Data and Optimization</h2><p>The previous section established the foundational concepts of Reinforcement Learning (RL), differentiating it from supervised learning and outlining its core components. This section explores the pivotal advancements that led to the emergence of Deep Reinforcement Learning (Deep RL), a powerful paradigm that combines the perceptual capabilities of deep neural networks with the decision-making frameworks of classical RL, fundamentally transforming the field of AI.</p><h3>The Revolution of Deep Learning: Feature Extraction and Generative Models</h3><p>Deep Learning fundamentally transformed the field of Artificial Intelligence by enabling models to learn intricate, hierarchical representations directly from raw, high-dimensional data. Prior to the Deep Learning revolution, extracting meaningful features from raw inputs&#8212;a crucial step for any AI model&#8212;was often a manual and laborious process known as feature engineering. This reliance on human expertise and domain-specific knowledge presented a significant bottleneck for scalability and generalization.</p><p>Deep neural networks, particularly convolutional neural networks (CNNs) for image data and recurrent neural networks (RNNs) for sequential data, demonstrated unprecedented capabilities in automatically learning and extracting abstract features from high-dimensional inputs without explicit human programming. This allowed models to identify complex patterns and abstract representations from data like pixels or raw audio, a capability indispensable for an RL agent operating in realistic environments. Furthermore, advancements in generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), showcased deep learning's power in synthesizing realistic data, a capability that indirectly contributes to robust training through data augmentation or the development of more accurate environment models in complex RL scenarios. These breakthroughs in automatic perception and representation learning laid the essential groundwork for integrating deep neural networks into RL agents, enabling them to process the complex sensory information of the real world.</p><h3>Historical Roots of Modern Reinforcement Learning: Classical RL and Control Optimization</h3><p>Reinforcement Learning boasts a rich history that significantly predates the Deep Learning revolution. Classical RL algorithms, such as <strong>Q-learning</strong> and <strong>SARSA</strong>, provided foundational theoretical frameworks for how an agent could learn to maximize cumulative rewards through iterative trial and error. These methods focused on learning optimal value functions (estimating the goodness of states or state-action pairs) or policies (direct mappings from states to actions) primarily in discrete, often finite, state-action spaces. They effectively addressed challenges like the credit assignment problem, where an agent must determine which past actions were responsible for a delayed reward.</p><p>Concurrently, the mature field of optimal control theory, deeply rooted in mathematics and engineering, developed robust methods for designing controllers that optimize system behavior over time. Techniques like dynamic programming provided rigorous frameworks for sequential decision-making, often assuming a complete and accurate model of the environment. While highly effective in well-defined, lower-dimensional problems with clear state representations, these classical approaches faced significant scalability challenges. They struggled immensely when confronted with high-dimensional state spaces (e.g., raw pixel inputs), continuous action spaces (e.g., robotic joint torques), or complex, unknown environments where a perfect model was unavailable or intractable to define. This <em>"curse of dimensionality"</em> was a primary barrier to applying RL to real-world complexities.</p><h3>Deep Reinforcement Learning (Deep RL): Merging Perception with Decision-Making</h3><p>Deep Reinforcement Learning emerged as a powerful paradigm from the synergistic combination of Deep Learning's unparalleled representation learning capabilities and Reinforcement Learning's robust decision-making frameworks. This merger directly addressed the critical scalability limitations that plagued classical RL algorithms. By employing deep neural networks as highly flexible and powerful function approximators, Deep RL agents can effectively handle high-dimensional, raw sensory inputs such as pixels from a camera, raw audio signals, or complex sensor readings.</p><p>In this integrated architecture, the deep neural network serves as the "perception" layer. It processes the raw observations, automatically extracting meaningful, abstract features and representations that are crucial for understanding the environment's state. Subsequently, the classical RL algorithms leverage these rich, learned representations to learn optimal "decision-making" strategies. The deep network can approximate complex non-linear mappings from states to actions, value functions, or even environment models, allowing the agent to generalize from a limited set of experiences. This profound integration allowed RL to move far beyond tabular methods and simple, hand-engineered state representations, enabling agents to learn directly from raw sensor data and operate effectively in highly complex, realistic, and previously intractable environments.</p><h3>Pioneering Achievements: From AlphaGo's 'Move 37' to Emergent Behaviors and 'The Bitter Lesson'</h3><p>The advent of Deep RL ushered in a series of groundbreaking achievements that not only captured widespread public attention but also profoundly demonstrated the paradigm's immense potential. A landmark success was DeepMind's Deep Q-Network (DQN), which achieved human-level performance across a diverse suite of Atari 2600 video games by learning directly from raw pixel inputs, showcasing the power of end-to-end learning.</p><p>A more profound milestone, however, was AlphaGo's historic victory over the world champion Go player, Lee Sedol. Notably, <strong>Move 37</strong> in Game 2, an unconventional and seemingly counter-intuitive move, demonstrated a level of strategic intuition and creativity previously thought exclusive to human masters. This achievement highlighted Deep RL's capacity for complex, long-term planning and its ability to discover novel, highly effective strategies in domains with immense state spaces. </p><p>Beyond specific games, Deep RL has enabled the emergence of sophisticated and often surprising behaviors in diverse domains, including complex robotic control, multi-agent coordination, and intricate simulations. These successes frequently involve agents discovering non-intuitive or highly optimized solutions that surpass human-designed approaches. These groundbreaking achievements collectively underscore "The Bitter Lesson," a concept emphasizing that general methods that leverage massive amounts of computation and data scale more effectively and ultimately outperform human-designed knowledge or handcrafted features, reinforcing the power of end-to-end learning in complex domains.</p><h2>RL in Action: Diverse Applications Across Industries</h2><p>The previous sections established the foundational principles of Reinforcement Learning (RL) and explored the transformative impact of Deep Learning on its capabilities. With a robust understanding of how RL agents learn optimal behaviors through interaction and cumulative reward, this section now shifts focus to the practical deployment of RL across various industries.</p><p>Reinforcement Learning's inherent ability to navigate complex, dynamic environments and solve sequential decision-making problems makes it uniquely suited for a wide array of real-world applications. Its versatility allows it to not only optimize existing processes and discover novel strategies but also to enable autonomous agents in scenarios where traditional, rule-based methods fall short due to the sheer complexity or uncertainty of the environment. RL's capacity for adaptive learning from experience is key to its success in these diverse domains.</p><p>The following areas represent significant domains where Reinforcement Learning is actively being applied:</p><h3>Game Playing and Robotics: Mastering Complex Control and Emergent Behaviors</h3><p>Reinforcement Learning has demonstrated remarkable success in mastering complex games, often surpassing human capabilities. These environments provide structured, yet highly dynamic, scenarios with clear reward signals, allowing agents to learn optimal policies for navigating vast state-action spaces. Similarly, in robotics, RL enables agents to learn intricate motor control and develop sophisticated behaviors directly from interaction with physical or simulated environments. This includes tasks ranging from manipulation and locomotion to complex navigation, where the agent learns to adapt its actions based on real-time sensory input to achieve desired objectives.</p><h3>Healthcare and Finance: Optimizing Decisions in Critical Domains</h3><p>In critical sectors such as healthcare and finance, RL offers powerful tools for optimizing decision-making processes under uncertainty. In healthcare, potential applications include personalizing treatment recommendations by adapting to patient responses over time, optimizing drug discovery pipelines through iterative experimentation, and managing chronic diseases with dynamic intervention strategies. In finance, RL can be applied to develop adaptive trading strategies, optimize portfolio allocation, and enhance risk management by learning from market dynamics and making sequential decisions that aim to maximize long-term returns while mitigating exposure. The ability of RL to account for long-term consequences and stochastic environments is particularly valuable here.</p><h3>Recommender Systems and Resource Management: Personalized Experiences and Efficiency</h3><p>RL's capacity for sequential decision-making extends to enhancing user experiences and optimizing resource allocation. In recommender systems, RL agents can learn to suggest items that not only satisfy immediate user preferences but also maximize long-term user engagement and satisfaction by understanding the evolving user journey and predicting future interactions. For resource management, RL can optimize complex systems such as energy consumption in smart grids, traffic flow in urban networks, or logistical operations in supply chains. By making adaptive decisions based on real-time data and environmental feedback, RL agents can improve efficiency, reduce waste, and enhance system performance dynamically.</p><h3>Beyond Traditional Fields: New Frontiers of Reinforcement Learning Application</h3><p>The applicability of Reinforcement Learning continues to expand beyond these established domains, demonstrating its profound versatility. New frontiers are constantly being explored, showcasing RL's potential to address novel challenges in areas ranging from accelerated scientific discovery and materials design to intelligent infrastructure management and smart cities. In these emerging fields, RL's core principle of learning optimal actions through interaction&#8212;even when the optimal path is unknown or the environment is highly complex&#8212;remains a powerful paradigm for solving increasingly intricate problems and driving innovation.</p><h2>Beyond the Basics: Advanced Concepts and Future Challenges in Deep Reinforcement Learning</h2><p>The previous sections established the foundational principles and diverse applications of Reinforcement Learning (RL) and Deep Reinforcement Learning (Deep RL). While Deep RL has achieved remarkable successes, its deployment in complex, real-world scenarios still presents significant challenges. This section explores advanced concepts and addresses key limitations, outlining the future trajectory of Deep RL and its role in the pursuit of Artificial General Intelligence (AGI).</p><h3>Addressing Limitations: Sparse Rewards and Sample Efficiency</h3><p>A primary challenge in many complex RL environments is the issue of <strong>sparse rewards</strong>. In such scenarios, positive feedback is infrequent or only occurs at the culmination of a long sequence of actions, making it difficult for an agent to learn which actions contributed to the eventual success. For example, in a complex robotic assembly task, a reward might only be given upon successful completion of the entire assembly, rather than for each intermediate step. This sparsity significantly complicates the <strong>credit assignment problem</strong>, where the agent struggles to attribute success or failure to specific preceding actions. Without clear, frequent signals, the learning process can be extremely slow or even fail to converge on an effective policy.</p><p>Closely related is <strong>sample efficiency</strong>. Deep RL algorithms often require an immense number of interactions with the environment to learn an effective policy. This extensive data requirement can be prohibitive in real-world applications where interactions are costly, time-consuming, or unsafe (e.g., in robotics for physical damage or in healthcare for patient safety). Research in this area focuses on developing methods that enable agents to learn effectively from limited experience, such as experience replay enhancements, model-based RL, and curriculum learning, although specific methodologies are beyond the scope of this discussion. Improving sample efficiency is crucial for making Deep RL practical for real-world deployment.</p><h3>Learning from Others: Imitation Learning and Inverse Reinforcement Learning</h3><p>When direct reward engineering is challenging or expert demonstrations are readily available, alternative paradigms like <strong>Imitation Learning (IL)</strong> and <strong>Inverse Reinforcement Learning (IRL)</strong> become valuable. <strong>Imitation Learning</strong> allows an agent to learn a policy by observing and mimicking an expert's behavior. Conceptually, this treats the problem as a supervised learning task where the expert's observations are inputs and their corresponding actions are the desired outputs (labels). This approach bypasses the need for a manually designed reward function altogether, directly learning a behavioral policy from demonstrations.</p><p><strong>Inverse Reinforcement Learning</strong> takes a different, more profound approach. Instead of learning a policy from a predefined reward, IRL aims to infer the underlying reward function that best explains an observed expert's behavior. The premise is that the expert is acting optimally with respect to some unknown reward function. Once this implicit reward function is inferred, standard RL algorithms can then be applied to optimize a policy for that learned reward. Both IL and IRL offer powerful ways to leverage human expertise to accelerate and simplify the learning process for complex tasks, especially where specifying explicit rewards is impractical.</p><h3>Adapting and Transferring Knowledge: Transfer Learning and Meta-learning ('Learning to Learn')</h3><p>For RL agents to be truly versatile and efficient, they must possess the ability to adapt to new tasks and environments without starting from scratch. <strong>Transfer Learning</strong> in RL involves leveraging knowledge acquired from solving one task or in one environment to improve learning performance on a different, but related, task or environment. This can manifest as pre-training a policy or value function on a simpler or related task, or transferring learned features and representations that are broadly applicable. The goal is to reduce the training time and data required for new tasks by capitalizing on previously gained insights.</p><p><strong>Meta-learning</strong>, often referred to as "learning to learn," pushes this concept further. A meta-learning agent is trained across a distribution of tasks such that it can quickly adapt to a new, unseen task with minimal additional training data or interactions. This involves learning general learning strategies, optimal initialization parameters, or efficient optimization procedures that enable rapid adaptation to novel situations. Unlike traditional transfer learning which reuses learned <em>knowledge</em>, meta-learning focuses on learning <em>how to learn</em> effectively across a family of tasks. These approaches are critical for achieving greater generalization and reducing the prohibitive sample complexity often associated with training agents for diverse real-world applications.</p><h3>The Path to General Intelligence: Reinforcement Learning with Human Feedback (RLHF) and Current Challenges</h3><p>Reinforcement Learning plays a pivotal role in the ongoing pursuit of Artificial General Intelligence (AGI). A key development in this direction, particularly for large language models (LLMs), is <strong>Reinforcement Learning with Human Feedback (RLHF)</strong>. RLHF involves fine-tuning a model (e.g., an LLM) using human preferences as a reward signal. Humans provide feedback by rating the quality, helpfulness, or safety of different model outputs. An RL agent then learns a policy that aligns the model's behavior more closely with human values and intentions, effectively "teaching" the model what constitutes a good response. This approach has been instrumental in making LLMs more helpful, harmless, and honest, demonstrating RL's power in aligning complex AI systems with human objectives.</p><p>Despite these advancements, significant conceptual and practical challenges remain on the path to AGI. These include developing agents that can robustly handle open-ended, ill-defined problems, exhibit common-sense reasoning, and possess true understanding beyond mere pattern matching or statistical correlation. While RL provides a powerful framework for learning complex behaviors, further research is essential to overcome current limitations and unlock its full potential in creating adaptable, truly intelligent systems capable of tackling the most complex real-world problems.</p><h2>Conclusion</h2><p>Reinforcement Learning, especially Deep RL, has emerged as a transformative force in AI, empowering intelligent agents to learn optimal behaviors through iterative interaction and cumulative reward in complex, dynamic environments. Its fusion with deep neural networks has overcome previous limitations, leading to groundbreaking achievements across diverse domains from gaming and robotics to healthcare and finance. While challenges like sample efficiency and sparse rewards remain, ongoing research into advanced techniques such as transfer learning and RLHF is continually expanding its capabilities. As we push these frontiers, RL is set to be a cornerstone in developing more adaptable, truly intelligent systems and advancing the path towards Artificial General Intelligence.</p>]]></content:encoded></item><item><title><![CDATA[Implementing GPT-Style Attention: A Step-by-Step Guide with PyTorch]]></title><description><![CDATA[Learn how to build and optimize attention mechanisms for transformer models, from basic self-attention to the multi-head attention architecture used in state-of-the-art language models]]></description><link>https://neuraforge.substack.com/p/implementing-gpt-style-attention</link><guid isPermaLink="false">https://neuraforge.substack.com/p/implementing-gpt-style-attention</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Tue, 21 Jan 2025 07:15:20 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Attention mechanisms have revolutionized the field of natural language processing (NLP) in recent years, enabling models to effectively capture long-range dependencies and achieve state-of-the-art performance on a wide range of tasks. At the heart of modern language models like the Transformer and GPT series lies the self-attention mechanism, a powerful tool for relating different positions within an input sequence.</p><p>In this blog post, we'll explore the inner workings of attention, starting from the limitations of traditional approaches and building up to the efficient multi-head attention used in today's cutting-edge models. You can follow this blog post along with this <a href="https://colab.research.google.com/drive/1fi1YFBixhPjConeVDogSxcwiPSSrvkZX?usp=sharing">Colab notebook</a>.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3956" height="2220" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2220,&quot;width&quot;:3956,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;cable network&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="cable network" title="cable network" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Taylor Vick</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Recap: The Problem with Modeling Long Sequences</h2><p>Recurrent Neural Networks (RNNs) and encoder-decoder models have been widely used for processing sequential data in natural language processing tasks. However, these architectures face several challenges when dealing with long sequences:</p><ul><li><p><strong>Information Bottleneck Problem</strong>: RNNs and encoder-decoder models compress the entire input sequence into a fixed-size hidden state vector. As the sequence length grows, it becomes increasingly difficult to pack all the necessary information into this fixed-size representation. Important details from earlier parts of the sequence can be lost or overwritten as the hidden state is updated at each step, making it challenging to capture long-range dependencies.</p></li><li><p><strong>Vanishing or Exploding Gradient Problem</strong>: During training, as the gradient signal is backpropagated through time, it can either decay exponentially (vanishing) or grow exponentially (exploding).</p><ul><li><p>Vanishing gradients make it difficult for the model to learn long-range dependencies, as the gradient signal becomes too weak to effectively update earlier parts of the network.</p></li><li><p>Exploding gradients can cause the model to become unstable and diverge during training.</p></li></ul></li></ul><div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!25PW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" width="375" height="134" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:134,&quot;width&quot;:375,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3488,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"></picture><div></div></div></a><figcaption class="image-caption">Source: https://github.com/sooftware/seq2seq</figcaption></figure></div><p>In encoder-decoder models, the decoder has limited access to the context from the input sequence. The fixed-size hidden state from the encoder is the only information available to the decoder at each generation step, which can be insufficient for capturing all the relevant context. This is particularly problematic for tasks like machine translation, where understanding the entire source sequence is crucial for generating accurate translations.</p><p>Furthermore, RNNs and encoder-decoder models process the input sequence sequentially, one token at a time. This sequential processing can be time-consuming, especially for long sequences, as the computation cannot be easily parallelized. Each hidden state update depends on the previous hidden state, creating an inherent sequential dependency that limits the ability to take advantage of modern hardware like GPUs that excel at parallel processing.</p><p>These limitations motivated the development of attention mechanisms, which allow the model to selectively focus on different parts of the input sequence during processing. By enabling the model to access and utilize relevant information from the entire sequence, attention mechanisms can effectively capture long-range dependencies and overcome the limitations of fixed-size hidden state representations.</p><h2>Capturing Dependencies with Attention Mechanisms</h2><p>To address the limitations of RNNs and encoder-decoder models, researchers introduced attention mechanisms. Attention allows the model to selectively focus on different parts of the input sequence during processing, enabling it to capture long-range dependencies more effectively.</p><p>One of the first attention mechanisms proposed was the <em><strong>Bahdanau</strong></em> attention, introduced in 2014 for neural machine translation. In this approach, the decoder can attend to relevant parts of the source sequence at each generation step, rather than relying solely on the fixed-size hidden state from the encoder. This is achieved by computing attention weights that determine the importance of each source token for the current decoding step.</p><p>The attention mechanism works by calculating a compatibility score between the current decoder hidden state and each encoder hidden state. These scores are then normalized using a softmax function to obtain attention weights. The weighted sum of the encoder hidden states, based on the attention weights, forms the context vector that provides relevant information to the decoder at each step.</p><p>By allowing the decoder to access and utilize information from the entire source sequence, the Bahdanau attention mechanism enables the model to capture long-range dependencies and generate more accurate translations. This approach laid the foundation for subsequent developments in attention mechanisms.</p><p>Building upon this idea, the Transformer architecture, introduced in the influential paper <em><strong>"Attention Is All You Need" by Vaswani et al. in 2017</strong></em>, took attention to the next level with the self-attention mechanism. Self-attention extends the attention concept to capture dependencies within a single sequence, rather than just between the encoder and decoder.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!WXL9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" width="380" height="560" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:560,&quot;width&quot;:380,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:71606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Transformer Architecture proposed by Vasvani et al in 2017</a></figcaption></figure></div><p>In self-attention, each token in the input sequence attends to all other tokens in the sequence, allowing the model to capture rich, context-dependent representations. This mechanism enables the model to directly learn the relationships and dependencies between different positions in the sequence, without relying on recurrent or convolutional operations.</p><p>The self-attention mechanism forms the core of the Transformer architecture and has revolutionized natural language processing. It has led to the development of powerful language models like BERT, GPT, and their variants, which have achieved state-of-the-art performance on a wide range of tasks, including language understanding, generation, and translation.</p><p>By leveraging attention mechanisms, particularly self-attention, models can effectively capture long-range dependencies, handle variable-length sequences, and process information in parallel. This has greatly enhanced the ability of models to understand and generate coherent and contextually relevant language.</p><p>In the following sections, we will dive deeper into the details of self-attention and explore its implementation in modern language models.</p><h2>Simplified Self-Attention Mechanism</h2><p>To gain a better understanding of how self-attention works, let's start with a simplified version of the mechanism and walk through the computation step by step. Consider an input sequence <code>X</code> of length 6, where each token is represented by a 3-dimensional embedding vector. The goal of self-attention is to compute a new representation for each token that incorporates information from the entire sequence. This is achieved by calculating attention weights between pairs of tokens and using these weights to compute weighted sums of the input embeddings.</p><p>First, we compute the dot product between each pair of token embeddings. The dot product serves as a measure of similarity between tokens, indicating how much they should attend to each other. In PyTorch, we can compute the dot products efficiently using matrix multiplication:</p><pre><code><code>import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

attn_scores = inputs @ inputs.T
</code></code></pre><p>In the code above, <code>attn_scores[i][j]</code> represents the dot product between the embeddings of tokens i and j. The resulting <code>attn_scores</code> matrix captures the similarity scores between all pairs of tokens.</p><p>Next, we apply the softmax function to each row of the <code>attn_scores</code> matrix to obtain the attention weights. The softmax function normalizes the scores, converting them into probabilities that sum up to 1. This ensures that the attention weights can be interpreted as the relative importance of each token for a given token:</p><pre><code><code>attn_weights = torch.softmax(attn_scores, dim=-1)</code></code></pre><p>After applying the softmax function, <code>attn_weights[i][j]</code> represents the normalized attention weight indicating how much token i attends to token j.</p><p>Finally, we compute the self-attended representations by taking a weighted sum of the input embeddings using the attention weights:</p><pre><code><code>context_vecs = attn_weights @ inputs</code></code></pre><p>The resulting <code>context_vecs</code> matrix contains the self-attended representations for each token. Each row in <code>context_vecs</code>is a weighted sum of the input embeddings, where the weights are determined by the attention weights. This allows each token to incorporate information from the entire sequence, weighted by the relevance of each token.</p><p>This simplified version of self-attention demonstrates the core idea of allowing tokens to attend to each other and compute new representations based on the entire sequence. However, in practice, the self-attention mechanism used in Transformer models includes additional components, such as trainable weight matrices and scaling factors, which we will explore in the next section.</p><h2>Math Behind Self-Attention</h2><p>In the previous section, we explored a simplified version of self-attention that directly used the input embeddings to compute attention scores and context vectors. However, in practice, the self-attention mechanism used in Transformer models incorporates trainable weight matrices to project the inputs into query, key, and value representations before computing the attention scores.</p><p>In self-attention, each input vector <strong>xi</strong> is projected onto three distinct vectors: query <strong>qi</strong>, key <strong>ki</strong>, and value <strong>vi</strong>. </p><p>These projections are performed via learnable weight matrices <strong>Wq</strong>, <strong>Wk</strong>, and <strong>Wv</strong>, resulting in:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;q_i = x_i W_Q, \\quad k_i = x_i W_K, \\quad v_i = x_i W_V&quot;,&quot;id&quot;:&quot;OTDSYPPSNS&quot;}" data-component-name="LatexBlockToDOM"></div><p>These weight matrices are initialized randomly and optimized during training.</p><p>The simplified matrix representation, where the query, key, and value matrices are computed as a single operation, is given by:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\text{attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V&quot;,&quot;id&quot;:&quot;IYMDIGSHSI&quot;}" data-component-name="LatexBlockToDOM"></div><p>The working of the above attention calculation is explained in the next section.</p><h2>Adding Trainable Weights to Self-Attention</h2><p>The query, key, and value matrices (Q, K, V) are obtained by multiplying the input embedding matrix <code>X</code> with learned weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>, respectively:</p><pre><code><code>import torch.nn as nn

d_in = 3
d_out = 2

W_query = nn.Linear(d_in, d_out)
W_key   = nn.Linear(d_in, d_out)
W_value = nn.Linear(d_in, d_out)

print(W_query)

# Calculate queries, keys and values
queries = W_query(inputs)
keys    = W_key(inputs)
values  = W_value(inputs)</code></code></pre><p>Here, <code>d_in</code> represents the input embedding dimension, and <code>d_out</code> represents the output dimension of the projected queries, keys, and values. The weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code> are initialized randomly and learned during the training process.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8H5b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" width="418" height="641" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:641,&quot;width&quot;:418,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Scaled Dot-Product Attention</a></figcaption></figure></div><p>By projecting the inputs into separate query, key, and value spaces, the model can learn to capture different aspects of the input embeddings that are relevant for computing attention. This allows for more expressive and flexible representations compared to directly using the input embeddings.</p><p>After obtaining the queries, keys, and values, the attention scores are computed as the scaled dot product between the queries and keys:</p><pre><code><code>attn_scores = queries @ keys.T / keys.shape[-1]**0.5
print(attn_scores)</code></code></pre><p>The scaling factor (the square root of the key dimension) is introduced to mitigate the effect of large magnitudes in the dot products, which can lead to extremely small gradients when passed through the <strong>softmax</strong> function. This scaling helps stabilize the training process and improve convergence.</p><p>Once the attention scores are computed, the rest of the self-attention mechanism remains the same as in the simplified version. The attention weights are obtained by applying the <strong>softmax</strong> function to the scores, and the context vectors are computed as the weighted sum of the values using the attention weights.</p><p>To encapsulate this computation in a more compact and reusable form, we can define a Python class that implements the self-attention mechanism with trainable weights:</p><pre><code><code>class SelfAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()

        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

    def forward(self, x):
        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vecs = attn_weights @ values

        return context_vecs
</code></code></pre><p>The <code>SelfAttention</code> class uses PyTorch's <code>nn.Linear</code> module to define the trainable weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>. The <code>forward</code> method performs the self-attention computation, taking the input embeddings <code>x</code> and returning the context vectors.</p><p>By incorporating trainable weights into the self-attention mechanism, Transformer models can learn to adapt the attention computation to the specific requirements of the task at hand. This flexibility and expressiveness have contributed to the success of Transformer-based models in various natural language processing tasks.</p><h2>Causal Attention: Masking Future Tokens</h2><p>In certain tasks, such as language modeling or text generation, it's crucial to prevent the self-attention mechanism from accessing information from future tokens. This is where causal attention, also known as masked attention, comes into play.</p><p>Causal attention restricts the self-attention computation to only consider the tokens up to the current position in the sequence. In other words, when computing the attention scores for a given token, only the tokens that appear before it in the sequence are considered.</p><p>To achieve this, we modify the attention weight matrix by applying a mask that sets the upper triangular part of the matrix to negative infinity. This effectively prevents the model from attending to future tokens.</p><p>Here's an example of how to create the mask and apply it to the attention scores:</p><pre><code><code>import torch

def create_mask(context_length):
    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
    return mask.bool()

context_length = attn_scores.shape[0]
mask = create_mask(context_length)
attn_scores = attn_scores.masked_fill(mask, -torch.inf)
print(attn_scores)
</code></code></pre><p>In the code above, we create a mask using PyTorch's <code>torch.triu</code> function, which sets the elements above the main diagonal to 1 and the rest to 0. We then convert the mask to a boolean tensor and use it to fill the upper triangular part of the <code>attn_scores</code> matrix with negative infinity.</p><p><strong>By setting the masked positions to negative infinity, we ensure that the softmax function will assign zero attention weights to those positions, effectively ignoring the future tokens.</strong></p><p>After applying the mask, we proceed with the rest of the self-attention computation as usual, applying the softmax function to obtain the attention weights and computing the context vectors.</p><pre><code><code>attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
print(attn_weights)</code></code></pre><p>It's worth noting that causal attention is particularly important in autoregressive models like GPT, where the model generates tokens sequentially and should only have access to the previously generated tokens at each step.</p><h3>Adding Dropout</h3><p>In addition to masking future tokens, we can also incorporate dropout regularization to the attention weights. Dropout helps prevent overfitting by randomly setting a fraction of the attention weights to zero during training. This encourages the model to rely on a broader set of tokens and reduces the risk of memorizing specific patterns.</p><p>Here's an example of how to apply dropout to the attention weights:</p><pre><code><code>dropout = nn.Dropout(p=0.1)
attn_weights = dropout(attn_weights)</code></code></pre><p>In the code above, we create an instance of PyTorch's <code>nn.Dropout</code> module with a dropout probability of 0.1. We then apply the dropout to the <code>attn_weights</code> matrix, randomly setting 10% of the attention weights to zero and increasing the remaining values in the matrix by 10%.</p><p>By incorporating causal attention and dropout regularization, we can effectively mask future tokens and improve the generalization ability of our self-attention-based models.</p><p>To encapsulate the causal attention mechanism, we can define a <code>CausalAttention</code> class that inherits from the <code>SelfAttention</code> class and adds the masking and dropout functionality:</p><pre><code><code>import torch.nn as nn

class CausalAttention(nn.Module):
    """
    Implements causal attention with dropout and masking.

    Args:
        d_in (int): Input embedding dimension.
        d_out (int): Output embedding dimension.
        context_length (int): Length of the context (number of tokens).
        dropout (float): Dropout rate. Default is 0.1.
        qkv_bias (bool): If True, adds a learnable bias to the Q, K, V projections. Default is False.
    """
    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):
        super().__init__()

        # Linear layers for K, Q, V projections
        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
            'mask',
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        """
        Forward pass for causal attention.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_tokens, d_in).

        Returns:
            torch.Tensor: Context vectors of shape (batch_size, num_tokens, d_out).
        """

        b, num_tokens, d_in = x.shape

        # Compute keys, queries, and values
        keys = self.W_keys(x)
        query = self.W_query(x)
        values = self.W_value(x)

        # Calculate attention scores
        att_scores = query @ keys.transpose(1, 2)  # Transpose to get (batch_size, num_tokens, num_tokens)

        # Apply mask to prevent attending to future tokens
        att_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)

        # Compute attention weights
        attn_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)

        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights)

        # Compute context vectors
        context_vec = attn_weights @ values

        return context_vec</code></code></pre><p>The <code>CausalAttention</code> class takes additional arguments <code>context_length</code> and <code>dropout</code> to specify the maximum sequence length and the dropout probability, respectively. It also registers the mask as a buffer to ensure it is properly moved to the appropriate device along with the model.</p><p>In the <code>forward</code> method, we apply the mask to the attention scores using <code>masked_fill</code>, ensuring that future tokens are ignored. We then apply the <strong>softmax</strong> function, perform dropout regularization, and compute the context vectors as before.</p><p>By using the <code>CausalAttention</code> class, we can easily incorporate causal attention and dropout regularization into our self-attention-based models, enabling them to handle tasks that require masking future tokens.</p><h2>Multi-Head Attention</h2><p>Multi-head attention is an extension of the self-attention mechanism that allows the model to attend to different parts of the input sequence in multiple ways simultaneously. Instead of performing a single attention operation, multi-head attention splits the input embeddings into multiple smaller matrices (heads) and applies self-attention to each head independently. The results from all heads are then concatenated and linearly transformed to produce the final output.</p><p>The motivation behind multi-head attention is to enable the model to capture different types of relationships and dependencies within the input sequence. Each head can focus on different aspects of the input, allowing the model to learn a more diverse and nuanced representation.</p><p>Here's a step-by-step breakdown of the multi-head attention process:</p><ul><li><p><strong>Splitting the Input Embeddings</strong>: The input embeddings are split into multiple smaller matrices, each representing a different head. The number of heads is a hyperparameter that can be tuned based on the specific task and model architecture. If the input embeddings have dimension <code>d_out</code> and there are <code>num_heads</code> heads, each head will have a dimension of <code>d_head = d_out // num_heads</code>.</p></li><li><p><strong>Applying Self-Attention to Each Head</strong>: For each head, the input embeddings are projected into query, key, and value matrices using separate linear transformations. The self-attention mechanism is then applied to each head independently, computing the attention scores, attention weights, and context vectors for each head.</p></li><li><p><strong>Concatenating the Head Outputs</strong>: The context vectors from all heads are concatenated along the embedding dimension to form a single matrix. This concatenated matrix has a dimension of <code>d_model</code>, which is the same as the original input embeddings.</p></li><li><p><strong>Linear Transformation</strong>: The concatenated matrix is passed through a final linear transformation to produce the output of the multi-head attention mechanism. This linear transformation allows the model to combine and mix the information from different heads.</p></li></ul><pre><code><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, num_heads, context_length, dropout):
        super().__init__()
        
        assert (d_out % num_heads == 0), "d_out must be divisible by num_heads" 
        
        self.d_in = d_in
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        # Linear layers for Q, K, V projections
        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

&#9;&#9;# Linear layer to combine head outputs
        self.out_proj = nn.Linear(d_out, d_out)

&#9;&#9;# Dropout layer
        self.dropout = nn.Dropout(dropout)

&#9;&#9;# Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
&#9;&#9;    "mask", 
&#9;&#9;&#9;torch.triu(torch.ones(context_length, context_length), diagonal=1)
&#9;&#9;)
        
    def forward(self, x):
    
        batch_size, num_tokens, d_in = x.shape

&#9;&#9;# Compute keys, queries, and values
        keys = self.W_key(x)      # (batch_size, num_tokens, d_out)
        queries = self.W_query(x) # (batch_size, num_tokens, d_out)
        values = self.W_value(x)  # (batch_size, num_tokens, d_out)


&#9;&#9;# Reshape to (batch_size, num_tokens, num_heads, head_dim)
        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)

&#9;&#9;# Transpose to (batch_size, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

&#9;&#9;# Calculate attention scores
        attn_scores = queries @ keys.transpose(2, 3) 
        # Shape: (batch_size, num_heads, num_tokens, num_tokens)

&#9;&#9;# Apply mask to prevent attending to future tokens
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)

&#9;&#9;# Compute attention weights
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        
        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights) 


&#9;&#9;# Compute context vectors
&#9;&#9;context_vec = attn_weights @ values 
&#9;&#9;# Shape: (batch_size, num_heads, num_tokens, head_dim)

        # Reshape and combine heads
        context_vec = context_vec.transpose(1, 2) # (batch_size, num_tokens, num_heads, head_dim)
        context_vec = context_vec.contiguous().view(batch_size, num_tokens, -1)
        # (batch_size, num_tokens, d_out)
        
        # Apply final linear projection
&#9;&#9;context_vec = self.out_proj(context_vec) # (batch_size, num_tokens, d_out)

&#9;&#9;return context_vec
</code></code></pre><p>The <code>__init__</code> method initializes the necessary parameters and modules for multi-head attention:</p><ul><li><p>It takes the input dimension <code>d_in</code>, output dimension <code>d_out</code>, number of heads <code>num_heads</code>, context length <code>context_length</code>, and dropout probability <code>dropout</code>.</p></li><li><p>It asserts that <code>d_out</code> is divisible by <code>num_heads</code> to ensure proper splitting of dimensions.</p></li><li><p>It initializes the linear transformations for the query, key, and value matrices (<code>self.W_query</code>, <code>self.W_key</code>, <code>self.W_value</code>) and an additional output projection matrix (<code>self.out_proj</code>).</p></li><li><p>It also registers the causal mask as a buffer using <code>self.register_buffer()</code>.</p></li></ul><p>The <code>forward</code> method performs the multi-head attention computation:</p><ol><li><p>It applies the linear transformations to the input <code>x</code> to obtain the query, key, and value matrices (<code>queries</code>, <code>keys</code>, <code>values</code>).</p></li><li><p>It splits the matrices into multiple heads by reshaping and transposing the tensors. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>, where <code>head_dim</code> is <code>d_out // num_heads</code>.</p></li><li><p>It computes the attention scores by performing matrix multiplication between the queries and keys using the <code>@</code>operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, num_tokens)</code>.</p></li><li><p>It applies the causal mask to the attention scores using <code>masked_fill_()</code>, setting the upper triangular part to negative infinity. This ensures that each token can only attend to the tokens that appear before it in the sequence.</p></li><li><p>It applies the softmax function to the masked attention scores to obtain the attention weights. The scaling factor <code>keys.shape[-1]**0.5</code> is used to stabilize the gradients.</p></li><li><p>It computes the context vectors by multiplying the attention weights with the values using the <code>@</code> operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>.</p></li><li><p>It transposes and reshapes the context vectors to <code>(batch_size, num_tokens, d_out)</code> to combine the outputs from all heads.</p></li><li><p>It applies the output projection matrix (<code>self.out_proj</code>) to the combined context vectors to obtain the final output.</p></li></ol><p>The <code>MultiHeadAttention</code> class efficiently implements multi-head attention by performing the computations in a single pass. It takes advantage of tensor operations and reshaping to parallelize the computations across multiple heads.</p><p>By using this efficient implementation, the model can capture different types of relationships and dependencies within the input sequence, allowing it to learn more expressive and nuanced representations for various natural language processing tasks.</p><p>The <code>MultiHeadAttention</code> class can be used as a building block in larger models, such as the Transformer architecture, to leverage the power of multi-head attention in a computationally efficient manner.</p><h2>The GPT Architecture: Harnessing the Power of Multi-Head Attention</h2><p>The GPT (Generative Pre-trained Transformer) architecture, which includes models like GPT-2 and GPT-3, has revolutionized the field of natural language processing. At the core of the GPT architecture lies the multi-head attention mechanism, which enables the model to capture rich linguistic patterns and generate coherent and contextually relevant text.</p><p>For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600. The embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out).</p><p>Here's a code snippet showcasing the initialization of the multi-head attention module in the GPT-2 architecture:</p><pre><code><code>torch.manual_seed(123)

# Sample inputs for num_heads = 12, d_in = d_out = 768, context_length = 1024
batch = torch.rand(2, 1024, 768) 
# Two inputs with 1024 tokens each; each token has embedding dimension 768.

print(batch.shape)

batch_size, context_length, d_in = batch.shape
d_out = 768
num_heads = 12

mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, 0.0)
context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
</code></code></pre><h2>Conclusion</h2><p>In this blog post, we have explored the concept of attention mechanisms and their significant impact on natural language processing tasks. Starting with a simplified version of self-attention and progressing to the more advanced and efficient multi-head attention, we have seen how attention allows models to selectively focus on relevant parts of the input sequence and capture long-range dependencies effectively. Self-attention enables each token to attend to every other token in the sequence, facilitating the learning of rich, context-dependent representations. Multi-head attention takes this a step further by allowing models to capture different types of relationships and dependencies simultaneously, enhancing their expressive power and ability to understand and generate natural language.</p><p>Congrats on sticking with the blog and understanding the importance of attention in coding the GPT-style models from scratch. Implementing attention mechanisms, especially multi-head attention, is crucial for building the Transformer architecture. By delving into the details and implementing it efficiently, you&#8217;ve gained valuable insights into the core component that powers many top-notch NLP models.</p><div><hr></div><p>Thanks for reading NeuraForge: AI Unleashed!</p><p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p><p>Connect with me on <a href="https://www.linkedin.com/in/narasimhakarthik/">LinkedIn</a>. </p><p></p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[The Ultimate Guide to Preparing Text Data for Language Modeling with PyTorch]]></title><description><![CDATA[Master tokenization, Byte Pair Encoding, Sampling windows, and Embeddings]]></description><link>https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text</link><guid isPermaLink="false">https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 06 Jan 2025 20:58:53 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>When working with large language models (LLMs), one of the most crucial steps is preparing the textual data in a format that these models can understand and learn from. This process involves converting raw text into numerical vectors, known as embeddings, as LLMs cannot directly process plain text.</p><p>In this post, we'll take a deep dive into the techniques and best practices for text preprocessing and embedding generation using PyTorch, a popular deep learning framework. We'll cover everything from basic tokenization to implementing advanced algorithms like Byte Pair Encoding (BPE), creating efficient data sampling techniques, and building embedding layers from scratch. By the end, you'll have a solid understanding of how to prepare text data for training powerful language models. To explore the concepts further and see the code in action, check out the accompanying Colab notebook <a href="https://colab.research.google.com/drive/1n6UjZRTRP0yRdcvJLHK2CRfNL8Gj_-IL?usp=sharing">here</a> and follow along with the step-by-step examples.</p><p>Let's get started!</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="5472" height="3648" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3648,&quot;width&quot;:5472,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;open book lot&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="open book lot" title="open book lot" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Patrick Tomasso</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Understanding Embeddings: The Bridge Between Text and Mathematics</h2><p>Before we dive into the technical implementation details, let's understand what embeddings are and why they're crucial for language models. Think of embeddings as a way to translate words into numbers &#8211; but not just any numbers. They're carefully crafted numerical representations that capture the meaning, relationships, and context of words in a way that computers can process.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!m1KP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" width="1100" height="566" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:566,&quot;width&quot;:1100,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:113900,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">Embedding representations</a></em></figcaption></figure></div><h3>What Are Embeddings and Why Do We Need Them?</h3><p>At their core, embeddings are dense vectors (arrays of numbers) that represent words or tokens in a continuous vector space. When you feed the word "cat" to a computer, you can't just use the letters "c-a-t" - computers need numbers to perform calculations. An embedding transforms "cat" into a vector like [0.2, -0.5, 0.8, ...], where each number helps represent different aspects of the word's meaning.</p><p>What makes embeddings powerful is their ability to capture semantic relationships. Words with similar meanings end up having similar numerical representations. For example, the embeddings for "cat" and "kitten" would be more similar to each other than to the embedding for "submarine". This similarity can be measured mathematically, allowing models to understand relationships between words.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!UPsT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:269534,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: <a href="https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings">Representation of embedding vectors in 2D</a></figcaption></figure></div><p>Modern embedding systems typically represent words in high-dimensional spaces. For example:</p><ul><li><p>GPT-2 uses 768-dimensional embeddings for its smallest model</p></li><li><p>GPT-3's largest model uses 12,288-dimensional embeddings</p></li><li><p>BERT-base uses 768-dimensional embeddings</p></li></ul><p>The real power of embeddings comes from their ability to learn from data. During model training, these embeddings are automatically adjusted to capture relationships present in the training data, adapting to specific domains and discovering nuanced patterns that might not be obvious to human designers.</p><h2>Text Tokenization and Preprocessing Techniques</h2><p>The first step in preparing text for LLMs is <strong>tokenization</strong> - breaking down raw text into smaller units called tokens. Tokens can be individual words, subwords, or even characters. The goal is to create a finite set of meaningful units that the model can learn from.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!rt5h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" width="862" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:862,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109042,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></figcaption></figure></div><p>However, raw text often contains noise and inconsistencies that can hinder the tokenization process. These include:</p><ul><li><p>Inconsistent casing (e.g., "Hello" vs "hello")</p></li><li><p>Punctuation attached to words (e.g., "world!")</p></li><li><p>Special characters and contractions (e.g., "don't", "U.S.A.")</p></li><li><p>Unknown or rare words</p></li></ul><p>To handle these issues and perform effective tokenization, we can use a combination of text preprocessing techniques and regular expressions in Python. Here's an example code snippet that demonstrates this:</p><pre><code><code>import re

UNK = '&lt;unk&gt;'  # Token for unknown words
EOS = '&lt;eos&gt;'  # Token for end of text

def tokenize(text, known_words):
    # Lowercase the text
    text = text.lower()

    # Split on whitespace and punctuation using regular expressions
    tokens = re.findall(r"\w+|[^\w\s]", text)

    # Replace unknown words with &lt;unk&gt; token
    tokens = [t if t in known_words else UNK for t in tokens]

    # Append &lt;eos&gt; token to the end of the text
    tokens.append(EOS)

    return tokens

# Example usage
text = "Hello, world! This is a sample sentence."
known_words = {'this', 'is', 'a', 'sample', 'sentence'}

print(tokenize(text, known_words))</code></code></pre><pre><code>['&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', 'this', 'is', 'a', 'sample', 'sentence', '&lt;unk&gt;', '&lt;eos&gt;']</code></pre><p>Let's break down the tokenization process step by step:</p><ol><li><p>First, we convert the entire text to lowercase using <code>text.lower()</code>. This ensures consistent casing across all words.</p></li><li><p>Next, we use a regular expression <code>r"\w+|[^\w\s]"</code> to split the text on whitespace and punctuation. The regex pattern <code>\w+</code> matches one or more word characters , while <code>[^\w\s]</code> matches any single character that is not a word character or whitespace. This effectively separates words and punctuation into individual tokens.</p></li><li><p>We then replace any unknown words (i.e., words not in the <code>known_words</code> set) with a special <code>&lt;unk&gt;</code> token. This helps the model handle out-of-vocabulary words gracefully during training and inference.</p></li><li><p>Finally, we append an <code>&lt;eos&gt;</code> token to the end of the tokenized text to mark the end of the sequence. This is useful for the model to learn when a text or document ends.</p></li></ol><h2>Understanding and Implementing Byte Pair Encoding</h2><p>While the tokenization approach we discussed so far works well for many cases, it has some limitations. One major drawback is the handling of unknown or rare words. Replacing all uncommon words with a generic <code>&lt;unk&gt;</code> token can lead to loss of information and hinder the model's ability to understand the nuances of the text.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Z7y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" width="602" height="279" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:279,&quot;width&quot;:602,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40598,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em>...</figcaption></figure></div><p>This is where Byte Pair Encoding (BPE) comes into play. BPE is a <strong>subword tokenization algorithm</strong> that iteratively builds a vocabulary of subword units based on their frequency in the training corpus. It starts with individual characters and progressively merges them into larger subword units until a desired vocabulary size is reached. This allows BPE to effectively handle out-of-vocabulary words by representing them as combinations of subword units.</p><p>Let's walk through a step-by-step example to better understand how BPE constructs its vocabulary. Imagine we have the following list of words:</p><pre><code><code>['low', 'lower', 'newest', 'widest']</code></code></pre><ul><li><p>Step 1: Initialization</p></li></ul><p>BPE begins by splitting each word into individual characters and appending a special end-of-word symbol, typically denoted by <code>&lt;/w&gt;</code>, to mark the end of each word. This initial segmentation looks like this:</p><pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre><ul><li><p>Step 2: Frequency Counting</p></li></ul><p>Next, BPE counts the frequency of each character pair in the corpus. In this example, the most frequent pair is <code>e</code> followed by <code>&lt;/w&gt;</code>, as it appears in two words: <code>lower</code> and <code>newest</code>:</p><ul><li><p>Step 3: Merging</p></li></ul><p>BPE merges the most frequent pair into a new subword unit. In our example, <code>e&lt;/w&gt;</code> becomes a single unit i.e. considered as single token in vocabulary:</p><pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre><ul><li><p>Step 4: Iteration</p></li></ul><p>The process of frequency counting and merging is repeated iteratively. In the next iteration, the most frequent pair is <code>es</code> followed by <code>&lt;/w&gt;</code>, so they get merged:</p><pre><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w es&lt;/w&gt;', 'w i d es&lt;/w&gt;']</code></pre><p>This iterative process continues until one of two conditions is met:</p><ol><li><p>A desired vocabulary size is reached (e.g., 10,000 subword units).</p></li><li><p>No more frequent pairs are found (i.e., all possible merges have been performed).</p></li></ol><p>The resulting set of subword units, along with their frequencies, forms the final BPE vocabulary. To further illustrate how BPE handles out-of-vocabulary words, let's consider an example. Suppose we have a BPE vocabulary that includes the subword units <code>low</code>, <code>est</code>, and <code>&lt;/w&gt;</code>, but not the word <code>lowest</code>. When encountering <code>lowest</code>, BPE would break it down into the known subword units:</p><pre><code>['low', 'est', '&lt;/w&gt;']</code></pre><p>By representing <code>lowest</code> as a combination of subword units, BPE enables the model to process and generate words it hasn't seen during training.</p><h3><br>Byte Pair Encoding in Python</h3><p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library. <code>tiktoken</code> is an existing Python open source library (<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>), which implements the BPE algorithm very efficiently based on source code in Rust. It can be installed as follows:</p><p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library:</p><pre><code>pip install tiktoken</code></pre><pre><code><code>import tiktoken

# Load the BPE tokenizer 
bpe_tokenizer = tiktoken.get_encoding("gpt2")

text = "This is an example of byte pair encoding! xhsbfubs"

# Tokenize the text using BPE
tokens = bpe_tokenizer.encode(text) 
decoded = bpe_tokenizer.decode(tokens) 

print(f"Encoded tokens: {tokens}") 
print(f"Decoded text: {decoded}")

# Encoded tokens: [1212, 318, 281, 1672, 286, 18022, 5166, 21004, 0, #  # 2124, 11994, 19881, 23161] 
# Decoded text: This is an example of byte pair encoding! xhsbfubs
</code></code></pre><p>The <code>tiktoken</code> library provides an efficient implementation of the BPE algorithm used by OpenAI's GPT models. We first load the BPE tokenizer with <code>tiktoken.get_encoding("gpt2")</code>, which gives us access to the same tokenizer used by the GPT-2 model.</p><p>We then encode our text using <code>bpe_tokenizer.encode(text)</code>, which applies the BPE algorithm and returns a list of token IDs. These IDs correspond to the subwords in the BPE vocabulary.</p><p>Finally, we can decode the token IDs back into the original text using <code>bpe_tokenizer.decode(tokens)</code>. This demonstrates that BPE can effectively tokenize and reconstruct the text without losing information.</p><p>The real power of BPE lies in its ability to handle out-of-vocabulary words. Since it breaks down words into subwords, even if a word is not explicitly present in the vocabulary, it can still be represented by a combination of subwords. This allows the model to understand and generate words it hasn't seen during training. By understanding and implementing Byte Pair Encoding, you can take your text preprocessing to the next level and build more powerful and versatile language models.</p><h2>Creating and Managing Sampling Windows</h2><p>Now that we have our text data tokenized into a sequence of token IDs, the next step is to prepare it for training our language model. But how exactly do we feed this data to the model?</p><p>To answer that, let's first understand how language models like GPT learn. During training, the model tries to predict the next token in a sequence given the tokens that come before it. For example, if the input is "The cat sat on the", the model learns to predict the next most likely token, such as "mat" or "couch".</p><p>To facilitate this learning process, we need to create input-target pairs from our tokenized text. The input will be a sequence of tokens, and the target will be the next token that follows this sequence. We can generate these pairs using a technique called <strong>sampling windows</strong>. The sampling window is popularly also known as <strong>context length</strong>.</p><p>Imagine our tokenized text as a long ribbon. We take a small window of a fixed size (say, 50 tokens) and slide it over the ribbon. At each step, the tokens inside the window become our input, and the token immediately following the window becomes the target. We keep sliding the window until we reach the end of the ribbon.</p><p>Here's a visual representation:</p><pre><code><code>[The, cat, sat, on, the, mat, ., &lt;eos&gt;]
 |   window 1    |
      |   window 2    |
           |   window 3    |</code></code></pre><p>In window 1, the input is <code>[The, cat, sat, on, the]</code> and the target is <code>mat</code>. In window 2, the input is <code>[cat, sat, on, the, mat]</code> and the target is <code>.</code>. And so on.</p><p>By creating these sampling windows, we break down our long text into manageable sequences that the model can learn from. The size of the window is a hyperparameter that we can tune. A larger window allows the model to learn from more context, but it also increases the computational complexity.</p><p>Now, let's see how we can implement this in Python. We'll use PyTorch's <code>Dataset</code> and <code>DataLoader</code> classes to create an efficient data pipeline.</p><pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, tokens, window_size):
        # Store the tokenized text
        self.tokens = tokens  
        # Store the size of the sampling window
        self.window_size = window_size  

    def __len__(self):
        # Return the total number of sampling windows
        return len(self.tokens) - self.window_size

    def __getitem__(self, idx):
        # Get the input-target pair for the given index
        input_seq = self.tokens[idx:idx+self.window_size]  # Input sequence
        target_seq = self.tokens[idx+1:idx+self.window_size+1]  # Target sequence (shifted by 1)
        return torch.tensor(input_seq), torch.tensor(target_seq)

# Example usage
tokens = [1212, 318, 281, 1672, 286, 2419, 683, 26254, 0] # Tokenized text
dataset = TextDataset(tokens, window_size=5)  # Create a TextDataset with window size of 5
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # Create a DataLoader with batch size of 2 and shuffling enabled

for inputs, targets in dataloader:
    print(inputs)  # Print the input sequences
    print(targets)  # Print the corresponding target sequences
    break  # Break after the first batch (for demonstration purposes)
</code></code></pre><pre><code>tensor([[ 1672, 286, 2419, 683, 26254], 
&#9;&#9;[ 1212, 318, 281, 1672, 286]]) 

tensor([[ 286, 2419, 683, 26254, 0], 
&#9;&#9;[ 318, 281, 1672, 286, 2419]])</code></pre><p>Let's break this down step by step:</p><ol><li><p>We define a custom <code>TextDataset</code> class that inherits from PyTorch's <code>Dataset</code> class. This class takes the tokenized text and the window size as input.</p></li><li><p>The <code>__len__</code> method returns the total number of sampling windows we can create from the tokenized text. We subtract the window size to avoid going out of bounds.</p></li><li><p>The <code>__getitem__</code> method is the heart of the dataset. It takes an index <code>idx</code> and returns the input-target pair for the corresponding sampling window. The input is <code>tokens[idx:idx+window_size]</code> and the target is <code>tokens[idx+1:idx+window_size+1]</code>, i.e., the input sequence shifted by one token.</p></li><li><p>We then create an instance of the <code>TextDataset</code> with our tokenized text and a window size of 5.</p></li><li><p>We wrap the dataset in a <code>DataLoader</code>, which allows us to batch the data and shuffle it for training. Here, we use a batch size of 2.</p></li><li><p>Finally, we loop over the dataloader to get batches of input-target pairs. Each input is a tensor of shape <code>(batch_size, window_size)</code>, and each target is a tensor of shape <code>(batch_size, window_size)</code>.</p></li></ol><p>Now consider the following code which provides the best practice for creating datasets and dataloaders:</p><pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class GPTDataset(Dataset):
    def __init__(self, text, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []
        
        # Tokenize entire text
        token_ids = tokenizer.encode(text)
        
        # Create overlapping sequences
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1:i + max_length + 1]
            
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))
            
    def __len__(self):
        return len(self.input_ids)
        
    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader(text, batch_size=4, max_length=256, stride=128):
    """Create an efficient data loader for training"""
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDataset(text, tokenizer, max_length, stride)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    return dataloader
</code></code></pre><h2>Building Token Embeddings from Scratch</h2><p>So far, we've seen how to preprocess text data and convert it into sequences of token IDs using techniques like tokenization and Byte Pair Encoding. The next crucial step is to transform these discrete token IDs into continuous vector representations, known as <strong>embeddings</strong>.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8wV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" width="1372" height="966" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:966,&quot;width&quot;:1372,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41167,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png</figcaption></figure></div><p>Embeddings are dense, low-dimensional vectors that capture semantic and syntactic information about the tokens. By representing tokens as embeddings, we enable the language model to learn meaningful relationships and patterns in the text data.</p><p>In PyTorch, we can create embeddings using the <code>nn.Embedding</code> layer. This layer maps each token ID to a corresponding vector of a specified size.</p><p>Here's an example of how to create an embedding layer in PyTorch:</p><pre><code><code>import torch
import torch.nn as nn

vocab_size = 10000  # Size of the vocabulary (number of unique tokens)
embed_size = 128  # Dimensionality of the embedding vectors

embedding_layer = nn.Embedding(vocab_size, embed_size)
</code></code></pre><p>In this code snippet, we define an embedding layer with a vocabulary size of 10,000 and an embedding size of 128. This means that each token ID will be mapped to a 128-dimensional vector.</p><p>To use the embedding layer, we simply pass the token IDs through it:</p><pre><code><code>token_ids = torch.tensor([1, 2, 3, 4])  # Example token IDs
embeddings = embedding_layer(token_ids)

print(embeddings.shape)  

# Output: torch.Size([4, 128])
</code></code></pre><p>Here, we pass a tensor of token IDs through the embedding layer, and it returns the corresponding embeddings. The resulting <code>embeddings</code> tensor has a shape of <code>(4, 128)</code>, indicating that we have 4 tokens, each represented by a 128-dimensional vector.</p><p>But how does the embedding layer know what values to assign to each token's embedding vector? Initially, the embedding layer is <strong>randomly</strong> initialized. During the training process, the language model learns to adjust these embeddings based on the patterns and relationships in the text data.</p><p>However, the token embeddings each word independently, regardless of its position. This is where positional embeddings come in &#8211; they help the model understand where each word appears in the sequence.</p><h3>Adding Positional Embeddings</h3><p>The self-attention mechanism in transformer models is inherently position-agnostic. When looking at token embeddings alone, the model has no way to know whether "cat" appears at the beginning, middle, or end of the sentence. Positional embeddings solve this by adding position-specific information to each token embedding.</p><p>Think of it this way: if token embeddings tell us "what" the word is, positional embeddings tell us "where" it appears. When we combine them, the model gets both pieces of information simultaneously.</p><h4>Implementing Positional Embeddings</h4><p>Let's implement a complete embedding system that combines both token and positional embeddings. We choose <code>max_sequence_length</code> based on how long our input sequences might be:</p><pre><code><code># Define max_sequence_length as 512
max_sequence_length&nbsp;=&nbsp;512&nbsp;

# Create positional embedding layer
position_embedding&nbsp;=&nbsp;nn.Embedding(max_sequence_length,&nbsp;embed_size)
</code></code></pre><p>Now, generate position indices for our sequence:</p><pre><code><code># If our token_ids has length 4, we need positions [0, 1, 2, 3]&nbsp;
positions&nbsp;=&nbsp;torch.arange(len(token_ids))&nbsp;
print(f"Position indices:&nbsp;{positions}")

# Output: Position indices: tensor([0, 1, 2, 3])</code></code></pre><p>Now get the embeddings for these positions or indices:</p><pre><code><code># Get embeddings from position_embeddings layer
position_embeddings&nbsp;=&nbsp;position_embedding(positions)&nbsp;
print(f"Position embedding shape:&nbsp;{position_embeddings.shape}")

# Output: Position embedding shape: torch.Size([4, 128])</code></code></pre><p>Now combine both token embeddings and positional embeddings:</p><pre><code><code>combined_embeddings = embeddings + position_embeddings
print(f"Combined embedding shape: {combined_embeddings.shape}")

# Output: Combined embedding shape: torch.Size([4, 128])</code></code></pre><h3>Implementing Embeddings (Best Practise)</h3><p>Let's implement a complete embedding system that combines both token and positional embeddings:</p><pre><code><code># Best practices implementation
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_sequence_length):
        super().__init__()
        
        # Initialize embeddings with proper scaling
        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.position_embedding = nn.Embedding(max_sequence_length, embedding_dim)
        
        # Initialize weights using normal distribution
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.position_embedding.weight, std=0.02)
        
    def forward(self, token_ids):
        # Apply scaling to token embeddings
        token_embeddings = self.token_embedding(token_ids) * self.scale
        
        # Create and cache position indices
        if not hasattr(self, '_position_ids'):
            self._position_ids = torch.arange(
                token_ids.size(1), 
                device=token_ids.device
            )
        
        # Add positional embeddings
        return token_embeddings + self.position_embedding(self._position_ids)</code></code></pre><p>Let's break down how this works:</p><ol><li><p><strong>Token Embeddings</strong>: Each word gets transformed into a dense vector through the <code>token_embedding</code> layer, just as we discussed earlier.</p></li><li><p><strong>Position Numbers</strong>: We create a sequence of position indices (0, 1, 2, ...) for each position in our input sequence.</p></li><li><p><strong>Position Embeddings</strong>: These indices get transformed into position-specific vectors through the <code>position_embedding</code> layer.</p></li><li><p><strong>Combination</strong>: We add the token and positional embeddings together. This addition operation preserves both the meaning of the word (from token embeddings) and its position (from positional embeddings).</p></li></ol><h2>Wrapping Up</h2><p>In this comprehensive guide, we've explored the fundamental building blocks of text preprocessing for language modeling. We started by diving into tokenization techniques, learning how to break down raw text into meaningful units while handling challenges like punctuation, casing, and special characters. Next, we discovered the power of Byte Pair Encoding (BPE) for creating subword vocabularies that effectively handle rare and unknown words. We then learned how to construct efficient sampling windows to prepare tokenized text for training, and finally, we built token embeddings from scratch using PyTorch, incorporating positional information to capture word order and context.</p><p>Remember, the techniques and concepts we've discussed are not just theoretical - they have practical applications in a wide range of natural language processing tasks, such as language translation, text summarization, sentiment analysis, and more. By mastering these fundamentals, you'll be equipped to tackle complex language modeling challenges and build impressive AI systems.</p><div><hr></div><p>Thanks for reading NeuraForge: AI Unleashed!</p><p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p><p></p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&utm_medium=email&utm_content=share&action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&utm_medium=email&utm_content=share&action=share"><span>Share</span></a></p><p></p><h1></h1><p></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[PyTorch in Practice: Essential Building Blocks for Modern Deep Learning]]></title><description><![CDATA[From Tensors to Neural Networks: Understanding Core Components]]></description><link>https://neuraforge.substack.com/p/pytorch-in-practice-essential-building</link><guid isPermaLink="false">https://neuraforge.substack.com/p/pytorch-in-practice-essential-building</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 30 Dec 2024 17:39:16 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h1>Introduction</h1><p>As deep learning continues to advance artificial intelligence applications, PyTorch has established itself as a fundamental framework powering everything from computer vision systems to large language models. Originally developed by Meta&#8217;s AI Research lab, PyTorch combines Python's flexibility with deep learning capabilities through a powerful, intuitive interface.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6000" height="4000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4000,&quot;width&quot;:6000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;blue building block lot&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="blue building block lot" title="blue building block lot" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Iker Urteaga</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h3>Core Components of PyTorch</h3><p>PyTorch's architecture rests on three key components that work together to enable efficient deep learning development:</p><ol><li><p><strong>Dynamic Tensor Library</strong></p><ul><li><p>Extends NumPy's array programming capabilities</p></li><li><p>Provides seamless CPU and GPU acceleration</p></li><li><p>Implements efficient mathematical operations for deep learning computations</p></li></ul></li><li><p><strong>Automatic Differentiation Engine (Autograd)</strong></p><ul><li><p>Computes gradients automatically through computational graphs</p></li><li><p>Manages backpropagation for neural network training</p></li></ul></li><li><p><strong>Deep Learning Framework</strong></p><ul><li><p>Delivers modular neural network components</p></li><li><p>Implements optimized loss functions and optimizers</p></li></ul></li></ol><h2>Getting Started with PyTorch</h2><h3>Installation and Setup</h3><p>PyTorch can be installed directly using pip, Python's package installer:</p><pre><code><code>pip install torch</code></code></pre><p>However, for optimal performance, it's recommended to install the version specifically compatible with your system's hardware. Visit&nbsp;<a href="https://pytorch.org/">pytorch.org</a>&nbsp;to get the appropriate installation command based on your:</p><ul><li><p>Operating system</p></li><li><p>Package manager preference (pip/conda)</p></li><li><p>CUDA version (for GPU support)</p></li><li><p>Python version</p></li></ul><h3>GPU Support and Compatibility</h3><p>PyTorch seamlessly integrates with NVIDIA GPUs through CUDA. To verify GPU availability in your environment:</p><pre><code>import torch

# Check GPU availability
gpu_available = torch.cuda.is_available()
print(f"GPU Available: {gpu_available}")

# Get GPU device count if available
if gpu_available:
    print(f"Number of GPUs: {torch.cuda.device_count()}")</code></pre><p>If a GPU is detected, you can move tensors and models to GPU memory using:</p><pre><code># Create a tensor
tensor = torch.tensor([1.0, 2.0, 3.0])

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
tensor = tensor.to(device)</code></pre><h3>Apple Silicon Support</h3><p>For users with Apple M1/M2/M3 chips, PyTorch provides acceleration through the Metal Performance Shaders (MPS) backend. Verify MPS availability:</p><pre><code>import torch

# Check MPS (Metal Performance Shaders) availability
mps_available = torch.backends.mps.is_available()
print(f"MPS Available: {mps_available}")

# If MPS is available, you can use it as device
if mps_available:
    device = torch.device("mps")
    # Move tensors/models to MPS device
    tensor = tensor.to(device)</code></pre><p>For ease of usage, I recommend using <a href="https://colab.research.google.com/">Google Colab</a> i.e. a popular jupyter notebook&#8211;like environment, which provides time-limited access to GPUs.</p><h2>Understanding Tensors</h2><h3>What Are Tensors?</h3><p>Tensors are mathematical objects that generalize vectors and matrices to higher dimensions. In PyTorch, tensors serve as fundamental data containers that hold and process multi-dimensional arrays of numerical values. These containers enable efficient computation and automatic differentiation, making them essential for deep learning operations. PyTorch tensors are similar to Numpy arrays in basic sense. </p><h3>Scalers, Vectors, Matrices and Tensors</h3><div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png&quot;}],&quot;caption&quot;:&quot;Source: https://dev.to/mmithrakumar/scalars-vectors-matrices-and-tensors-with-tensorflow-2-0-1f66&quot;,&quot;alt&quot;:&quot;Tensors&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png&quot;}},&quot;isEditorNode&quot;:true}"></div><p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorch&#8217;s&nbsp;`Tensor`&nbsp;class using the&nbsp;`torch.tensor`&nbsp;function as shown in the following listing.</p><pre><code>import torch

# Scalar (0-dimensional tensor)
scalar = torch.tensor(1)     

# Vector (1-dimensional tensor)
vector = torch.tensor([1, 2, 3])    

# Matrix (2-dimensional tensor)
matrix = torch.tensor([[1, 2], 
                      [3, 4]])     

# 3-dimensional tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]], 
                        [[5, 6], [7, 8]]])</code></pre><p>Each tensor type maintains its specific dimensionality, accessible through the&nbsp;<strong>.shape</strong>&nbsp;attribute:</p><pre><code>print(f"Scalar shape: {scalar.shape}")      # torch.Size([])
print(f"Vector shape: {vector.shape}")      # torch.Size([3])
print(f"Matrix shape: {matrix.shape}")      # torch.Size([2, 2])
print(f"3D tensor shape: {tensor3d.shape}") # torch.Size([2, 2, 2])</code></pre><h3>Tensor Data Types and Precision</h3><p>PyTorch supports various data types with different precision levels, optimized for different computational needs:</p><p>Some of the common torch datatypes available with torch are <code>float32</code>, <code>float64</code>, <code>float16</code>, <code>bfloat16</code>, <code>int8</code>, <code>uint8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>.</p><p>The choice of precision impacts both memory usage and computational efficiency:</p><ul><li><p><code>float32</code>: Standard for most deep learning tasks</p></li><li><p><code>float16</code>: Reduced precision, useful for memory optimization</p></li><li><p><code>bfloat16</code>: Brain Floating Point, balances precision and range</p></li></ul><h3>Floating Data Types</h3><p>PyTorch supports various floating-point precisions for tensors, each serving different computational needs:</p><ul><li><p><code>torch.float32</code> (default): 32-bit precision offering 6-9 decimal places, optimal for most deep learning tasks</p></li><li><p><code>torch.float64</code>: 64-bit double precision with 15-17 decimal places, suitable for high-precision numerical computations</p></li><li><p><code>torch.float16</code>: 16-bit half precision with 3-4 decimal places, useful for memory-efficient operations</p></li><li><p><code>torch.bfloat16</code>: Brain floating point format with 2-3 decimal precision, balancing range and precision</p></li></ul><pre><code><code>import torch

float32_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  
float64_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)  
float16_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16)  
bfloat16_tensor = torch.tensor([1.0, 2.0], dtype=torch.bfloat16)</code></code></pre><h3>Integer Types</h3><p>PyTorch supports various integer data types, each with specific memory allocations and value ranges:</p><ul><li><p><code>int8</code>: 8-bit signed integers (-128 to 127)</p></li><li><p><code>uint8</code>: 8-bit unsigned integers (0 to 255)</p></li><li><p><code>int16</code>: 16-bit signed integers (-32768 to 32767)</p></li><li><p><code>int32</code>: 32-bit signed integers (-2^31 to 2^31-1)</p></li><li><p><code>int64</code>: 64-bit signed integers (-2^63 to 2^63-1), default integer type in PyTorch</p></li></ul><pre><code><code>import torch

int8_tensor = torch.tensor([1, 2], dtype=torch.int8)     
uint8_tensor = torch.tensor([1, 2], dtype=torch.uint8)   
int16_tensor = torch.tensor([1, 2], dtype=torch.int16)   
int32_tensor = torch.tensor([1, 2], dtype=torch.int32)   
int64_tensor = torch.tensor([1, 2], dtype=torch.int64)
</code></code></pre><h3>Datatype Conversion</h3><p>We can convert tensors from one datatype to another using the <code>.to</code> method.</p><pre><code><code># Converting between data types
tensor = torch.tensor([1, 2, 3])
float_tensor = tensor.to(torch.float32) # Convert from int64 to float32
int_tensor = tensor.to(torch.int32)     # Convert from float32 to int32
</code></code></pre><h3>Common Tensor Operations</h3><p>PyTorch provides several fundamental tensor operations essential for deep learning computations. Here are the key operations with their implementations and specific use cases.</p><h4>1. Tensor Creation and Shape Manipulation</h4><p>Creating tensors and understanding their shape are fundamental operations in PyTorch:</p><pre><code><code>import torch

# Create 2D tensor
tensor2d = torch.tensor([[1, 2, 3], 
                        [4, 5, 6]])

# Check tensor shape
shape = tensor2d.shape  
# Returns: torch.Size([2, 3])
</code></code></pre><p>For the above tensor, the shape if 2 x 3 i.e. 2 rows and 3 columns. We can change the shape of the array by maintaining the total size of the array using reshape method.</p><h4>2. Reshaping Operations</h4><p>PyTorch offers two methods for tensor reshaping:</p><pre><code><code># Reshape tensor from (2,3) to (3,2)
reshaped_tensor = tensor2d.reshape(3, 2)

# Alternative using view
viewed_tensor = tensor2d.view(3, 2)
</code></code></pre><p><strong>Technical Note</strong>: <code>.view()</code> and <code>.reshape()</code> differ in memory handling:</p><ul><li><p><code>.view()</code>: Requires contiguous memory layout</p></li><li><p><code>.reshape()</code>: Works with any memory layout, performs copy if necessary</p></li></ul><h4>3. Matrix Operations</h4><p>PyTorch implements efficient matrix operations essential for linear algebra computations:</p><pre><code><code># Transpose operation
transposed = tensor2d.T

# Matrix multiplication methods
result1 = tensor2d.matmul(tensor2d.T)  # Using matmul
result2 = tensor2d @ tensor2d.T        # Using @ operator
</code></code></pre><p>Output shapes for a 2x3 input tensor:</p><ul><li><p>Transpose: 3x2</p></li><li><p>Matrix multiplication with transpose: 2x2</p></li></ul><p>These operations form the foundation for neural network computations and linear algebra operations in deep learning models. For an exhaustive list of tensor operations, refer to the <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>.</p><h2>Automatic Differentiation</h2><h3>Understanding Computational Graphs</h3><p>PyTorch builds computational graphs that track operations performed on tensors. These graphs enable automatic differentiation through the <code>autograd</code> system, making gradient computation efficient and programmatic.</p><p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network&#8212;we will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p><p>Consider the following example of a single layer neural network performing logistic regression with single weight and bias.</p><pre><code><code>import torch
import torch.nn.functional as F

# Initialize inputs and parameters
y = torch.tensor([1.0])           # Target
x1 = torch.tensor([1.1])          # Input
w1 = torch.tensor([2.2], 
                  requires_grad=True)  # Weight
b = torch.tensor([0.0], 
                 requires_grad=True)   # Bias

# Forward pass computation
z = x1 * w1 + b                   # Linear computation
a = torch.sigmoid(z)              # Activation
loss = F.binary_cross_entropy(a, y)    # Loss computation
</code></code></pre><p>We have used the <code>torch.nn.functional</code> module from <code>torch</code> which provides many utility functions like loss functions, activations etc required to write and train deep neural networks.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!6wqh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" width="1156" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:1156,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26016,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p><h3>Gradient Computation with Autograd</h3><p>To train the above model, we have to compute the gradients of loss w.r.t <code>w1</code> and <code>b</code> which will be further used to update the existing weights iteratively. This is where PyTorch makes our life easier by automatically calculating them using the <code>autograd</code> engine.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!D2gj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" width="1120" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68642,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p><p>PyTorch's autograd system automatically computes gradients for all tensors with <code>requires_grad=True</code>. Here's how to compute gradients:</p><pre><code><code>from torch.autograd import grad

# Manual gradient computation
grad_L_w1 = grad(loss, w1, retain_graph=True)
grad_L_b = grad(loss, b, retain_graph=True)

# Alternative using backward()
loss.backward()
print(w1.grad)    # Access gradient for w1
print(b.grad)     # Access gradient for b
</code></code></pre><p><strong>Technical Note</strong>: When using <code>backward()</code>:</p><ul><li><p>Gradients accumulate by default</p></li><li><p>Use <code>zero_grad()</code> before each backward pass in training loops</p></li><li><p><code>retain_graph=True</code> allows multiple backward passes</p></li></ul><p>The <code>grad</code> function is used to get gradients manually and it is useful for debugging and demonstration purposes. Using the <code>backward()</code> function automatically calculates for all the tensors which has <code>requires_grad=True</code> set and gradients will be stored inside <code>.grad</code> property.</p><h2>Building Neural Networks with PyTorch</h2><p>Next, we focus on PyTorch as a library for implementing deep neural networks. While our previous example demonstrated a single neuron for classification, practical applications require complex architectures like transformers and ResNets that process multiple inputs through various hidden layers to produce outputs. Manually calculating and updating individual weights becomes impractical at this scale. PyTorch provides a structured approach through its neural network modules, enabling efficient implementation of sophisticated architectures.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!AV-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" width="860" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:860,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:90385,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p><h3>Introduction to torch.nn.Module</h3><p>The <code>torch.nn.Module</code> serves as PyTorch's foundational class for neural networks, providing a systematic way to define and manage model architectures, parameters, and computations. This base class handles essential functionalities including parameter management, device placement, and training behaviors.</p><p>The subclass has the following components:</p><ul><li><p><code>__init__</code>: We define the layers of neural networks in the constructor of the subclass defined and how the layers interact during forward propagation.</p></li><li><p><code>forward</code>: The forward method describes how the input data passes through the network and comes together as a computation graph.</p></li></ul><h3>Creating Custom Neural Network Architectures</h3><p>Complex neural networks require multiple layers with specific activation functions. Here's a practical implementation of a multi-layer neural network:</p><pre><code><code>class DeepNetwork(nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(num_inputs, 30),     # First hidden layer
            nn.ReLU(),                     # Activation function
            nn.Linear(30, 20),             # Second hidden layer
            nn.ReLU(),                     
            nn.Linear(20, num_outputs)      # Output layer
        )

    def forward(self, x):
        return self.layers(x)
</code></code></pre><p><code>nn.Sequential</code> provides a container for stacking layers in a specific order, streamlining the forward pass implementation.</p><h3>Model Parameters and Initialization</h3><p>PyTorch automatically handles parameter initialization, but you can access and modify parameters:</p><pre><code><code>model = DeepNetwork(50, 3)

# Count trainable parameters
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Trainable parameters: {num_params}")

# Access layer parameters
for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()}")

# Custom initialization
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

model.apply(init_weights)</code></code></pre><p>Each parameter for which <code>requires_grad=True</code> counts as a trainable parameter and will be updated during training. In the above code, this referes to the weights initialized in <code>torch.nn.Linear</code> layers.</p><h3>Forward Propagation Implementation</h3><p>Forward propagation defines how input data flows through the network. Let's initialise random values and pass it through the model.</p><pre><code><code># Sample forward pass
model = DeepNetwork(50, 3)
batch_size = 32
input_features = torch.randn(batch_size, 50)

with torch.no_grad():
    outputs = model(input_features)

print(f"Output shape: {outputs.shape}")</code></code></pre><h3>Training Mode vs. Evaluation Mode</h3><p>PyTorch models have distinct training and evaluation modes that affect certain layers' behavior:</p><pre><code><code>model = DeepNetwork(50, 3)

# Training mode
model.train()
training_output = model(input_features)  # Layers like Dropout and BatchNorm active

print(training_output)

# Evaluation mode
model.eval()
with torch.no_grad():
    eval_output = model(input_features)  # Deterministic behavior
    print(eval_output)</code></code></pre><p>PyTorch models operate in two distinct modes:</p><ol><li><p>Training Mode (<code>model.train()</code>):</p><ul><li><p>Activates Dropout and BatchNorm layers</p></li><li><p>Enables gradient computation and tracking</p></li><li><p>Maintains computational graph for backpropagation</p></li></ul></li><li><p>Evaluation Mode (<code>model.eval()</code> with <code>torch.no_grad()</code>):</p><ul><li><p>Disables Dropout and freezes BatchNorm statistics</p></li><li><p>Prevents gradient computation and tracking</p></li><li><p>Optimizes memory usage by eliminating gradient storage</p></li><li><p>Reduces computational overhead during inference</p></li></ul></li></ol><p>This mode management ensures efficient resource utilization while maintaining appropriate model behavior for both training and inference phases.</p><h2>Efficient Data Handling</h2><p>Efficient data handling is crucial for developing robust deep learning models. PyTorch provides two primary tools for data management: the <code>Dataset</code> and <code>DataLoader</code> classes.</p><h3>1. Dataset and DataLoader Overview</h3><p>PyTorch's data handling framework consists of</p><ul><li><p><code>Dataset</code>: Defines data access and preprocessing</p></li><li><p><code>DataLoader</code>: Handles batch creation, shuffling, and parallel loading</p></li></ul><p>Let's implement a simple classification dataset to demonstrate these concepts:</p><pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

# Training classification data
X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])

# Testing dataset
X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])
</code></code></pre><h3>2. Creating Custom Dataset Objects</h3><p>Next, we create a custom dataset class, <code>SampleDataset</code>, by subclassing from PyTorch&#8217;s <code>Dataset</code> parent class. It has following properties:</p><ul><li><p><code>__init__</code>: Initialize dataset attributes.</p></li><li><p><code>__getitem__</code>: Define data access for individual samples</p></li><li><p><code>__len__</code>: Return total number of samples</p></li></ul><pre><code><code>from torch.utils.data import Dataset

class SampleDataset(Dataset):
    def __init__(self, X, y):
    """Initialize the dataset with features and labels"""
        self.features = X
        self.labels = y

    def __getitem__(self, index):
    """Retrieve a single example and its label"""     
        one_x = self.features[index]     
        one_y = self.labels[index]       
        return one_x, one_y              

    def __len__(self):
    """Get the total number of examples in the dataset"""
        return self.labels.shape[0]     

train_ds = SampleDataset(X_train, y_train)
test_ds = SampleDataset(X_test, y_test)
</code></code></pre><h3>3. Implementing DataLoader</h3><p>DataLoaders handle the heavy lifting of batching, shuffling, and parallel data loading. Now we can create <code>DataLoaders</code> from the <code>SampleDataset</code> object created. This can be done as follows:</p><pre><code><code># Create DataLoader with specific configurations
train_loader = DataLoader(
    dataset=train_ds,     # Dataset Instance
    batch_size=2,         # Number of samples per batch
    shuffle=True,         # Shuffle the training data
    num_workers=0         # Number of parallel workers
    drop_last=True.       # Drop incomplete batch
)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,        # No need to shuffle test data
    num_workers=0
)</code></code></pre><p>Some key parameters of Dataloaders class are as follows:</p><ul><li><p><code>dataset</code>: The Dataset instance to load data from</p></li><li><p><code>batch_size</code>: Number of samples per batch</p></li><li><p><code>shuffle</code>: Whether to shuffle data between epochs</p></li><li><p><code>num_workers</code>: Number of subprocesses for data loading</p></li><li><p><code>drop_last</code>: Whether to drop the last incomplete batch</p></li><li><p><code>pin_memory</code>: Pin memory for faster data transfer to GPU</p></li></ul><h3>Complete Example with Best Practices (Best Practice)</h3><p>Here's a comprehensive implementation incorporating all concepts:</p><pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class SampleDataset(Dataset):
    def __init__(self, X, y, transform=None):
        self.features = X
        self.labels = y
        self.transform = transform # Input transformations if required
    
    def __getitem__(self, index):
        x = self.features[index]
        y = self.labels[index]
        
        if self.transform:
            x = self.transform(x)
            
        return x, y
    
    def __len__(self):
        return len(self.labels)

# Configuration for optimal performance
def create_data_loader(dataset, batch_size, is_training=True):
    return DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=is_training,
        num_workers=4 if is_training else 2,
        pin_memory=torch.cuda.is_available(),
        drop_last=is_training,
        persistent_workers=True
    )

# Usage example
if __name__ == "__main__":
    # Create dataset
    dataset = SampleDataset(X_train, y_train)
    
    # Create data loader
    train_loader = create_data_loader(
        dataset=dataset,
        batch_size=32,
        is_training=True
    )
    
    # Training loop example
    for epoch in range(num_epochs):
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Training operations here
            pass
</code></code></pre><p>This implementation provides a robust foundation for handling data in PyTorch, incorporating best practices for memory management and parallel processing. Adjust the configurations based on your specific use case and available computational resources.</p><h2>Implementing Training Loops in PyTorch</h2><p>A PyTorch training loop consists of several key components:</p><ul><li><p>model initialization,</p></li><li><p>optimizer configuration,</p></li><li><p>loss function definition, and</p></li><li><p>the iterative training process.</p></li></ul><p>Here's a structured implementation showcasing these elements.</p><h3>Basic Training Loop Structure (Best Practice)</h3><pre><code><code>import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    num_epochs: int,
    learning_rate: float,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
) -&gt; dict:
    
    # Initialize optimizer and loss function
    optimizer = Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    # Move model to device
    model = model.to(device)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_accuracy': []
    }
    
    # Training loop
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Move data to device
            features = features.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Accumulate loss
            train_loss += loss.item()
            
            # Optional: Print batch progress
            if batch_idx % 100 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs} | '
                      f'Batch: {batch_idx}/{len(train_loader)} | '
                      f'Loss: {loss.item():.4f}')
        
        # Calculate average training loss
        train_loss = train_loss / len(train_loader)
        history['train_loss'].append(train_loss)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(device)
                labels = labels.to(device)
                
                # Forward pass
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                # Accumulate validation metrics
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        # Calculate validation metrics
        val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * correct / total
        
        # Store validation metrics
        history['val_loss'].append(val_loss)
        history['val_accuracy'].append(val_accuracy)
        
        # Print epoch summary
        print(f'Epoch: {epoch+1}/{num_epochs} | '
              f'Train Loss: {train_loss:.4f} | '
              f'Val Loss: {val_loss:.4f} | '
              f'Val Accuracy: {val_accuracy:.2f}%')
    
    return history

# Example Usage
def main():
    # Assume we have model and data loaders defined
    model = DeepNetwork()
    
    # Training configuration
    config = {
        'num_epochs': 10,
        'learning_rate': 0.001,
    }
    
    # Train model
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=test_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
</code></code></pre><p>The training loop does the following gradient descent as following:</p><ul><li><p>The training process involves passing logits to the <code>cross_entropy</code> loss function, which internally applies <code>softmax</code> for optimized performance and numerical stability.</p></li><li><p>The <code>loss.backward()</code> call computes gradients through PyTorch's computational graph</p></li><li><p>The <code>optimizer.step()</code> step updates the model parameters using these gradients</p></li><li><p>The <code>optimizer.zero_grad()</code> must be called every training iteration to reset gradients, preventing unintended accumulation that could distort the optimization process.</p></li></ul><h2>Model Persistence in PyTorch: Saving and Loading</h2><p>PyTorch provides efficient mechanisms for model persistence through its state dictionary system. The state dictionary (<code>state_dict</code>) maintains a mapping between layer identifiers and their corresponding parameters (weights and biases).</p><h3>Basic Model Persistence</h3><h4>Saving Models</h4><p>After training the model, it is necessary to save the model weights to reuse later for further training or deployment. Save a model's learned parameters using the state dictionary:</p><pre><code><code>import torch

# Save model parameters
torch.save(model.state_dict(), "model_parameters.pth")</code></code></pre><h3>Loading Models</h3><p>The <code>torch.load("model_parameters.pth")</code> function reads the file <code>"model_parameters.pth"</code> and reconstructs the Python dictionary object containing the model&#8217;s parameters while <code>model.load_state_dict()</code> applies these parameters to the model, effectively restoring its learned state from when we saved it.</p><p>We need the instance of the model in memory to apply the saved parameters. Here, the <code>NeuralNetwork(2,</code> <code>2)</code> architecture needs to match the original saved model exactly.</p><pre><code><code># Initialize model architecture
model = NeuralNetwork(num_inputs=2, num_outputs=2)

# Load saved parameters
model.load_state_dict(torch.load("model_parameters.pth"))
</code></code></pre><h2>Comprehensive Model Persistence (Best Practice)</h2><p>For production scenarios, save additional information alongside model parameters:</p><pre><code><code># Save complete model state
checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
    'model_config': {
        'num_inputs': 2,
        'num_outputs': 2
    }
}
torch.save(checkpoint, "model_checkpoint.pth")

# Load complete model state
checkpoint = torch.load("model_checkpoint.pth")
model = NeuralNetwork(**checkpoint['model_config'])
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
</code></code></pre><h2>Conclusion</h2><p>PyTorch's architecture provides a robust foundation for deep learning development through its integrated components: tensor computations, automatic differentiation, and neural network modules. The framework's design enables efficient model implementation through dynamic computation graphs, GPU acceleration, and intuitive APIs for data processing and model construction.</p><p>For continued learning and implementation guidance, refer to PyTorch's official documentation which provides comprehensive updates on best practices, optimizations, and emerging capabilities. This ensures your deep learning applications remain aligned with current framework standards and performance benchmarks.</p><div><hr></div><p>Thanks for reading NeuraForge: AI Unleashed! </p><p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[Value-Based Policy Training in Reinforcement Learning]]></title><description><![CDATA[Insights in a Jiffy #5: How Agents Use State-Value and Action-Value Functions to Optimize Choices]]></description><link>https://neuraforge.substack.com/p/value-based-policy-training-in-reinforcement</link><guid isPermaLink="false">https://neuraforge.substack.com/p/value-based-policy-training-in-reinforcement</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 30 Sep 2024 14:05:33 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Reinforcement Learning (RL) is a branch of artificial intelligence that focuses on creating agents capable of making smart decisions by interacting with their environment through trial and error. This interaction is facilitated by a feedback loop involving states, actions, and rewards. The environment provides a state, the agent takes an action, and the environment responds with a reward and a new state. The goal of RL is to find a policy that maximizes the expected return when the agent acts according to it.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3448" height="4592" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4592,&quot;width&quot;:3448,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;white robot toy holding black tablet&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="white robot toy holding black tablet" title="white robot toy holding black tablet" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Owen Beard</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h3>Policy and Decisions - Recap</h3><p>At the heart of RL is the concept of a policy, which <strong>determines what action to take given the current state</strong>. Policies can be deterministic, always returning the same action for a given state, or stochastic, outputting a probability distribution over possible actions. The ultimate goal is to find the optimal policy (&#960;*) that maximizes the expected return. </p><p>There are two approaches to <strong>finding an optimal policy</strong> for the RL problem at hand:</p><h4>Policy-based Approach</h4><ul><li><p>Policy-based methods directly train the policy to determine which action to take in a given state. </p></li><li><p>This is achieved by optimizing the policy function to maximize the expected rewards. The policy is typically represented by a neural network and is trained without a value function (used to tell how good the agent is at particular state or to take particular action - Used in Value based Approach). </p></li><li><p>The policy is not defined by hand but is learned through training.</p></li></ul><h4><strong>Value-Based Approach</strong></h4><ul><li><p>Value-based methods work indirectly by learning a value function that estimates how good it is to be in a particular state or which action to take in a given state.</p></li><li><p>The value function is trained in value based approach and the policy is defined by hand i.e. there is a fixed policy function. For example,<strong> a greedy policy </strong>always chooses the action/state that leads to the highest value.</p></li><li><p>Based on the information provided by the Value function (i.e. usually which state is more valuable or which action is best to take), the policy will decide the next action/state to move to.</p></li></ul><p>In value-based methods, the link between the value function and policy is crucial. The <strong>policy uses the value function to make decisions</strong>. The trained value function outputs the action-value pair for each state, which is used by the predefined policy function to choose the relevant action.</p><p>For example, the <strong>epsilon-greedy policy</strong> balances exploration and exploitation by choosing the action with the highest value most of the time but occasionally selects a random action.</p><p>Going on we will focus more on the Value based functions and its different variations.</p><h3>Different types of Value based Functions</h3><p>There are two types of value based functions that can be used to to get the expected return or reward for agent to take a decision according to a fixed policy. </p><h4><strong>State-Value Function</strong></h4><p>The state-value function, <em><strong>V(s)</strong></em>, outputs the expected return if the <strong>agent</strong> starts in a <strong>state s</strong> and follows the policy forever afterward. It is defined as:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V(s) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t = s \\right]\n\n&quot;,&quot;id&quot;:&quot;OAVNLLZQGZ&quot;}" data-component-name="LatexBlockToDOM"></div><p><strong>Interpretation</strong>:</p><ul><li><p><strong>Rt</strong>: The immediate reward received at time t.</p></li><li><p><strong>&#947;</strong>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><strong>S_t = s</strong>: The condition that the agent is in state s at time t.</p></li></ul><p>This formula calculates the expected cumulative reward starting from state s and following the policy. It considers the immediate reward and the discounted future rewards.</p><p>Imagine a maze where each state has a value representing how good it is to be in that state. The state-value function assigns these values based on the expected return from starting in that state and following the policy.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ju4u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" width="1400" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c</figcaption></figure></div><h4>Action-Value Function</h4><p>The action-value function, <em><strong>Q(s,a)</strong></em>, returns the expected return if the agent starts in <strong>state s</strong>, takes <strong>action a</strong>, and then follows the policy forever after. It is defined as:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q(s,a) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t = s, A_t = a \\right]&quot;,&quot;id&quot;:&quot;ELWFLVAQWL&quot;}" data-component-name="LatexBlockToDOM"></div><p><br><strong>Interpretation</strong>:</p><ul><li><p><em><strong>Rt</strong></em>: The immediate reward received at time t.</p></li><li><p><em><strong>&#947;</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><em><strong>S_t = s</strong></em>: The condition that the agent is in state <em><strong>s</strong></em> at time t.</p></li><li><p><em><strong>A_t = a</strong></em>: The condition that the agent takes action <em><strong>a</strong></em> at time t.</p></li></ul><p><strong>Visual Representation</strong>:<br>Picture a similar maze but with each state-action pair having a value. The action-value function evaluates how good it is to take a specific action in a given state, considering the expected return from that action and the subsequent policy.</p><h3>The need for Bellman Equation</h3><p>In the different types of Value Functions mentioned above, we can see that the expected return to make a decision/take an action depends on agent starting in a state <em><strong>s</strong></em> and following the policy forever or until the episode ends and then summing the rewards. This is a computationally expensive process if it has to be repeated at every state. </p><p>The Bellman equation provides a solution by breaking down the value function into smaller, manageable parts. It simplifies the calculation by considering the <strong>immediate reward</strong> and the <strong>discounted value of the next state</strong>:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V(s) = \\max_a \\left( R(s,a) + \\gamma V(s') \\right)\n&quot;,&quot;id&quot;:&quot;HIRLROAHVO&quot;}" data-component-name="LatexBlockToDOM"></div><p><strong>Interpretation</strong>:</p><ul><li><p><em><strong>V(s)</strong></em>: The value of being in state <em><strong>s</strong></em>.</p></li><li><p><em><strong>R(s,a)</strong></em>: The immediate reward received when taking action <em><strong>a</strong></em> in state <em><strong>s</strong></em>.</p></li><li><p><em><strong>&#947;</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><em><strong>V(s')</strong></em>: The value of the next state s'.</p></li></ul><p>This recursive equation allows for efficient computation of state values without needing to calculate the expected return for each state from scratch. The Bellman equation is crucial for making value-based methods computationally feasible.</p><h2><strong>Conclusion</strong></h2><p>Understanding these concepts is essential for grasping the fundamentals of reinforcement learning. The Bellman equation plays a pivotal role in making value-based methods efficient, and visualizing state-value and action-value functions helps in understanding how agents make decisions based on these values. </p><div><hr></div><p>This article is part of our ongoing series on Reinforcement Learning and represents Issue #5 in the "Insights in a Jiffy" collection. We encourage you to read our previous issues in this series for a more comprehensive understanding <a href="https://neuraforge.substack.com/t/reinforcement-learning">here</a>. Each article builds upon the concepts introduced in earlier posts, providing a holistic view of the Reinforcement Learning landscape.</p><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a class="button primary button-wrapper" href="https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM"><span>Share</span></a></p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[Understanding Reinforcement Learning: Policy-Based and Value-Based Approaches]]></title><description><![CDATA[Insights in a Jiffy #4: How Agents Learn to Make Optimal Choices]]></description><link>https://neuraforge.substack.com/p/understanding-reinforcement-learning</link><guid isPermaLink="false">https://neuraforge.substack.com/p/understanding-reinforcement-learning</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 02 Sep 2024 00:30:13 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Reinforcement Learning (RL) is a fascinating field of artificial intelligence that focuses on how agents learn to make decisions in complex environments.  In this blog, we'll explore the two main approaches for solving RL problems: <strong>policy-based and value-based methods</strong>. Let's dive in and uncover how these strategies enable machines to learn and make optimal choices.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3024" height="3780" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3780,&quot;width&quot;:3024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;man standing in the middle of woods&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="man standing in the middle of woods" title="man standing in the middle of woods" srcset="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Vladislav Babienko</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>The Policy: The Agent's Brain</h2><p>At the heart of RL is the concept of a policy, which can be thought of as the agent's brain. Here's what you need to know:</p><ul><li><p>A policy (often denoted as &#960;) is a function that determines what action to take given the current state.</p></li><li><p>It defines the agent's behaviour at any given time.</p></li><li><p>The ultimate goal in RL is to find the optimal policy (&#960;*) that maximizes the expected return when the agent acts according to it.</p></li></ul><p>Imagine a robot learning to navigate a maze. The policy would be the rules the robot follows to decide which direction to move based on its current position and what it can "see" around it.</p><h2>Two Approaches to Train the RL Agent</h2><p>There are two main ways to train an RL agent:</p><ol><li><p>Directly: Policy-based methods</p></li><li><p>Indirectly: Value-based methods</p></li></ol><p>Let's explore each of these approaches.</p><h3>Policy-Based Methods</h3><p>Policy-based methods directly teach the agent which action to take in a given state. They work by optimizing the policy function to maximize the expected rewards.</p><p>There are two types of policies in policy-based methods:</p><ol><li><p><strong>Deterministic Policies</strong>: For a given state, these always return the same action. Therefore, <br><em><strong>action = policy(state)</strong></em><br>Example: If our maze-solving robot is at a T-junction, a deterministic policy might always choose to turn right.</p></li><li><p><strong>Stochastic Policies</strong>: These output a probability distribution over possible actions. Therefore, <br><em><strong>policy(actions | state) = probability distribution over the set of actions given the current state</strong></em><br>Example: At the same T-junction, a stochastic policy might assign a 70% chance to turn right and a 30% chance to turn left.</p></li></ol><h4>Example of a Policy-Based Method: REINFORCE Algorithm</h4><p>The REINFORCE algorithm is a classic policy-based method. Here's how it might work for our maze-solving robot:</p><ol><li><p>The robot starts with a random policy.</p></li><li><p>It attempts to solve the maze multiple times, keeping track of the actions it took and the rewards it received.</p></li><li><p>After each attempt, it adjusts its policy:</p><ul><li><p>Actions that led to solving the maze quickly are made more likely.</p></li><li><p>Actions that lead to dead ends or longer paths are made less likely.</p></li></ul></li><li><p>Over time, the robot learns a policy that consistently solves the maze efficiently.</p></li></ol><h3>Value-Based Methods</h3><p>Value-based methods work indirectly by teaching the agent to estimate how good it is to be in a particular state, or how good it is to take a specific action in a given state.</p><p>Key concepts in value-based methods:</p><ul><li><p>State Value (V): The expected total reward if the agent starts in a specific state and follows the current policy.</p></li><li><p>Action Value (Q): The expected total reward if the agent takes a specific action in a given state and then follows the current policy.</p></li></ul><p>In value-based methods, the agent chooses actions that lead to states with higher values.</p><h4>Example of a Value-Based Method: Q-Learning</h4><p>Q-learning is a popular value-based method that learns the quality of actions in states, represented by Q-values. Here's a simplified explanation of how it works:</p><ol><li><p>Q-values represent the expected cumulative reward of taking a particular action in a given state and then following the optimal policy thereafter.</p></li><li><p>The Q-function maps state-action pairs to these Q-values.</p></li></ol><p>For our maze-solving robot:</p><ol><li><p>The robot starts with no knowledge of the maze, so all Q-values are initialized to zero.</p></li><li><p>As the robot explores the maze, it updates its estimates of the Q-values for each state-action pair.</p></li><li><p>The robot chooses actions based on these Q-values, balancing between exploiting known good actions and exploring new ones.</p></li><li><p>Over time, the Q-values converge, and the robot learns to choose actions that lead it efficiently through the maze.</p></li></ol><h2>Deep Reinforcement Learning</h2><p>Deep Reinforcement Learning (DRL) combines RL with deep neural networks to handle high-dimensional state spaces and complex environments. In DRL, neural networks are used to approximate either the policy (in policy-based methods) or the value function (in value-based methods).</p><p>For our maze-solving robot, imagine if instead of a simple grid-based maze, it had to navigate a complex 3D environment using camera inputs. This would create a high-dimensional state space that traditional RL methods struggle with. DRL can handle this by:</p><ol><li><p>Using convolutional neural networks to process visual input and extract relevant features.</p></li><li><p>Employing deep neural networks to approximate the Q-function (in Deep Q-Networks) or the policy function (in policy gradient methods).</p></li></ol><p>A popular DRL algorithm is the Deep Q-Network (DQN), which extends Q-learning by using a deep neural network to approximate the Q-function, taking raw pixels as input and outputting Q-values for each possible action.</p><h2>Conclusion</h2><p>In this "Insights in a Jiffy," we've introduced the core concepts of reinforcement learning &#8211; policies and values &#8211; and how they form the foundation of how agents learn to make optimal choices. Both policy-based and value-based methods have their strengths, and the advent of deep reinforcement learning has further expanded their capabilities.</p><p>However, we've only scratched the surface. Each of these approaches deserves a deeper dive to truly understand their intricacies and applications. Stay tuned for future issues where we'll dedicate entire articles to explore policy-based methods, value-based methods, and deep reinforcement learning in much greater detail.</p><p>The journey into the world of autonomous decision-making is just beginning, and there's much more to discover in the exciting field of reinforcement learning.</p><div><hr></div><p>This article is part of our ongoing series on Reinforcement Learning and represents Issue #4 in the "Insights in a Jiffy" collection. We encourage you to read our previous issues in this series for a more comprehensive understanding <a href="https://neuraforge.substack.com/t/reinforcement-learning">here</a>. Each article builds upon the concepts introduced in earlier posts, providing a holistic view of the Reinforcement Learning landscape.</p><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&utm_medium=email&utm_content=share&action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&utm_medium=email&utm_content=share&action=share"><span>Share</span></a></p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[Fast and Efficient finetuning of LLMs: QLoRA]]></title><description><![CDATA[#12 Advanced Generative AI: Efficient Finetuning of Quantised LLMs]]></description><link>https://neuraforge.substack.com/p/fast-and-efficient-finetuning-of</link><guid isPermaLink="false">https://neuraforge.substack.com/p/fast-and-efficient-finetuning-of</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 19 Aug 2024 00:30:16 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities across a wide range of tasks. However, fine-tuning these massive models for specific applications presents significant challenges, particularly in terms of computational resources and memory requirements. Enter <strong>QLoRA</strong> (Quantised Low-Rank Adaptation), an innovative technique that combines the benefits of <strong>quantization</strong> and <strong>low-rank adaptation</strong> to enable cheap, fast and efficient fine-tuning of LLMs when hardware resources are limited. </p><p>In this blog post, we'll explore QLoRA's quantisation and low-rank adaptation, implementation, and impact on training LLMs.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="2617" height="1608" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1608,&quot;width&quot;:2617,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;time lapse photography of three men cycling&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="time lapse photography of three men cycling" title="time lapse photography of three men cycling" srcset="https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1516147697747-02adcafd3fda?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxzcGVlZHxlbnwwfHx8fDE3MjM5MjM4Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">paolo candelo</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Background - Quantisation and LoRA</h2><p>Before we delve into QLoRA, let's establish a foundational understanding of its key components, i.e. Quantisation and Low-Rank adaptation (LoRA)</p><h3>Quantization </h3><p>Quantization is a technique for reducing the precision of a model's parameters and activations. By representing these values with fewer bits, quantization significantly reduces memory usage and computational requirements, often with minimal impact on model performance. </p><h4>Precision-based Quantisation</h4><p>Here are different precision quantisation <strong>representation</strong> data that are currently used for training models:</p><ol><li><p><strong>FP32 (32-bit floating-point)</strong>: </p><ul><li><p>Standard precision is used in most deep-learning training.</p></li><li><p>Provides a wide dynamic range (1.18 &#215; 10^-38 to 3.4 &#215; 10^38) and high precision. It has 32 bits, 8 exponent bits and 23 fraction bits.</p></li><li><p>The memory required to store one value is 4 bytes.</p></li><li><p>Serves as the baseline for comparing other quantization methods.</p></li></ul></li><li><p><strong>FP16 (16-bit floating-point)</strong>: </p><ul><li><p>Reduces memory usage by half compared to FP32. </p></li><li><p>Dynamic range of 6.10 &#215; 10^-5 to 6.55 &#215; 10^4. It has 16 bits, 5 exponent bits and 10 fraction bits.</p></li><li><p>The memory required to store one value is 2 bytes.</p></li><li><p>Commonly used in mixed-precision training to balance accuracy and efficiency.</p></li></ul></li><li><p><strong>BF16 (Brain Floating Point)</strong>:</p><ul><li><p>Uses 16 bits like FP16 but with a different distribution: 16 bits, 8 exponent bits, and 7 fraction bits.</p></li><li><p><strong>BF16</strong> is a compromise between FP32 and FP16. It is designed to maintain much of FP32's dynamic range while offering the memory and computational benefits of a 16-bit format.</p></li><li><p>Offers a larger dynamic range than FP16 (1.18 &#215; 10^-38 to 3.4 &#215; 10^38), making it more suitable for training.</p></li><li><p>The memory required to store one value is 2 bytes.</p></li><li><p>Increasingly popular in modern AI hardware due to its balance of range and precision.</p></li></ul></li><li><p><strong>INT8 (8-bit integer)</strong>:</p><ul><li><p>Represents values using 8 bits, typically in the range -128 to 127 or 0 to 255. It has 8 bits, 8 exponent bits and 7 fraction bits.</p></li><li><p>Dramatically reduces memory usage and increases inference speed.</p></li><li><p>Requires careful calibration to maintain accuracy, often using techniques like quantization-aware training or post-training quantization.</p></li></ul></li><li><p><strong>INT4 (4-bit integer)</strong>: </p><ul><li><p>Pushes the boundaries of low-precision representation, using only 4 bits per value.</p></li><li><p>Requires advanced techniques to maintain model quality.</p></li><li><p>The focus of recent research, including QLoRA, for ultra-efficient model compression.</p></li></ul></li></ol><h4>Quantization Schemes</h4><p>A quantization scheme is a method for mapping a large set of input values to a smaller set of output values. It is typically used to reduce the precision of data representation. For example, a quantization scheme will be used to convert and represent data from FP32 format to BF16 or INT8 format.</p><p>Some important quantization factors to be considered are:</p><ul><li><p><strong>Scaling Factor</strong>: </p><ul><li><p>The value is used to convert between the original floating-point values and the quantised integer values.</p></li><li><p>It helps maintain the relative relationships between values while mapping them to a smaller integer range.</p></li><li><p>Formula: <strong>scale = (float_max - float_min) / (int_max - int_min)</strong></p></li><li><p>Usage: <strong>quantized_value = round(original_value / scale)</strong></p></li></ul></li><li><p><strong>Zero-point</strong>:</p><ul><li><p>The zero-point is the integer value that represents the real-value zero in the quantized space.</p></li><li><p>It allows the representation of both positive and negative values using only unsigned integers.</p></li><li><p>Formula: <strong>zero_point = round(-float_min / scale)</strong></p></li><li><p>Usage: <strong>quantized_value = round(original_value / scale) + zero_point</strong></p></li></ul></li></ul><p>Based on the above parameters, some important quantization schemes include the following:</p><ol><li><p><strong>Linear Quantization</strong>: </p><ul><li><p>Maps floating-point values to integers using a linear scaling factor and zero-point.</p></li><li><p>Quantization formula: <strong>q = round(x / scale) + zero_point</strong></p></li><li><p>Dequantization formula: <strong>x = (q - zero_point) * scale</strong></p></li><li><p>Simple to implement but may not capture the distribution of weights effectively.</p></li></ul></li></ol><ol start="2"><li><p><strong>Non-linear Quantization</strong>: </p><ul><li><p>It uses non-linear mapping between floating-point and quantized values.</p></li><li><p>Can better represent the typical distribution of weights in neural networks, which often follow a normal or log-normal distribution.</p></li><li><p>Examples include <strong>logarithmic quantization</strong> and the <strong>NormalFloat</strong> scheme used in QLoRA.</p></li></ul></li><li><p><strong>Symmetric vs Asymmetric Quantization</strong>: </p><ul><li><p>Symmetric: Uses a zero-point at 0, simplifying computations. Formula: <br><strong>q = round(x / scale)</strong></p></li><li><p>Asymmetric: Allows for a non-zero offset, potentially capturing the weight distribution better. Uses the full formula: <br><strong>q = round(x / scale) + zero_point</strong></p></li></ul></li></ol><p> Now, let&#8217;s understand the Low-Rank Adaptation in detail.</p><h3>Low-Rank Adaptation (LoRA) in Depth</h3><p>LoRA, introduced by Hu et al. (2021), is a parameter-efficient fine-tuning (PEFT) method that&nbsp;<strong>freezes the pre-trained model</strong>&nbsp;weights and <strong>injects trainable low-rank matrices </strong>into each layer of the transformer architecture.</p><p>The fundamental idea behind LoRA is to represent the weight updates during fine-tuning as the product of two low-rank matrices rather than updating the entire model. This approach significantly reduces the number of trainable parameters while allowing for effective model adaptation to new tasks.</p><h4>Mathematical Formulation of LoRA</h4><p>Pre-trained LLMs have a low intrinsic dimension and can still learn effectively despite being randomly projected to a smaller space. </p><p>Let <code>W&#8320; &#8712; &#8477;&#7496;&#739;&#7496;</code> be the pre-trained weights of a layer in the original model. During fine-tuning with LoRA, instead of directly updating W&#8320;, we introduce a low-rank update:</p><p><code>W = W&#8320; + BA</code></p><p>Where:</p><ul><li><p><code>B &#8712; &#8477;&#7496;&#739;&#691;</code> is a matrix of dimension <code>d &#215; r</code></p></li><li><p><code>A &#8712; &#8477;&#691;&#739;&#7496;</code> is a matrix of dimension <code>r &#215; d</code></p></li><li><p><em>r</em> is the rank of the update (typically much smaller than d and k)</p></li></ul><p>The product <code>BA</code> represents the weight update and <strong>only A and B are trained during fine-tuning</strong>. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!RXtj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!RXtj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 424w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 848w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1272w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png" width="1247" height="528" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:528,&quot;width&quot;:1247,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:69174,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!RXtj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 424w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 848w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1272w, https://substackcdn.com/image/fetch/$s_!RXtj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2139aecb-362d-4b27-9882-69f44fb6db39_1247x528.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p>The above weight matrix weight decomposition is applied to the self-attention  modules of the transformer. Therefore, only these modules will be trained during training while the remaining pre-trained model weights are frozen. </p><h4>Key Components of LoRA</h4><ol><li><p><strong>Rank (r)</strong>:</p><ul><li><p>Determines the expressiveness of the update.</p></li><li><p>A lower rank means fewer parameters but potentially less adaptability.</p></li><li><p>Typically, it ranges from 1 to 64, with 8 or 16 being common choices.</p></li><li><p>The number of trainable parameters introduced by LoRA is </p><p><code>2 * r * (d + d) </code>for each adapted layer.</p></li></ul></li><li><p><strong>Scaling Factor (&#945;)</strong>: </p><ul><li><p>Used to adjust the magnitude of the LoRA update.</p></li><li><p>The actual update is scaled: <code>&#945;(BA) / r</code></p></li><li><p>It helps in balancing the contribution of the pre-trained weights and the LoRA update.</p></li><li><p>It can be thought of as an additional hyperparameter controlling the learning rate of LoRA parameters.</p></li></ul></li><li><p><strong>Target Modules</strong>: </p><ul><li><p>Specifies which layers or sub-modules of the model to apply LoRA to.</p></li><li><p>Common choices include <strong>attention layers (query and value projections)</strong> in transformer models.</p></li><li><p>Selecting appropriate target modules can significantly impact the efficiency-performance trade-off.</p></li></ul></li></ol><h4>LoRA in Practise </h4><p>During fine-tuning, LoRA is implemented as follows:</p><ol><li><p><strong>Initialization</strong>: </p><ul><li><p>LoRA matrices (A and B) are typically initialized randomly using a normal distribution with a small variance.</p></li><li><p>The scaling factor &#945; is set to a small value (e.g., 1) at the start of training.</p></li></ul></li><li><p><strong>Training Process</strong>: </p><ul><li><p>During the forward pass, the LoRA update is added to the output of the target modules (as shown in the figure above)<br><code>y = W&#8320;x + BAx</code></p></li><li><p>Gradients are computed with respect to A and B using standard backpropagation, leaving W&#8320; unchanged.</p></li><li><p>The effective learning rate for LoRA parameters is scaled by <code>&#945;/r</code>.</p></li></ul></li><li><p><strong>Inference</strong>: </p><ul><li><p>For efficient inference, the LoRA updates can be merged with the original weights: W = W&#8320; + BA</p></li><li><p>This allows for using the adapted model without additional computational overhead during inference.</p></li></ul></li></ol><p>Now, let&#8217;s understand the big picture of how QLoRa is implemented as a combination of LoRA and Quantization. </p><h3>QLoRA: Combining Quantisation and LoRA</h3><p>QLoRA, introduced by Dettmers et al. (2023), integrates advanced quantization techniques with LoRA to create a highly efficient fine-tuning method for large language models.</p><p>Before we delve into the specifics of the QLoRa workflow, we should note some key innovations introduced in the QLoRA paper. </p><h4>Key Innovations</h4><ol><li><p><strong>4-bit NormalFloat Quantization</strong></p></li></ol><p>Normal float is a novel quantization data type optimized for training neural networks while maintaining performance levels.</p><ul><li><p>Assumes a normal distribution of weights, which is common in trained neural networks.</p></li><li><p>Uses a <strong>non-linear quantization scheme</strong> to provide better precision for values near zero.</p></li><li><p>Outperforms other 4-bit quantization methods for large language models.</p></li></ul><p>The NormalFloat quantization process involves:</p><ol><li><p>Estimating the parameters (&#956;, &#963;) of the normal distribution that best fits the weight tensor.</p></li><li><p>Defining non-linear quantization boundaries based on the cumulative distribution function (CDF) of the normal distribution.</p></li><li><p>Mapping weights to 4-bit integers based on these boundaries.</p></li></ol><p>4-bit Normal Float is the datatype used to quantise and store base model weights during QLoRA training. </p><ol start="2"><li><p><strong>Double Quantization</strong></p></li></ol><p>Double quantization is a technique that further reduces memory usage:</p><ol><li><p>First, it quantizes model weights to 4-bit precision using NormalFloat.</p></li><li><p>Then, it quantizes the resulting quantization constants (scaling factors and zero-points) to 8-bit precision.</p></li></ol><p>This two-step process significantly reduces the memory footprint of quantization constants, which can be substantial in large models.</p><ol start="3"><li><p><strong>Paged Optimizers</strong></p></li></ol><p>Paged optimizers efficiently manage memory by:</p><ul><li><p>Utilizing CPU memory as a backup when GPU memory is exhausted.</p></li><li><p>Implementing a sophisticated paging system to swap data between GPU and CPU memory.</p></li><li><p>Minimizing the performance impact of using CPU memory through careful optimization and prefetching strategies.</p></li></ul><h4>Integration of Quantization and LoRA</h4><p>QLoRA combines quantization and LoRA in a synergistic manner:</p><ol><li><p><strong>Quantized Base Model</strong>:</p><ul><li><p>The <strong>pre-trained model weights (W&#8320;)</strong> are quantized to 4-bit precision using <strong>NormalFloat</strong>.</p></li><li><p>This drastically reduces the memory footprint of the base model.</p></li></ul></li><li><p><strong>Full-precision LoRA Updates</strong>:</p><ul><li><p>LoRA matrices (A and B) are kept at higher precision (typically 16-bit, i.e. BF16) during training.</p></li><li><p>This allows for accurate gradient computation and weight updates.</p></li></ul></li><li><p><strong>Quantization-aware Training</strong>:</p><ul><li><p>During the forward pass, quantized weights are dequantized, the LoRA update is applied, and the result is requantized.</p></li><li><p>This process ensures that the model learns to perform well within the constraints of quantization.</p></li></ul></li><li><p><strong>Memory-efficient Optimization</strong>:</p><ul><li><p>Paged optimizers manage the memory of both the quantized base model and the full-precision LoRA parameters.</p></li><li><p>Gradient accumulation is used to simulate larger batch sizes without increasing memory requirements.</p></li></ul></li></ol><h4>End-to-End QLoRA Workflow</h4><ol><li><p>Load the pre-trained model and quantize it to 4-bit precision using NormalFloat.</p></li><li><p>Add LoRA adapters to the quantized model, initializing them in 16-bit precision (BF16)</p></li><li><p>Use paged optimizers and gradient accumulation for memory-efficient training.</p></li><li><p>During training, perform quantization-aware forward and backward passes.</p></li><li><p>Update only the LoRA parameters, keeping the quantized base model fixed.</p></li><li><p>For inference, merge the LoRA updates with the quantized base model or keep them separate for task-specific adaptation.</p></li></ol><p>This combination of techniques allows <strong>QLoRA to fine-tune models with billions of parameters on consumer-grade hardware, democratizing access to state-of-the-art language models.</strong></p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!58n6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!58n6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 424w, https://substackcdn.com/image/fetch/$s_!58n6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 848w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1272w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif" width="600" height="439" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:439,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:322447,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!58n6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 424w, https://substackcdn.com/image/fetch/$s_!58n6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 848w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1272w, https://substackcdn.com/image/fetch/$s_!58n6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93b1f9f1-8654-4de0-a61e-3f56d22951cb_600x439.gif 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p>Now that we have understood the theory of how QLoRA works in detail let&#8217;s implement it in code and fine-tune a large language model.</p><h2>Implementation of QLoRA </h2><ol><li><p>First, Install the necessary libraries:</p></li></ol><p>Let's walk through a practical implementation of QLoRA using the Hugging Face Transformers and PEFT libraries.</p><pre><code>pip install transformers peft bitsandbytes accelerate</code></pre><ol start="2"><li><p>Load and Quantize the model.</p></li></ol><p>Here, we are loading a 6.7 billion-parameter model in a <strong>4-bit NormalFloat </strong>datatype with <strong>double quantization</strong> enabled. The LoRA parameters are loaded in the <strong>BF16</strong> datatype.</p><pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "facebook/opt-6.7b"  # Example large model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)</code></pre><ol start="3"><li><p>Configure LoRA</p></li></ol><p>Convert the pre-trained model into the LoRA model by adding the LoRA adapters and specifying parameters like rank (r), alpha, and target modules. </p><pre><code>from peft import LoraConfig, get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)</code></pre><ol start="4"><li><p>Load and Prepare a Dataset</p></li></ol><p>For this example, let&#8217;s use the IMDb dataset from the datasets library to fine-tune our LLM.</p><pre><code>from datasets import load_dataset

dataset = load_dataset("imdb")  # Example dataset

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])</code></pre><ol start="5"><li><p>Configure Training Arguments</p></li></ol><p>Let&#8217;s configure the training arguments before we start training the model using the <strong>transformers</strong> library. Note the optimizer is set as&nbsp;<strong>paged_adamw_8bit&nbsp;</strong>to efficiently manage memory with the CPU as a backup.</p><pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    save_total_limit=3,
    logging_steps=100,
    optim="paged_adamw_8bit"
)</code></pre><ol start="6"><li><p>Train the model</p></li></ol><pre><code>from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

trainer.train()</code></pre><p>Post training the model is saved for further inference.</p><h2>Conclusion </h2><p>QLoRA (Quantized Low-Rank Adaptation) represents a significant leap forward in model fine-tuning. By combining quantization techniques with low-rank adaptation, QLoRA dramatically reduces the memory footprint required for training while maintaining model quality. This breakthrough allows for faster, more efficient fine-tuning of large language models on consumer-grade hardware, opening up new possibilities for customization and specialization of AI models.</p><p>The advent of QLoRA, alongside other innovative training methods like LoRA, PEFT, and instruction fine-tuning, is democratizing access to powerful language models. These techniques are making it possible for researchers, developers, and organizations of all sizes to work with and adapt state-of-the-art LLMs for specific applications. As these methods continue to evolve, we're moving closer to a future where advanced AI capabilities are not limited to tech giants but are accessible to a global community of innovators.</p><h3>References</h3><ol><li><p>Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314.</p></li><li><p>Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</p></li><li><p>https://huggingface.co/blog/4bit-transformers-bitsandbytes</p></li></ol><div><hr></div><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community. Thank you!</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[From Decisions to Rewards: Understanding the RL Decision Making Process]]></title><description><![CDATA[Insights in a Jiffy #3: How Rewards Drive Smart Decisions in Learning Agents]]></description><link>https://neuraforge.substack.com/p/from-decisions-to-rewards-understanding</link><guid isPermaLink="false">https://neuraforge.substack.com/p/from-decisions-to-rewards-understanding</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Wed, 14 Aug 2024 23:30:48 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Imagine you're new to a city and trying to find the best restaurant for dinner. Each time you eat out, you rate your experience. Sometimes, you return to restaurants you enjoy (exploitation), while other times, you try new places (exploration). Over time, you learn which restaurants consistently provide the best meals, helping you make better dining choices.</p><p>This everyday scenario mirrors the core principles of Reinforcement Learning (RL). In this blog post, we'll explore three fundamental concepts in RL: the mechanics of rewards, the mathematics of discounting, and the strategic balance between exploration and exploitation.</p><p>Let's delve into these concepts, examining their technical foundations and practical implications using our restaurant-finding scenario as a running example.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3024" height="4032" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4032,&quot;width&quot;:3024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;group of people inside the restaurant&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="group of people inside the restaurant" title="group of people inside the restaurant" srcset="https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1508424757105-b6d5ad9329d0?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxyZXN0YXVyYW50fGVufDB8fHx8MTcyMzY2NDE2MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">kayleigh harrington</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>The Mechanics of Rewards in Reinforcement Learning</h2><h3>Defining Rewards</h3><p><strong>Rewards</strong> in Reinforcement Learning (RL) are crucial because they are the only feedback an agent receives to determine whether an action is good or bad. Rewards are numerical feedback signals that the environment provides to the agent after each action. In our restaurant scenario, the reward could be your satisfaction rating after each meal, perhaps on a scale from 1 to 10.</p><p>The cumulative reward at each time step <em>t</em> represents the total rewards an agent has accumulated up to that point. In our example, this would be the sum of all your restaurant ratings over time.</p><p>Formally, the cumulative reward R(&#964;) at time step <em>t</em> can be expressed as:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;R(\\tau) = r_{t} + r_{t+1} + r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} r_{t+k}\n&quot;,&quot;id&quot;:&quot;FAVYNMJJQN&quot;}" data-component-name="LatexBlockToDOM"></div><p>Where R(&#964;) is the cumulative reward.</p><p>However, this approach assumes that all rewards, regardless of when they are received, are equally valuable, which isn't typically the case in real-world scenarios. In reality, rewards that occur sooner are often more valuable and predictable than those received in the distant future. This is where <strong>reward discounting</strong> comes into play, allowing the agent to weigh short-term and long-term rewards differently.</p><h3>Reward Discounting </h3><p>To account for the varying importance of rewards over time, we introduce a <strong>discount factor</strong> <em>&#947;</em>, where <em>&#947;</em> is a value between 0 and 1 (usually between 0.95 and 0.99). The discount factor helps prioritize short-term versus long-term rewards:</p><ul><li><p>A larger <em>&#947;</em> (closer to 1) implies a smaller discount, meaning the agent values future rewards more and is more focused on long-term success.</p></li><li><p>A smaller <em>&#947;</em> (closer to 0) implies a larger discount, meaning the agent values immediate rewards more and focuses on short-term gains.</p></li></ul><p>The formula for the <strong>discounted cumulative reward</strong>&#8203; at time step <em>t</em> is given by:</p><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;G_t = \\gamma^0 r_{t} + \\gamma^1 r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots\n&quot;,&quot;id&quot;:&quot;QBZJFCUFNV&quot;}" data-component-name="LatexBlockToDOM"></div><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;G_t = r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n&quot;,&quot;id&quot;:&quot;FGNJDJMZWU&quot;}" data-component-name="LatexBlockToDOM"></div><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!VTZr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VTZr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 424w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 848w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1272w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png" width="1404" height="672" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:672,&quot;width&quot;:1404,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108790,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VTZr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 424w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 848w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1272w, https://substackcdn.com/image/fetch/$s_!VTZr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3283d5ac-80f3-4087-a30b-7b80d87f6dda_1404x672.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p>In our restaurant example, a high <em>&#947;</em> might represent your willingness to try a new restaurant with great reviews, even if it's farther away or more expensive. A low &#947; would mean you're more likely to stick with nearby, familiar options.</p><h2>Exploration and Exploitation Tradeoff</h2><p>In RL, agents face the challenge of balancing <strong>exploration</strong> and <strong>exploitation</strong>:</p><ul><li><p><strong>Exploration</strong> involves trying out new actions to discover more information about the environment. This helps the agent learn about potentially better actions that it hasn't encountered before.</p></li><li><p><strong>Exploitation</strong> involves using the knowledge the agent has already acquired to maximize its reward by choosing the best-known actions.</p></li></ul><p>Given that RL operates under the <strong>reward hypothesis</strong>&#8212;the idea that the goal is to maximize cumulative reward&#8212;agents might be tempted to exploit known actions repeatedly. However, this can trap the agent in suboptimal behaviour, preventing it from exploring potentially better actions.</p><p>To avoid this, a balance between exploration and exploitation is necessary. The agent must explore enough to discover better strategies while exploiting its current knowledge to achieve high rewards. Striking this balance is key to successful learning in RL.</p><p> In our scenario:</p><ul><li><p>Exploration is trying new restaurants you've never visited before.</p></li><li><p>Exploitation is returning to restaurants you know you like.</p></li></ul><p>If you only ate at your favourite restaurant (pure exploitation), you might miss out on discovering even better places. Conversely, you might have too many mediocre meals if you always try new restaurants (pure exploration).</p><h2>Conclusion</h2><p>In conclusion, rewards and discounting play a crucial role in helping reinforcement learning agents evaluate actions over time. However, the agent also relies on a <strong>policy</strong>, which serves as its decision-making framework. The policy dictates how the agent chooses actions based on the current state, ultimately guiding it toward maximizing long-term rewards. Next, we&#8217;ll explore how policies are formed and optimized in RL.</p><div><hr></div><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[Reinforcement Learning Essentials: A Quick Guide]]></title><description><![CDATA[Insights in a Jiffy #2: Understanding the fundamentals - Rewards, Observations, Actions and Tasks]]></description><link>https://neuraforge.substack.com/p/reinforcement-learning-essentials</link><guid isPermaLink="false">https://neuraforge.substack.com/p/reinforcement-learning-essentials</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 12 Aug 2024 00:30:58 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Imagine you're teaching a robot to play chess. The robot doesn't know the rules or strategies; it just knows it can move pieces on the board. How would it learn to play and eventually become a grandmaster? This is where Reinforcement Learning (RL) comes into play.</p><p>In RL, our chess-playing robot would learn by playing many games, receiving rewards for good moves (like capturing pieces or checkmating) and penalties for bad ones (like losing pieces or getting checkmated). Over time, it would develop strategies to maximize its rewards &#8211; essentially learning to play chess through trial and error.</p><p>This chess example encapsulates the essence of Reinforcement Learning: an agent (our robot) interacting with an environment (the chessboard), taking actions (moving pieces), and receiving rewards (winning or losing). As we explore the key concepts of RL in this blog post, we'll see how this chess scenario illustrates each principle, from the fundamental reward hypothesis to the nature of episodic tasks.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3517" height="2814" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2814,&quot;width&quot;:3517,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;chess pieces on chess board&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="chess pieces on chess board" title="chess pieces on chess board" srcset="https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1611195974226-a6a9be9dd763?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxjaGVzcyUyMHN0cmF0ZWd5fGVufDB8fHx8MTcyMzM5OTk1Mnww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Rafael Rex Felisilda</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><p></p><h2>The Reward Hypothesis: The North Star of RL</h2><p>At the heart of RL lies a beautifully simple idea: <strong>the reward hypothesis. </strong>It states that all goals in RL can be described as maximising expected cumulative rewards. In other words:</p><ul><li><p>An agent's optimal behaviour is learned by taking actions that maximize the expected total reward over time.</p></li><li><p>This hypothesis shapes how we approach problem-solving in RL, focusing on <strong>reward-driven learning</strong>.</p></li></ul><p>Think of it as the "North Star" guiding all RL algorithms. Whether we're training agents to play chess, drive a car, or manage a power grid, we're essentially asking it to maximize some notion of reward.</p><p><strong>Real-world example:</strong> In autonomous vehicle training, the reward might be a combination of factors: maintaining a safe distance from other vehicles (+), reaching the destination quickly (+), avoiding traffic violations (-), and ensuring passenger comfort (+). The RL agent (the car's AI) learns to make driving decisions that balance these rewards optimally.</p><h2>The Markov Property</h2><p>The Markov property is a crucial concept in RL that significantly simplifies the decision-making process:</p><ul><li><p>It states that the agent needs only the current state to decide its next action, not the entire history of states and actions.</p></li><li><p>This property makes RL problems more tractable by reducing the amount of information the agent needs to consider.</p></li></ul><p>While this might seem like an oversimplification of reality (where past events often do matter), it's surprisingly effective in many scenarios and forms the basis of powerful RL algorithms.</p><p><strong>Real-world example:</strong> Consider a stock trading AI. While historical trends are built into the current state (e.g., in the form of technical indicators), the Markov property suggests that the AI's decision to buy, sell, or hold should be based on the current market state, not on remembering every fluctuation from the past.</p><h2>Observation Space and Action Space: The Agent's World and Choices</h2><p>Understanding the environment and possible actions is key in RL:</p><ol><li><p><strong>Observation Space:</strong></p><ul><li><p>Represents the information the agent receives from the environment.</p></li><li><p>Can be fully observable (state) or partially observable (observation).</p></li></ul><p><strong>Example:</strong></p><ul><li><p>Fully observable: In a chess game, the entire board configuration is known (state).</p></li><li><p>Partially observable: In poker, players only know their own and community cards, not the opponents' hands (observation).</p></li></ul></li><li><p><strong>Action Space:</strong></p><ul><li><p>The set of all possible actions an agent can take.</p></li><li><p>Can be discrete (finite number of actions) or continuous (infinite possibilities).</p></li></ul><p><strong>Example:</strong></p><ul><li><p>Discrete: In a recommendation system, actions might be "recommend" or "don't recommend" for each item.</p></li><li><p>Continuous: In robotics, the torque applied to each joint of a robotic arm can be any value within a range.</p></li></ul></li></ol><h2>Types of RL Tasks: Episodic vs Continuous</h2><p>RL tasks can be categorized into two main types, each with its own characteristics and challenges:</p><ol><li><p><strong>Episodic Tasks:</strong></p><ul><li><p>Have a clear starting and ending point.</p></li><li><p>Consists of a sequence of states, actions, rewards, and new states.</p></li></ul><p><strong>Example:</strong> A game of chess is episodic. It starts with a specific board setup and ends when there's a checkmate, stalemate, or draw. The RL agent can learn from completed games and improve its strategy over multiple episodes.</p></li><li><p><strong>Continuous Tasks:</strong></p><ul><li><p>Have no definitive endpoint and continue indefinitely.</p></li><li><p>The agent must learn to choose the best actions while constantly interacting with the environment.</p></li></ul><p><strong>Example:</strong> Managing the temperature in a large office building is a continuous task. The RL agent continuously adjusts heating and cooling systems based on current temperatures, weather forecasts, occupancy, and energy prices without a clear "end" to the task.</p></li></ol><h2>Looking Ahead: Discounting Rewards and the Exploration-Exploitation Tradeoff</h2><p>Now that we've covered the fundamental concepts of Reinforcement Learning, our next exploration will delve into two critical aspects that shape an RL agent's behaviour and learning process. We'll examine the intricacies of reward structures and how they guide an agent's decision-making. Additionally, we'll unpack the fascinating exploration-exploitation tradeoff, a key challenge in RL where agents must balance between exploring new actions and exploiting known successful strategies. These topics will provide deeper insights into how RL algorithms navigate complex decision spaces for optimal performance.</p><div><hr></div><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p><p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p><p></p>]]></content:encoded></item><item><title><![CDATA[Learn, Act, Adapt: Unveiling Reinforcement Learning ]]></title><description><![CDATA[Insights in a Jiffy #1: Teaching Machines to Make Smart Choices]]></description><link>https://neuraforge.substack.com/p/learn-act-adapt-unveiling-reinforcement</link><guid isPermaLink="false">https://neuraforge.substack.com/p/learn-act-adapt-unveiling-reinforcement</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 05 Aug 2024 00:30:08 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Fetch, Max! A Tail of Learning</h2><p>Imagine you're teaching your dog, Max, to fetch a ball. At first, Max doesn't know what to do. But as you play, he learns that bringing the ball back to you results in treats and praise. Chasing the ball but not returning it gets no reward while ignoring the ball might result in you ending the game and putting the ball away. Over time, Max learns the best action: fetch and return the ball. This is reinforcement learning in action!</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="5184" height="3456" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3456,&quot;width&quot;:5184,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;shallow focus photography of white shih tzu puppy running on the grass&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="shallow focus photography of white shih tzu puppy running on the grass" title="shallow focus photography of white shih tzu puppy running on the grass" srcset="https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534361960057-19889db9621e?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxpbnRlbGxpZ2VudCUyMGRvZyUyMHd8ZW58MHx8fHwxNzIyNzk2NTk3fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Joe Caione</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Decoding Reinforcement Learning</h2><p><strong>Reinforcement learning</strong> (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The goal is to maximize a cumulative reward signal over time. In simpler terms, it's about learning what to do&#8212;how to map situations to actions&#8212;to achieve the best outcome. Unlike supervised learning, where the correct answers are provided, RL relies on the agent discovering the best actions through trial and error.</p><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h2>The Building Blocks: RL Framework</h2><p>Let's break down the key components of RL using our dog training example:</p><ol><li><p><strong>Agent</strong>: This is the learner or decision-maker. In our example, it's Max, the dog.</p></li><li><p><strong>Environment</strong>: The world in which the agent operates. For Max, it's the backyard where you're playing fetch.</p></li><li><p><strong>State</strong>: The current situation of the agent in the environment. This could be Max's position relative to the ball and you.</p></li><li><p><strong>Action</strong>: What the agent can do. For Max, actions include running to the ball, picking it up, returning it, or ignoring it.</p></li><li><p><strong>Reward</strong>: The feedback from the environment. In our example, it's treats and praise (positive reward), no response (neutral reward), or ending the game (negative consequence).</p></li></ol><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!gWhx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gWhx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 424w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 848w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 1272w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gWhx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png" width="1456" height="775" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32128,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gWhx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 424w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 848w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 1272w, https://substackcdn.com/image/fetch/$s_!gWhx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa82de419-2e82-4dfe-869b-13d85f251ae7_1600x852.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p></p><h2>The Learning Loop: How It All Works Together</h2><p>Here's how the RL process unfolds:</p><ol><li><p>The agent (Max) observes the current state (the ball has been thrown).</p></li><li><p>Based on this state, the agent chooses an action (e.g., run to the ball).</p></li><li><p>The environment transitions to a new state as a result of this action (Max is now near the ball).</p></li><li><p>The environment provides a reward based on the action and new state (e.g., no reward yet, as Max hasn't returned the ball).</p></li><li><p>The agent uses this information to update its knowledge and improve future decisions.</p></li></ol><p>This process repeats, with the agent continuously learning to make better decisions to maximize its long-term rewards. Over time, Max would learn that running to the ball, picking it up, and returning it to you consistently results in the highest reward (treats and continued play), while ignoring the ball might lead to the less desirable outcome of the game-ending.</p><p>The agent interacts with the environment in a cycle of steps:</p><ol><li><p><strong>Observation</strong>: The agent observes the current state.</p></li><li><p><strong>Decision</strong>: Based on the state, the agent selects an action.</p></li><li><p><strong>Feedback</strong>: The environment responds with a new state and a reward.</p></li><li><p><strong>Update</strong>: The agent updates its understanding to improve future decisions.</p></li></ol><p>Through continuous interaction, the agent improves its policy&#8212;<strong>a strategy for choosing actions based on states</strong>&#8212;to maximize the cumulative reward.</p><h2>Beyond Fetch: The Power of Reinforcement Learning</h2><p>Reinforcement learning is a powerful approach that has applications far beyond dog training. It's used in robotics, game playing, autonomous vehicles, etc. By mimicking how animals learn through trial and error, RL is pushing the boundaries of what machines can achieve. In stock trading, for instance, RL algorithms analyze market trends and adapt their strategies to maximize returns, learning from each trade just as Max learns from each throw of the ball.</p><div><hr></div><p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community. </p><p></p><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[From Perplexity to ROUGE: Essential Metrics for Evaluating LLMs (Easy to Understand!)]]></title><description><![CDATA[#11 Advanced Generative AI: Understanding essential evaluation metrics.]]></description><link>https://neuraforge.substack.com/p/from-perplexity-to-rouge-essential</link><guid isPermaLink="false">https://neuraforge.substack.com/p/from-perplexity-to-rouge-essential</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 08 Apr 2024 17:04:00 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>Large Language Models (LLMs) are becoming increasingly powerful and influential, permeating various domains. This expansion underscores the need for strong evaluation techniques to confirm their dependability and efficacy. By applying metrics, we can evaluate each LLM's performance and compare various models comprehensively, guaranteeing that we employ the most efficient language technology available. In this blog, we will explore popular LLM evaluation metrics in detail.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="5184" height="3888" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3888,&quot;width&quot;:5184,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;gray and yellow measures&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="gray and yellow measures" title="gray and yellow measures" srcset="https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1550985543-49bee3167284?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxtZWFzdXJlfGVufDB8fHx8MTcxMjU5NDExMHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@wwarby">William Warby</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Why are LLM metrics important?</h2><p>As large language models (LLMs) continue to expand in scale, parameter count, and task versatility, concerns about their unpredictability and opaque nature intensify. LLMs' outputs are inherently non-deterministic, meaning semantically identical sentences can be expressed in varied ways. Given this, it becomes crucial to employ metrics as tools to assess the model's reliability and effectiveness. These functions provide a standardized means to evaluate LLMs' performance, ensuring their outputs are dependable and valuable for their intended applications.</p><h2>What are Metrics?</h2><p>Metrics are quantifiable indicators used to evaluate the performance and abilities of these models across different NLP tasks. They are instrumental in determining a model's accuracy, comparing various LLMs, and establishing benchmarks. These benchmarks set performance standards for new models to surpass, thereby fostering innovation and progress in the development of LLMs.</p><h2>Exploring popular metrics available</h2><ol><li><p><strong>Accuracy</strong></p></li></ol><p>In the context of Large Language Models (LLMs), accuracy is a crucial metric used to evaluate the model's performance. Here are some key points about accuracy as a metric in the context of LLMs:</p><ul><li><p><strong>Definition:</strong> Accuracy measures how well an LLM performs a task compared to a human-annotated or predefined standard. It is essentially a score that reflects the model's correctness in its predictions.</p></li><li><p><strong>Calculation:</strong> It is calculated as the ratio of correct predictions to the total number of predictions made, often expressed as a percentage. For example, in text classification tasks, accuracy would indicate how accurately the model can categorize text into the correct classes.</p></li></ul><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n&quot;,&quot;id&quot;:&quot;CSOOHOXQDW&quot;}" data-component-name="LatexBlockToDOM"></div><ul><li><p><strong>Scale:</strong> Accuracy is typically measured on a scale from 0 to 1, where 1 indicates perfect performance (100% accuracy), and 0 indicates complete failure (0% accuracy).</p></li><li><p><strong>Primary Tasks:</strong> Accuracy is particularly relevant in tasks like <strong>classification</strong> (e.g., sentiment analysis) and <strong>named entity recognition</strong>, where the goal is to identify categories or entities in the text correctly.</p></li></ul><ol start="2"><li><p><strong>Perplexity</strong></p></li></ol><p>Perplexity is a measure used to evaluate the performance of probabilistic models, such as language models, in natural language processing (NLP). It quantifies how well a model predicts a sample of text. In the context of Large Language Models (LLMs), perplexity is commonly used to assess the model's ability to understand and generate human-like language.</p><ul><li><p><strong>Definition:</strong> Perplexity measures the uncertainty of a language model in predicting the next word or token in a sequence. It reflects how well the model has captured the patterns in the training data and its ability to generate or comprehend text.</p></li><li><p><strong>Calculation:</strong> The perplexity of an LLM is calculated as the exponential of the average negative log-likelihood of a sequence of words. A higher perplexity score indicates that the model is less certain about its predictions, while a lower score suggests greater confidence and a better understanding of the language structure.</p></li></ul><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\begin{align*}\n\\text{Perplexity} &amp;= 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i)} \\\\\n\\text{where} &amp; \\\\\nN &amp; = \\text{Total number of words in the text} \\\\\nw_i &amp; = \\text{The } i\\text{-th word in the text} \\\\\np(w_i) &amp; = \\text{Probability assigned by the model to the } i\\text{-th word}\n\\end{align*}\n&quot;,&quot;id&quot;:&quot;AFANMZJKBF&quot;}" data-component-name="LatexBlockToDOM"></div><ul><li><p><strong>Scale:</strong> Perplexity is typically expressed as a positive number. Lower values indicate better model performance, with a perplexity of 1 being the ideal score, representing perfect prediction accuracy. Higher values indicate greater uncertainty in the model's predictions.</p></li><li><p><strong>Primary Tasks:</strong> Perplexity is especially relevant in tasks such as casual language modelling and text generation, where the goal is for the model to accurately predict or generate sequences of text that resemble human language.</p></li></ul><ol start="3"><li><p><strong>Bilingual Evaluation Understudy (BLEU)</strong></p></li></ol><p>The BLEU (Bilingual Evaluation Understudy) score is a widely used metric for evaluating the quality of machine-translated text against one or more reference translations. It measures how many words and phrases or n-grams (sequences of n words) match in the translated text compared to the reference translations, considering different lengths of n-grams to assess both the accuracy of individual words and the correctness of word sequences. Additionally, the BLEU score incorporates a brevity penalty to ensure that translations are accurate and of appropriate length, making it a valuable tool for assessing the effectiveness of language translation models in natural language processing (NLP).</p><ul><li><p><strong>Definition:</strong> The BLEU score is a metric that quantifies the similarity between machine-generated translations and one or more reference translations. It evaluates the precision of n-grams in the translated text and applies a brevity penalty to discourage overly short translations.</p></li><li><p><strong>Calculation:</strong> The BLEU score is calculated by comparing the n-grams of the translated text with those of the reference translations, computing precision for different n-gram lengths, and then combining these precision scores using a geometric mean. A brevity penalty is applied if the translated text is shorter than the reference translations.</p></li></ul><div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\begin{align*}\n\\text{BLEU} &amp;= \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right) \\\\\n\\text{where} &amp; \\\\\n\\text{BP} &amp; = \\text{Brevity Penalty, penalizes overly short translations} \\\\\nN &amp; = \\text{Maximum length of n-grams considered (usually 4)} \\\\\nw_n &amp; = \\text{Weight for the } n\\text{-th n-gram (often uniform, i.e., } 1/N\\text{)} \\\\\np_n &amp; = \\text{Precision of the } n\\text{-th n-gram,} \\\\\n    &amp; \\quad \\text{calculated as the ratio of the number of n-grams in the} \\\\\n    &amp; \\quad \\text{candidate translation that match the reference translation} \\\\\n    &amp; \\quad \\text{to the total number of n-grams in the candidate translation}\n\\end{align*}\n&quot;,&quot;id&quot;:&quot;KMTWKTJRXU&quot;}" data-component-name="LatexBlockToDOM"></div><p></p><ul><li><p><strong>Scale:</strong> The BLEU score ranges from 0 to 1, where 1 indicates a perfect match between the translated text and the reference translations. Higher scores represent better translation quality, with 0 indicating no overlap.</p></li><li><p><strong>Primary Tasks:</strong> The BLEU score is used primarily in language translation and summarization. In language translation, it evaluates the quality of translations from one language to another. In text summarization, it assesses how well the summarized text captures the essential information of the original text.</p></li></ul><ol start="4"><li><p><strong>Recall-Oriented Understudy for Gisting Evaluation (ROUGE)</strong></p></li></ol><p>The ROUGE score is a tool used to check how good a summary or translation is in the field of natural language processing (NLP). Unlike another tool called BLEU, which mainly looks at how precise a translation is, ROUGE looks at both how much important information is captured (recall) and how accurate the information is (precision) when comparing a machine-generated text to one or more texts written by humans. This makes ROUGE especially useful for tasks like creating summaries or translating texts, where it's important to get both the main ideas and the details right.</p><ul><li><p><strong>Definition:</strong> ROUGE measures the overlap between the generated text and the reference text(s), considering both the content and the structure. It aims to capture how well the generated text aligns with the reference text(s) in terms of accuracy and completeness.</p></li></ul><ul><li><p><strong>Main Variants:</strong></p><ul><li><p><strong>ROUGE-N:</strong> Evaluates the overlap of n-grams between the generated and reference texts, calculating both recall (the proportion of n-grams in the reference that appear in the generated text) and precision (the proportion of n-grams in the generated text that appear in the reference).</p></li><li><p><strong>ROUGE-L:</strong> It uses the Longest Common Subsequence (LCS) &#8211; the longest string of words appearing in the same order in both texts &#8211; to measure similarity, emphasizing the overall structure and flow of the content.</p></li><li><p><strong>ROUGE-W:</strong> Similar to ROUGE-L, it considers LCS but assigns more weight to longer subsequences, emphasizing the importance of capturing larger chunks of information in the correct order.</p></li><li><p><strong>ROUGE-S:</strong> This variant goes beyond strict word order. It considers "skip-bigrams" &#8211; pairs of words that might have other words in between them in the sentence. This helps capture the meaning even if the phrasing differs slightly between the generated and reference texts.</p></li></ul></li><li><p><strong>Scale:</strong> A higher ROUGE score signifies better agreement between the generated text and the reference text(s), indicating higher quality in terms of both accuracy and completeness.</p></li><li><p><strong>Tasks:</strong> The ROUGE score is primarily used in tasks such as automatic text summarization, where it evaluates how well the summarized text captures the essence of the original text, and in machine translation, where it assesses the quality of the translated text.</p></li></ul><h2>Conclusion</h2><p>In conclusion, using various metrics to evaluate Large Language Models (LLMs) is crucial for assessing their performance and guiding their development. Metrics like accuracy, perplexity, BLEU, ROUGE, and BERTScore help us understand different aspects of LLMs, from their precision to their ability to generate coherent text. By leveraging these metrics, we can identify areas for improvement and drive innovation in the field. As LLMs continue to evolve, so will the ways we evaluate them, ensuring they remain effective tools for various tasks.</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Can Less Be More? Exploring PEFT for LLMs]]></title><description><![CDATA[#10 Advanced Generative AI: Introduction to Parameter Efficient Fine tuning]]></description><link>https://neuraforge.substack.com/p/can-less-be-more-exploring-peft-for</link><guid isPermaLink="false">https://neuraforge.substack.com/p/can-less-be-more-exploring-peft-for</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Wed, 27 Mar 2024 18:01:38 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>The recent advent of open-source large language models like LLama2, Gemma Bloom, etc, has democratized access to a wealth of knowledge, unlocking the immense potential to create AI-powered applications. This development has piqued the interest of companies and individuals keen to train or fine-tune these large language models (LLMs) for their specific use cases. However, this endeavour comes with its own set of challenges. The training process for these models typically demands substantial computational resources and a vast amount of data to achieve high-performance levels, which can be a significant obstacle for many.</p><p>In response to this challenge, specialized training techniques have been devised to empower individuals and organizations to train and infer LLMs on local machines or utilize them with minimal or no cost. These innovative techniques are collectively known as <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> techniques. In this blog, we will delve deeper into the intricacies of PEFT and explore how they are revolutionizing the way we interact with large language models, making them more accessible and feasible for a wide range of users.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="4256" height="2832" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2832,&quot;width&quot;:4256,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;MacBook beside typewriter machine&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="MacBook beside typewriter machine" title="MacBook beside typewriter machine" srcset="https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1560092056-5669e776fc68?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwzfHxjb21wYXJpc29ufGVufDB8fHx8MTcxMTU2MTMxM3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@glenncarstenspeters">Glenn Carstens-Peters</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Motivation behind PEFT</h2><p>Large language models, which consist of billions of parameters, are typically trained on vast datasets to detect and learn complex patterns. This process, known as <strong>fine-tuning</strong>, involves retraining the entire neural network model with a new dataset. However, this approach presents several challenges, including the substantial computational resources required for training and the time it takes to complete the process. Additionally, there is a risk of <strong>catastrophic forgetting</strong>, where the neural network may lose the patterns it learned from previous training.  </p><p><strong>Catastrophic forgetting</strong> occurs when a model loses its ability to perform previously learned tasks as it adapts to new ones. Specifically, it happens when a model's weights, optimized for earlier tasks, are substantially overwritten during the training process for new tasks, resulting in a decline in the model's performance on the old tasks.</p><p>The motivation behind PEFT is to mitigate the challenges associated with traditional fine-tuning by focusing on adjusting only a small subset of the pre-trained model's parameters.</p><h2>What is PEFT?</h2><p><strong>Parameter-efficient fine-tuning (PEFT)</strong> techniques have been developed to address these issues. PEFT is a set of specialized techniques designed to perform training and inference on large language models while consuming significantly fewer resources. The rationale is that most of the pre-trained LLM's knowledge about language and the real world is already captured in the pre-trained parameters. Therefore, PEFT works on modifying a small subset of parameters that are specific to the new task and dataset, making the fine-tuning process more efficient and less prone to <strong>catastrophic forgetting</strong>.</p><p>PEFT offers several advantages over traditional fine-tuning, making it a more efficient and versatile approach:</p><ul><li><p><strong>Reduced Training Burden:</strong>&nbsp;PEFT significantly lowers the computational cost of training. Compared to traditional methods, it requires fewer data and resources, making it more accessible for various applications.</p></li><li><p><strong>Preserving Knowledge:</strong>&nbsp;PEFT tackles the challenge of <strong>catastrophic forgetting</strong>. By freezing most of the pre-trained LLM parameters, it ensures the model retains its general knowledge base while learning a new specific task.</p></li><li><p><strong>Adaptability with Limited Data:</strong>&nbsp;Even with small datasets for a specific task, PEFT can be surprisingly effective. This is because the model leverages the strong foundation of knowledge already learned during pre-training.</p></li><li><p><strong>Fast Switching Between Tasks:</strong>&nbsp;Unlike traditional fine-tuning, PEFT only fine-tunes a small set of parameters for each task. These "PEFT weights" are separate and easily swapped. This allows you to train PEFT models for different tasks independently. Then, you can switch functionalities on the same pre-trained LLM by swapping the relevant PEFT weights &#8211; a much faster and more efficient process.</p></li></ul><p>Overall, PEFT makes LLMs more accessible and efficient by reducing training costs, overcoming data limitations, and enabling smooth switching between tasks.</p><h2>Types of PEFT Techniques</h2><p>There's no one-size-fits-all approach to PEFT. Different techniques are suited for different tasks and data situations. Here's a breakdown of popular techniques available:</p><ul><li><p><strong>Adapter Modules:</strong> These are small neural network layers inserted between the layers of a pre-trained model. During fine-tuning, only the adapter parameters are updated, while the original model parameters remain unchanged. Adapters reduce the number of parameters that need to be trained, making the process more efficient.</p></li><li><p><strong>Prompt Tuning:</strong> This technique involves adding a small set of trainable parameters called <strong>prompts</strong> to the model's input. The prompts are optimized during fine-tuning, guiding the model to generate desired outputs without altering the pre-trained parameters.</p></li><li><p><strong>Low-Rank Adaptation:</strong> In this approach, the weight matrices of the pre-trained model are approximated using <strong>low-rank factorization</strong>. Only the low-rank factors are updated during fine-tuning, reducing the number of trainable parameters.</p></li><li><p><strong>BitFit:</strong> This technique involves fine-tuning only the <strong>bias</strong> parameters of the pre-trained model while keeping the rest of the parameters frozen. BitFit is based on the observation that bias parameters can have a significant impact on model performance with minimal computational overhead.</p></li><li><p><strong>Sparse Fine-Tuning:</strong> Sparse fine-tuning selectively updates only a <strong>small fraction</strong> <strong>of the model's parameters</strong>, chosen based on certain criteria such as their importance or sensitivity to the task at hand. This sparsity reduces the computational burden of fine-tuning.</p></li><li><p><strong>Layer Freezing:</strong> In this technique, only a <strong>subset of the layers</strong> in the pre-trained model is fine-tuned, while the rest are kept frozen. This reduces the number of parameters that need to be updated and can also prevent overfitting.</p></li></ul><p>These techniques represent a powerful toolbox for practitioners to leverage when fine-tuning LLMs in a parameter-efficient manner. Future blog posts will explore these techniques in greater detail, providing code-based implementations and in-depth technical discussions.</p><h2>Conclusion</h2><p>We've only scratched the surface of the immense power and capabilities of modern large language models (LLMs) and how we can efficiently harness this wealth of information and knowledge through Parameter-Efficient Fine-Tuning (PEFT) techniques. Stay tuned for our upcoming release, where we will delve deeper into a more comprehensive discussion, analysis, and code-based exploration of the key PEFT techniques mentioned above. Get ready to unravel the exciting world of generative AI and unlock the power of LLMs for your projects.</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">NeuraForge: AI Unleashed is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Decoding the Art: Understanding Text Generation with Transformers - II]]></title><description><![CDATA[#9 Advanced Generative AI: Exploring Text Generation Techniques with Transformers - Part II]]></description><link>https://neuraforge.substack.com/p/decoding-the-art-understanding-text-58b</link><guid isPermaLink="false">https://neuraforge.substack.com/p/decoding-the-art-understanding-text-58b</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Tue, 26 Mar 2024 17:47:57 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>The abilities and effectiveness of large language models (LLMs) increasingly resemble human-like qualities, surpassing human capabilities in certain aspects, thanks to their remarkable text generation capabilities. A key factor contributing to this success is the ability to fine-tune these models to produce coherent and creative text. By adjusting the mechanisms behind text generation, we can steer LLMs to generate text in desired ways. Continuing our blog series, we will delve into additional sampling methods, such as nucleus sampling, custom sampling, and beam search sampling, to further enhance the quality of text generation.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3308" height="4135" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4135,&quot;width&quot;:3308,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;person holding light bulb&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="person holding light bulb" title="person holding light bulb" srcset="https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493612276216-ee3925520721?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyYW5kb218ZW58MHx8fHwxNzExNDMzOTg5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@jdiegoph">Diego PH</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Exploring Text Generation Techniques</h2><h3><strong>1. Top-k Sampling</strong></h3><p>Top-k sampling is a widely used variation of random sampling that limits the pool of tokens we can sample at each timestep. The core concept of top-k sampling is to eliminate low-probability options by only considering the <em>k</em> tokens with the highest probability of sampling. This technique can often produce more natural-sounding text than other techniques.</p><h4><strong>How Top-k Sampling Works</strong></h4><ul><li><p>Top-k sampling restricts the possible next words to the k most probable ones, effectively shrinking the dictionary it uses for prediction.</p></li><li><p>Lower k values in top-k sampling promote conservative text generation by prioritizing highly probable words within the context.</p></li><li><p>For example, if k is 50, we choose the top 50 tokens from the vocabulary and then perform random sampling from these 50 tokens to generate text. </p></li></ul><h4><strong>Implementation</strong></h4><p>Top-k Sampling can be activated easily in the transformers <code>generate()</code> function by setting the <code>do_sample</code> parameter to True, specifying the <code>temperature</code> parameter to control creativity and <code>top_k</code> to define the k tokens from which to sample. The choice of the parameter <code>k</code> is to be manually chosen based on the vocabulary size and probability distribution. </p><p>Below is an example of how to implement top-k sampling with the <strong>GPT-2 XL </strong>model using the transformers library:</p><p><strong>Case I: With </strong><code>temperature=2.0</code><strong> (high) and </strong><code>top_k=70</code></p><pre><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup the model and tokenizer
checkpoint = "gpt2-xl"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

# Encode the inputs
input_text = "Once upon a time, there was a man who"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Top-k sampling
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=2.0)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who wanted an autographeter a lot in honor "something his old band, Bon Iver's indie icon brothers Scott and Jared had made for him", thus started "This Old Radio Show" in July 2016 by Aaron Schock's son Kyle as guest cohost. [1] As per [note2].""&lt;|endoftext|&gt;</code></pre><p>The above-generated output is very unusually diverse and doesn&#8217;t have clarity as to what it is conveying. Setting a high <code>temperature</code> parameter value (i.e. &gt;1.0) is responsible for the high variation and diversity in the generated content.</p><p><strong>Case II: With </strong><code>temperature=0.9</code><strong> and </strong><code>top_k=70</code></p><pre><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_k=50)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who made a beautiful thing, a beautiful thing that would bring happiness, joy, beauty, and hope to everyone - one that had a heart and soul that would not be broken.

The man, who made this, called it "I am the Sun".

His name was, and is, J.C. Sutter.</code></pre><p>We can observe that the above text has better clarity and coherence than the previous output. </p><p>Now, let&#8217;s explore another sampling method that aids in restricting the output distribution but with a dynamic cutoff, unlike top-k, where we have a fixed cut-off of tokens. </p><h3><strong>2. Nucleus Sampling</strong></h3><p> Nucleus Sampling, or top-p sampling, is another important technique for controlling the randomness and creativity of the generated text. With top-p sampling, instead of choosing a fixed cutoff value, we set a condition for when to cut off. This condition is when a certain probability mass in the selection is reached. </p><p>While top-k sampling focuses on the total number of considered words, top-p focuses on the total probability or cumulative probability captured. </p><h4><strong>How Nucleus Sampling Works</strong></h4><ul><li><p>This method sets a probability threshold (<code>p</code>) instead of a fixed number of words (<code>k</code>). </p></li><li><p>Then, it selects all the tokens from the vocabulary sorted in descending order until the cumulative probability reaches or exceeds the threshold <code>p</code>. </p></li><li><p>For example, if p is 95%, we choose all the tokens whose cumulative probability equals 95%. </p></li></ul><p>This technique can be more flexible than top-k because the number of words considered can be dynamically changed based on probability distribution. </p><h4><strong>Implementation</strong></h4><p>Nucleus sampling can be activated in the transformer&#8217;s <code>generate()</code> function by setting the probability threshold using the <code>top_p</code> parameter. Then, we can control the randomness and diversity using the <code>temperature</code> parameter. </p><p>Below is an example of how to implement top-p sampling with the <strong>GPT-2 XL </strong>model using the transformers library:</p><pre><code><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_p=0.9)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who came to the King of the Fairies and asked, "what is the best way to gain eternal life?" He heard that the Fairy Queen had a secret name, and would answer nothing, only that she was "the most beautiful lady in all the world." He asked her, "Whence comes that name?" And she answered that she had it</code></pre><p>Now, let&#8217;s understand how custom sampling performs for text generation, where we can employ both top_k and top_p parameters.</p><h3><strong>3. Custom Sampling</strong></h3><p>Custom sampling combines top_k and top_p sampling techniques to achieve the best of both worlds. If we set top_k=50 and top_p=0.9, this corresponds to the rule of choosing tokens with a probability mass of 90% from a pool of at most 50 tokens.</p><p>This method provides a finer degree of control over the generated text, helps generate grammatically correct text, and is potentially more creative than individual sampling techniques. </p><h4><strong>Implementation</strong></h4><p>We can implement this by defining both top_k and top_p parameters. Consider the example below.</p><pre><code>output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.9, top_p=0.9, top_k=50)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time ive been the sole proprietor of a small business, and since its inception i have been very much influenced by the people i have met through the website, and i will continue to do so.

My business goal is to grow a small business and take it to the next level, and i want to do that through a forum like this, </code></pre><h2>Conclusion</h2><p>The potential and prowess of large language models (LLMs) in generating text, images, and multimedia are advancing at an exponential rate and show no signs of slowing down as research and development efforts persist. As such, we must grasp and master various text generation techniques, which play a pivotal role in achieving desired outcomes from LLMs through appropriate fine-tuning. In conclusion, our exploration of these techniques is not just an academic exercise but a practical necessity in harnessing the full capabilities of LLMs in the ever-evolving landscape of artificial intelligence.</p><p>Happy Learning !!</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><p></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[Decoding the Art: Understanding Text Generation with Transformers - I]]></title><description><![CDATA[#8 Advanced Generative AI: Exploring Text Generation Techniques with Transformers - Part I]]></description><link>https://neuraforge.substack.com/p/decoding-the-art-understanding-text</link><guid isPermaLink="false">https://neuraforge.substack.com/p/decoding-the-art-understanding-text</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 25 Mar 2024 16:43:53 GMT</pubDate><enclosure url="https://substackcdn.com/image/fetch/$s_!W73Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction </h2><p>One of the most remarkable abilities of transformer models is their capacity to generate text that closely mimics human writing. With extensive training, large language models (LLMs) like GPT-3 can develop a wide range of skills for understanding and recognizing patterns in the text they've been trained on. These skills can be harnessed through various input prompts. In this blog, let&#8217;s  delve into different text generation techniques using transformer-based models and examine how various hyperparameters play a crucial role in shaping the LLM's response to a given prompt.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!W73Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!W73Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 424w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 848w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1272w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic" width="698" height="698" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:698,&quot;bytes&quot;:161002,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!W73Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 424w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 848w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1272w, https://substackcdn.com/image/fetch/$s_!W73Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc91b79e4-3dc6-40f6-8a34-348fabd4fd90.heic 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">AI-Generated Image: Robot Writing</figcaption></figure></div><h2><strong>Understanding Text Generation in Transformers</strong></h2><p>The decoder component of a transformer model primarily facilitates text generation. The decoder iteratively produces tokens based on the input prompt until they reach the specified length. This process, known as <strong>conditional text generation</strong>, depends on the input prompt to shape the output sequence.</p><p>The decoding method is central to this mechanism, which determines the selection of tokens from the vocabulary at each timestep. Various decoding techniques, each with its own set of hyperparameters, can be employed to achieve the desired output for a given prompt. In this blog, let&#8217;s explore some of these methods and their impact on text generation. </p><h2>Exploring Text Generation Techniques</h2><h3><strong>1. Greedy Search Decoding</strong></h3><p>Greedy search decoding is the most basic method used in text generation. This approach is characterized by its simplicity and speed, as it makes decisions based solely on the immediate next token without considering the overall sequence.</p><h4><strong>How Greedy Search Works</strong></h4><ul><li><p>It selects the token with the highest probability, or logits, at each timestep.</p></li><li><p>This approach is straightforward and can quickly generate text.</p></li></ul><h4><strong>Implementation</strong></h4><p>Greedy Search can be implemented using the transformers library by setting the <code>do_sample</code> parameter to <code>False</code> in the <code>generate()</code> function. We can also specify the maximum number of tokens generated by setting the <code>max_new_tokens</code> parameter.</p><p>Below is an example of how to implement greedy search decoding with the   <strong>GPT-2 XL</strong> model using the transformers library:</p><pre><code><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup the model and tokenizer
checkpoint = "gpt2-xl"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

# Encode the inputs
input_text = "Once upon a time, there was a"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Generate output using Greedy Search
output = model.generate(input_ids, max_new_tokens=70, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man named John. John was a man of God. He was a man of God who loved his family and his friends. He was a man of God who loved his country and his God. He was a man of God who loved his God and his country. He was a man of God who loved his God and his country. He was a
</code></pre><p>Analyzing the output generated by the model using greedy search decoding, we notice that while the text was successfully completed, the model repetitively produced the same sentence until reaching the maximum token limit. </p><h4><strong>Limitations</strong></h4><ul><li><p>Greedy search often leads to repetitive sequences, underscoring its limitations.</p></li><li><p>This limitation is particularly pronounced when diversity is desired in text generation.</p></li><li><p>The issue arises from the method's emphasis on choosing the highest probability word at each step.</p></li><li><p>As a result, it overlooks sequences that might have a higher overall probability.</p></li></ul><h4><strong>Use Cases</strong></h4><p>Although the greedy search has shortcomings, it can be effective for tasks that demand <strong>deterministic and factually accurate outputs, such as arithmetic problems</strong>. Nonetheless, it is less frequently employed for tasks requiring varied or imaginative text.</p><h3>2. Beam Search Decoding</h3><p>Beam search introduces a refined approach to token selection in text generation. Beam search is an algorithm used to improve the quality of text generation in models like those based on transformer architecture. It balances the brute-force exactness of exhaustive search and the practical efficiency of greedy search.</p><h4><strong>How Beam Search Works</strong></h4><ul><li><p>Rather than choosing the highest-probability token at each step, this method maintains the top-b most probable next tokens, where <code>b</code> represents the<em> <strong>beam width</strong></em><strong> </strong>or <em><strong>beam size</strong></em>.</p></li><li><p>It involves evaluating all potential extensions of the current set and selecting the <code>b</code> most promising ones. This process is repeated until the maximum number of tokens is reached, ensuring a more balanced search space exploration.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Q8xB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 424w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 848w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1272w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic" width="1456" height="876" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:54906,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Q8xB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 424w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 848w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1272w, https://substackcdn.com/image/fetch/$s_!Q8xB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c31e15-0f0c-4120-b58e-59e2ebc7af11.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: <a href="https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_beam_search/beam_2.jpg">HuggingFace</a> - Beam Search Step Visualization</figcaption></figure></div><h4><strong>Implementation</strong></h4><p>Beam Search can be activated with the <code>generate() </code>function by specifying the <code>num_beams </code>parameter. The more beams we choose, the better the result potentially gets. </p><p>Below is an example of how to implement beam search decoding with the      <strong>GPT-2 XL</strong> model using the transformers library:</p><p><strong>Case I</strong>: <strong>With </strong><code>num_beams=5</code></p><pre><code><code># Beam Search Decoding with num_beams=5
output = model.generate(input_ids, max_new_tokens=70, num_beams=5, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who lived in a small town. He had a wife and two children. One day, he went to the store to buy some milk. When he returned home, he found that his wife and children were gone. He searched for them, but they were nowhere to be found. He went to the police station to report the disappearance of his family
</code></pre><p>Analyzing the output from beam search decoding reveals that it generated more coherent and clear text. Let&#8217;s try another example by increasing the <code>num_beams</code> parameter to 10. </p><p><strong>Case II</strong>: <strong>With </strong><code>num_beams=10</code></p><pre><code># Beam Search Decoding with num_beams=10

output = model.generate(input_ids, max_new_tokens=70, num_beams=10, do_sample=False)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who had a dream.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.

He had a dream that one day he would be able to fly.</code></pre><p>From the second output, we can see that beam search also suffers from repetitive text. One way to address this is to impose an <em>n</em>-gram penalty with the <code>no_repeat_ngram_size</code> parameter that tracks which <em>n</em>-grams have been seen and sets the next token probability to zero if it would produce a previously seen <em>n</em>-gram, thereby reducing repetitions: </p><p><strong>Case I</strong>: <strong>With </strong><code>num_beams=5 </code><strong>and</strong><code> no_repeat_ngram_size=3</code></p><pre><code># Beam Search Decoding with num_beams=10 and no_repeat_ngram_size=3

output = model.generate(input_ids, max_new_tokens=70, num_beams=10, do_sample=False, no_repeat_ngram_size=3)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who lived in a small town. He had a wife, a son, and a daughter. One day, the man went to visit his son in the hospital. When he got there, he found out that his son had died of a heart attack. The man was devastated. He didn't know what to do, so he went to his</code></pre><p>Well, from this output, we can observe the generated text is more coherent and clear than the previous output, with <code>num_beams=10. </code></p><h4><strong>Limitations</strong></h4><ul><li><p>Beam Search suffers from issues like repetitive token generation and bias towards shorter sequences.</p></li><li><p>The beam search algorithm requires tracking multiple beams at each step, which increases memory usage and computational complexity compared to greedy search.  </p></li></ul><h4><strong>Use Cases</strong></h4><p>Beam Search with <em>n</em>-gram penalty is commonly used in applications like <strong>summarization</strong> and <strong>machine translation</strong> where factual correctness is emphasised over diversity. Now, let&#8217;s explore the text generation technique where we can make the generated text more creative and diverse.</p><h3>3. Random Sampling</h3><p>Random sampling is a strategy employed in text generation, particularly in scenarios where diversity is prioritized over factual accuracy, such as story generation or open-domain conversations. Unlike greedy search, which consistently opts for the token with the highest probability, this method introduces an element of randomness to the selection process.</p><h4><strong>How Random Sampling Works</strong></h4><ul><li><p>The model produces a probability distribution for the next token over the entire vocabulary based on the input prompt and the tokens generated.</p></li><li><p>Instead of choosing the most probable token, a token is randomly selected from this probability distribution, allowing for a broader exploration of possible sequences. </p></li><li><p>The degree of randomness in sampling can be adjusted using a parameter known as <code>temperature</code>. Higher <code>temperatures</code> lead to a more uniform distribution, enhancing randomness and diversity. Conversely, a lower <code>temperature</code> produces a sharper distribution, making the output more similar to the highest probability sequence.</p></li></ul><h4><strong>Implementation</strong></h4><p>Random Sampling decoding technique can be activated easily in the transformer&#8217;s <code>generate()</code> function by setting the <code>do_sample</code> parameter to <code>True</code> and specify the <code>temperature</code> to control the creativity and diversity of the created text.</p><p>Below is an example of how to implement random sampling with the <strong>GPT-2 XL</strong> model using the transformers library:</p><p><strong>Case I</strong>: <strong>When </strong><code>temperature</code><strong> is high (&gt;1.0) </strong></p><pre><code><code># Define the input prompt
input_text = "Once upon a time, there was a man who"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)

# Perform Random Sampling to generate 70 tokens
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=2.0)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who liked to cook. Every so often this man visited one part of Egypt more than three different times every fifteen years&#8230; until finally they saw him leave Egypt entirely forever, his back into the north&#8230; he set himself off somewhere. And on this very lonely place would he spend fifty five years, every generation&#8230;

How strange the sound those sixty-</code></pre><p>In the above output, we can observe that the generated text is very diverse in nature, without any formal plot in the story. This is due to setting the <code>temperature</code> parameter as 2 (i.e. high value)</p><p><strong>Case II</strong>:  <strong>When </strong><code>temperature</code><strong> is low (&lt;1.0) </strong></p><pre><code># Random Sampling with temperature=0.7
output = model.generate(input_ids, max_new_tokens=70, do_sample=True, temperature=0.7)
generated_text = tokenizer.decode(output[0])
print(generated_text)</code></pre><p><strong>Code Output:</strong></p><pre><code>Once upon a time, there was a man who made a name for himself in the world. He was handsome, charming, and popular with the ladies, but his wife was not satisfied with his success. She wanted more. She thought she knew what needed to be done, and she was determined to make him do it.</code></pre><p>Upon reviewing the output above, we can get the gist of the generated text and ensure that it maintains a reasonable level of diversity. </p><h4><strong>Limitations</strong></h4><ul><li><p>The randomness can sometimes result in nonsensical or off-topic text.</p></li><li><p>The overall quality and coherence of the text might be lower compared to more deterministic methods.</p></li><li><p>It cannot be used for tasks where factual correctness and deterministic outputs are expected.</p></li></ul><h4><strong>Use Cases</strong></h4><p>Random sampling can be particularly beneficial in creative writing, where text diversity is essential. By introducing an element of unpredictability, it can enhance the narrative's richness and originality. Additionally, random sampling can generate varied responses in text conversations and chatbots. This variability can contribute to a more human-like feel, making users' interactions with these systems more engaging and natural.</p><h2>Conclusion</h2><p>Various techniques, such as top-k sampling and nucleus sampling, can enhance the coherence and creativity of the text. The next blog will discuss these techniques in more detail. The notebook for the code provided above can be accessed at this <a href="https://www.kaggle.com/code/narasimhajwalapuram/text-generation-with-transformers/notebook?scriptVersionId=168118058">link</a>.</p><p>Happy Learning!</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[Unlock the Power of Generative AI: Mastering Personalized Model Development]]></title><description><![CDATA[#7 Generative AI with LLMs: Step-by-step guide to Fine-Tune GPT-2 with Custom Data Using Transformers]]></description><link>https://neuraforge.substack.com/p/unlock-the-power-of-generative-ai</link><guid isPermaLink="false">https://neuraforge.substack.com/p/unlock-the-power-of-generative-ai</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 22 Jan 2024 00:30:32 GMT</pubDate><enclosure url="https://substackcdn.com/image/fetch/$s_!UTP4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>The impact of the <strong>Generative Pre-trained Transformers</strong> (GPT) series by OpenAI in the revolution of large language models has been undeniable. In this blog, we delve into fine-tuning a variant of the GPT-2, <strong>GPT-2 small</strong> variant using Hugging Face's Transformers library, a process crucial for tailoring these models to specific language tasks.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!UTP4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UTP4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 424w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 848w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1272w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:346225,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UTP4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 424w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 848w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1272w, https://substackcdn.com/image/fetch/$s_!UTP4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20b18bf5-fd0f-424c-9964-3f009c64cb8d.heic 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Image Generated by Dall-E</figcaption></figure></div><h2>Decoding GPT-2 &amp; Transformers: The Power Duo in Language Modelling</h2><p>The GPT-2 model is a prominent example of a decoder-based language model developed by OpenAI. Transformer models, like GPT-2, are distinguished by their ability to process words in relation to all other words in a sentence. This contrasts with earlier models that processed text in sequential order.</p><p>Hugging Face's <strong>Transformers</strong> library is a comprehensive suite that simplifies using thousands of pre-trained models for various natural language processing tasks. It bridges these complex models and developers, providing tools for easy implementation, customization, and deployment of many open-source AI models, including GPT-2. This library is widely recognized for its user-friendly interface and extensive documentation, making cutting-edge AI accessible to novice and expert practitioners. The combination of GPT-2&#8217;s advanced capabilities and the Transformers library&#8217;s ease of use offers an unparalleled toolkit for developing sophisticated language processing applications. </p><h2>Language Modelling Deep Dive</h2><p>In this section, let&#8217;s dive deep into different stages of fine-tuning the GPT-2 model with a custom dataset. </p><h3>Setting Up the Development Environment</h3><p>Begin by preparing your development environment, a vital step for success in your project. Start with installing <strong>Python</strong>, then install Hugging Face's Transformers library, which is essential for this endeavour. Use Python's package installer, pip, for a smooth installation:</p><pre><code>!pip install transformers
!pip install datasets
!pip install accelerate -U
!pip install apache-beam</code></pre><p>Additionally, consider leveraging Google Colab for your project. <strong>Google Colab</strong> provides a cloud-based platform that offers free access to powerful GPUs, which can significantly accelerate the training and fine-tuning process of models like GPT-2. This makes it an ideal choice, especially if you have limited resources or require advanced computational capabilities. With these tools and platforms at your disposal, you are now equipped to dive into the fine-tuning of the GPT-2 language model. Google Colab can be accessed through this <a href="https://colab.research.google.com/">link</a>. </p><h3><strong>Delving into the Wikipedia Dataset</strong></h3><p>For fine-tuning GPT-2, the <strong><a href="https://huggingface.co/datasets/wikipedia">Wikipedia dataset</a></strong> from Hugging Face offers a comprehensive collection of articles, ideal for training language models. This dataset provides a diverse range of topics and writing styles, making it perfect for enhancing the linguistic understanding and generative capabilities of GPT-2. Load the dataset from the <strong>datasets</strong> library: </p><pre><code>from datasets import load_dataset

dataset = load_dataset("wikipedia", "20220301.en")</code></pre><p>We can view the contents of the dataset by printing the structure. </p><pre><code>print(dataset)</code></pre><pre><code>Output:

DatasetDict({
    train: Dataset({
        features: ['id', 'url', 'title', 'text'],
        num_rows: 6458670
    })
})</code></pre><p>Given the vast dataset size, which includes over <strong>6 million rows</strong>, we'll focus on a subset for this tutorial. Training on the entire dataset would demand extensive computational resources.</p><pre><code># Choose a data subset
data_subset = dataset['train'].select(range(1000))
print(data_subset)

# Split the dataset into training and validation sets
split = data_subset.train_test_split(test_size=0.1)
train_dataset = split['train']
val_dataset = split['test']</code></pre><h3>Loading the GPT-2 Model &amp; Tokenizer</h3><p>For this task, let&#8217;s use the <strong>small </strong>variant of the GPT-2 model. Load the GPT-2  model and its tokenizer from Hugging Face's <strong>Transformers</strong> library: </p><pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")</code></pre><p>Now, let&#8217;s process the Wikipedia dataset to prepare it for model training. </p><h3>Data preprocessing: Tokenization </h3><p>Before diving into the fine-tuning process, preparing the Wikipedia dataset appropriately is essential. This preparation involves tokenizing the text data and converting it into a format the GPT-2 model can understand and process efficiently. We use the <code>GPT2Tokenizer</code> loaded earlier to tokenize the text. This process breaks down the text into tokens or smaller pieces, aligning with the model's linguistic understanding. Once tokenized, the dataset is ready for model training, ensuring that the input data aligns with the internal workings of GPT-2. This step is crucial in ensuring effective training and fine-tuning of the model on the dataset.</p><p>Let&#8217;s split the dataset before proceeding with model training.   </p><pre><code># Function to tokenize the text
def tokenize_function(inputs):
    return tokenizer(inputs['text'],
                    padding="max_length",
                    max_length=512,
                    truncation=True,
                    return_overflowing_tokens=True,
                    return_length=True)

# Apply the tokenization to the dataset splits
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_valid = val_dataset.map(tokenize_function, batched=True)</code></pre><h3>Fine-tuning of GPT-2 with Wikipedia Dataset: Model training </h3><p>In the fine-tuning process, the <strong>TrainingArguments</strong> and the <strong>Trainer</strong> classes from the transformer library play pivotal roles. We can implement these classes in code as follows:</p><pre><code># Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    evaluation_strategy="epoch",
    save_steps=10_000,
    save_total_limit=2,
    report_to=None
)

# Custom function to compute perplexity
def compute_perplexity(eval_pred):
    logits, labels = eval_pred
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    loss_fct = torch.nn.CrossEntropyLoss()
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    return {"perplexity": torch.exp(loss)}</code></pre><p>The more detailed explanation is provided below,</p><ul><li><p><strong>TrainingArguments: </strong>We define various parameters for training our model. Some of the import parameters of this class are:</p><ul><li><p><strong>Output Directory (</strong><code>output_dir</code>): Specifies where to save the model.</p></li><li><p><strong>Number of Epochs (</strong><code>num_train_epochs</code>): Sets how many times the model will see the entire dataset.</p></li><li><p><strong>Batch Sizes (</strong><code>per_device_train_batch_size</code>, <code>per_device_eval_batch_size</code>): Determines the number of samples processed before the model is updated.</p></li><li><p><strong>Evaluation and Save Steps (</strong><code>eval_steps</code>, <code>save_steps</code>): Defines frequency of evaluation and model saving.</p></li><li><p><strong>Warmup Steps (</strong><code>warmup_steps</code>): Adjusts learning rate in the initial training phase.</p></li><li><p><strong>Logging Directory (</strong><code>logging_dir</code>): Location for storing training logs.</p></li></ul><p></p></li><li><p><strong>Trainer</strong>: The <code>Trainer</code> is a powerful class in the Transformers library that abstracts much of the training loop. Some essential parameters passed as inputs to this model are: </p><ul><li><p><strong>Model (</strong><code>model</code>): The pre-trained GPT-2 model to be fine-tuned.</p></li><li><p><strong>Training Arguments (</strong><code>args</code>): The <strong>TrainingArguments</strong> instance as detailed above.</p></li><li><p><strong>Training Dataset (</strong><code>train_dataset</code>): The subset of the Wikipedia dataset for training.</p></li><li><p><strong>Evaluation Dataset (</strong><code>eval_dataset</code>): The subset of the Wikipedia dataset for evaluation.</p></li><li><p><strong>Metrics Function (</strong><code>compute_perplexity</code>): Function to evaluate model performance with perplexity metric.</p></li></ul></li></ul><p>This structured approach in configuring the <code>TrainingArguments</code> and <code>Trainer</code> ensures an optimized environment for effectively fine-tuning the GPT-2 model on the chosen dataset.</p><pre><code># Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_valid,
    compute_metrics=compute_perplexity,
)

# Training the model
trainer.train()</code></pre><h3>Save the Model</h3><p>Once the model is trained, we can save the model for performing further fine-tuning or inference with  data. </p><pre><code># Save the model
model.save_pretrained("./gpt2-medium-finetuned")</code></pre><h2>Conclusion</h2><p>In this blog, we explored how to prepare and tokenize the dataset for Causal Language Modeling tasks and perform fine-tuning on the GPT-2 pre-trained model using the HuggingFace Transformers library. Using the above code and method, we can fine-tune any model and dataset of our choice, just by replacing the model checkpoint and the dataset source. Just ensure you have enough computing available in terms of GPUs and memory.</p><p>Thank you for reading!</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Demystifying LLM training: Investigating Fine-tuning techniques.]]></title><description><![CDATA[#6 Generative AI with LLMs: Advancing LLM Modelling with Task based Fine-tuning.]]></description><link>https://neuraforge.substack.com/p/demystifying-llm-training-investigating</link><guid isPermaLink="false">https://neuraforge.substack.com/p/demystifying-llm-training-investigating</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Thu, 26 Oct 2023 00:30:22 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction&nbsp;</h2><p>In current methods of In-context learning, the large language model will be used to perform inference by providing input prompts with relevant context and generating required completions. This technique works fine for larger models with billions of parameters like GPT4 but is unsuitable for performing efficient inference from smaller models like GPT2. One-shot learning and few-shot learning techniques take up the additional context window of the model, thereby reducing the amount of input information provided for generation required completion. To increase the quality of generated completion, one must perform instruction-based fine-tuning.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3193" height="4062" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4062,&quot;width&quot;:3193,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;3 blue and red diamond illustration&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="3 blue and red diamond illustration" title="3 blue and red diamond illustration" srcset="https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1597589827307-d393da1520d2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxMXx8ZmluZXxlbnwwfHx8fDE2OTgyNTQyMzd8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@lazycreekimages">Michael Dziedzic</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Instruction-based Finetuning of LLMs</h2><p><strong>Finetuning</strong>&nbsp;involves training the pre-trained LLMs with instructions on generating completions for a provided input prompt. We train the model on a set of prompt-completion pairs of text datasets to achieve this. Finetuning is a supervised learning algorithm that uses a pair of input prompts and desired completions as inputs and output, respectively, for model training. Finetuning is usually performed as an additional step on pre-trained models.&nbsp;</p><p><strong>Pretraining</strong> is a step performed before finetuning, and it is a type of self-supervised learning technique usually done for domain adaption and learning the patterns of the underlying language.&nbsp;</p><p>Pretrained models are not task-specific and cannot directly perform downstream tasks like question answering or sentiment analysis. During pretraining, the primary focus is on training models to comprehend and analyze the language's patterns and linguistic characteristics rather than focusing on specific tasks. Further finetuing on this pre-trained model with an instruction dataset, i.e., a dataset with defined tasks with input prompts and desired completions, will enable the model to learn the patterns and knowledge required to perform the task.&nbsp;</p><p>Currently, available models like GPT4, Falcon or LLama are finetuned on curated instruction datasets and can perform tasks like question answering, text summarization and code generation with suitable prompts provided. Let's explore the overview of the instruction-based finetuning process of LLMs.&nbsp;</p><h2>Overview of LLM finetuning process</h2><p>Performing instruction-based finetuning of LLMs can be achieved by following these steps.:</p><ul><li><p>Preparing Instruction Dataset</p></li></ul><p>The instruction dataset consists of a set of input prompts along with desired completions in a specific prompt format. An instruction dataset is usually created to support a variety of tasks by utilizing existing/available prompt templates for different tasks. Prompt templates are combined with data sources to create an instruction dataset. To train the model to generate coherent text for given instructions accurately, the pre-trained model undergoes finetuning. This is achieved through backpropagation, which allows the model to learn from its mistakes and improve its performance.</p><p>The model will generate required and relevant completion when unseen input is fed into the model with the prompt template used for training. This process of using the same prompt template for designing new prompts during inference for performing text generation is called prompt engineering during inference.&nbsp; Following is an example of  <strong><a href="https://github.com/tatsu-lab/stanford_alpaca#stanford-alpaca-an-instruction-following-llama-model">Stanford alpaca prompt format</a></strong> created for fine-tuning a variation of the Llama Model. </p><pre><code>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:</code></pre><ul><li><p>Split the Instruction Dataset</p></li></ul><p>After preparing the instruction dataset, it should be split into training and validation since Finetuning is an algorithm based on supervised learning.</p><ul><li><p>Perform Fine-tuning</p></li></ul><p>Then, the pre-trained model is further trained with the prompt-based dataset using the backpropagation learning algorithm. This process of finetuning the trained model on downstream applications is called&nbsp;<strong>Transfer Learning</strong>. This will enable the model to utilize the knowledge and patterns learnt from pretraining and quickly understand the structure and context of the specific tasks from the prompt-completion pairs from the prompt dataset.&nbsp;</p><h2>Different types of Finetuning and when it is done?</h2><p>Fine-tuning can be performed in different ways depending upon the amount of input samples available, the nature of the task being fine-tuned and the requirements needed. Based on the constraints, the Finetuning process can be performed in the following ways:</p><h3>Finetuning on Single-task</h3><p>Finetuning on a single task can be done by training the model on a prompt-based dataset that consists of prompt-completion pairs to perform a single task. For this technique, only&nbsp;<strong>500 to 1000</strong>&nbsp;prompt-completion pairs of a single task are needed for fine-tuning. This is a beneficial technique when the available dataset is minimal. This technique is usually applied in the use cases of training domain-specific models like aerospace, healthcare, etc, where the curated domain-specific dataset is not abundantly available.&nbsp;</p><p>However, fine-tuning the pre-trained model on a single task only might lead to training-related issues such as overfitting and catastrophic forgetting. When the available prompt dataset only consists of inputs with a single type of task, the model might overfit and not learn any significant patterns from the dataset. On the other hand, the model might forget the knowledge, understanding and patterns learnt during the pretraining process. This phenomenon is called <strong>Catastrophic Forgetting</strong>.&nbsp;</p><p>Catastrophic forgetting might not be an issue if the intended outcome of the fine-tuning is to perform a single task (or doesn&#8217;t perform multi-tasking) and the past understanding of the knowledge is not required. But in most cases, it is a drawback, and it can be prevented by doing the following.</p><ul><li><p>Finetune pre-trained models on&nbsp;<strong>multiple tasks</strong>&nbsp;simultaneously.&nbsp;</p></li><li><p>Creating an instruction-based dataset with more than&nbsp;<strong>50k-100k&nbsp;</strong>samples of prompt-completion pairs.</p></li><li><p>Considering&nbsp;<strong>Parameter Efficient Finetuning (PEFT)</strong>&nbsp;techniques for fine-tuning pre-trained models.&nbsp;</p></li></ul><p>Applications of the fine-tuning technique are primarily domain-based fine-tuning for a particular task only.&nbsp;</p><h3>Multi-Task Instruction Fineutning.</h3><p>Multi-task instruction fine-tuning consists of fine-tuning a pre-trained model with a prompt dataset with multiple tasks simultaneously, like text summarization, machine translation, question answering, etc. This enables the model to learn complex patterns from the dataset and prevent overfitting. This is one of the vital methods used to mitigate the issue of catastrophic forgetting while finetuning large language models. Provided these require lots and lots of curated prompt datasets.&nbsp;</p><p>Some popular multi-billion parameter models, like GPT4, ChatGPT, LLama, PaLm and Falcon, are trained using the Multi-Task instruction-based finetuning technique with sufficient data samples for each task in the prompt dataset.</p><h2>Conclusion</h2><p>Finetuning is a process of training pre-trained models on the prompt-based dataset, resulting in models with zero-shot capabilities for the tasks being trained for. This technique will enable fine-tuned models to perform required tasks by generating relevant completions for the provided input prompt, resulting in higher accuracy and more coherent text. Therefore, it is a vital step in training LLMs and using them to perform required tasks.&nbsp;</p><h2>Summary</h2><p>To summarise,&nbsp;</p><ul><li><p>In-context learning techniques like zero-shot and few-shot are inefficient regarding context windows and generating input-relevant completions.&nbsp;</p></li><li><p>Pretrained models alone cannot generate coherent text, and additional training is required.&nbsp;</p></li><li><p>Therefore, Instruction-based fine-tuning is performed on pre-trained models to improve the quality of generated text further.&nbsp;</p></li><li><p>The first step involves creating a prompt-based dataset by utilizing available/defining required prompt templates.</p></li><li><p>Supervised training of pre-trained models with the prompt dataset is performed using this backpropagation technique.&nbsp;</p></li><li><p>Instruction-based fine-tuning can be performed for single-task and multiple-task prompt datasets per requirements.&nbsp;</p></li></ul><p>Thank you for reading!</p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of advanced AI concepts! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Diving into LLM Training: Your Guide to Effective Pre-training]]></title><description><![CDATA[#5 Generative AI with LLMs: Exploring LLM Modelling with Pre-training.]]></description><link>https://neuraforge.substack.com/p/diving-into-llm-training-your-guide</link><guid isPermaLink="false">https://neuraforge.substack.com/p/diving-into-llm-training-your-guide</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Mon, 16 Oct 2023 14:46:11 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction </h2><p>Every LLM capable of generating text, understanding the language and performing various linguistic tasks accurately has undergone multiple stages of training on enormous amounts of text. This is because the deep learning model consists of learnable weights that are updated and tuned through training, making it more suitable and capable of performing many tasks at the human level. In this blog, let&#8217;s explore and understand more about an important technique used to train large language models called <strong>Pretraining</strong>. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="4898" height="3265" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3265,&quot;width&quot;:4898,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;text&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="text" title="text" srcset="https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1620969427101-7a2bb6d83273?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw0fHxuYXR1cmFsJTIwbGFuZ3VhZ2UlMjBwcm9jZXNzaW5nfGVufDB8fHx8MTY5NzM5OTg0MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@mrthetrain">Joshua Hoehne</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Making of LLMs: Pretraining technique</h2><p><strong>Pretraining</strong> is the process of training the LLMs by providing significant amounts (i.e. GBs and PBs) of unstructured data or text for the model to understand the underlying information and patterns of the text. Before pretraining, the deep learning model is initialized with random learnable weights or parameters. These weights are updated through the model training with large text datasets that result in these weights understanding and learning the essential patterns from the data. </p><p>Pretraining involves training the LLMs by providing the exact text as inputs and outputs. Still, outputs are right-shifted by one token, i.e. the input consists of <strong>n </strong>tokens, and the outputs will have <strong>n+1</strong> tokens, with the task of predicting the <strong>(n+1)th</strong> token in the output. This task is called <strong>next-word prediction</strong>, a type of <strong>Self-Supervised learning </strong>algorithm. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!i7NG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i7NG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 424w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 848w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 1272w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i7NG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic" width="1456" height="1053" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1053,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:235041,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i7NG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 424w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 848w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 1272w, https://substackcdn.com/image/fetch/$s_!i7NG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa493552b-acba-4cc8-aedf-dd8fac4cd2dd.heic 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">LLM Pretraining inputs and outputs. Source: https://docs.fast.ai/tutorial.text.html</figcaption></figure></div><p>The above image shows that the output text is the input with an additional token to be generated. </p><p>Self-supervised learning is a type of machine learning where the model learns to represent and understand the training data patterns without learning from explicitly labelled data. Since the input data also acts as the output or labelled data, pretraining can be classified as a self-supervised learning algorithm. This pretraining technique is the first foundational step for developing large language models.</p><p>Pretraining is usually performed as the initial training step of the LLM before finetuning it for a particular task. LLMs' pretraining varies for different models depending on the model's architecture. Let&#8217;s explore how pretraining is performed for encoder-only, decoder-only and encoder-decoder-only model types of the transformer model. </p><h2>Pretraining for Encoder-only Models</h2><p>Encoder-only models or <strong>Autoencoding models</strong> are pre-trained with the <strong>masked language modelling</strong> objective. Encoder-only models are best suited for tasks related to  feature extraction from input text, i.e. sentiment analysis, text classification and named entity recognition, to name a few. </p><p><strong>Masked Language Modelling</strong> learns to predict the masked token in the sequence by attending to both the left-side and right-side context of the masked token. This type of context is called a <strong>Bi-directional </strong>context. Some of the open-sourced pre-trained models are BERT, Roberta, etc. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!1vs8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1vs8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 424w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 848w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 1272w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1vs8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic" width="1134" height="968" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a9a7e304-c2b4-4177-921b-8e749a09d64e.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:968,&quot;width&quot;:1134,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29653,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!1vs8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 424w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 848w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 1272w, https://substackcdn.com/image/fetch/$s_!1vs8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9a7e304-c2b4-4177-921b-8e749a09d64e.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Masked Language Modelling with BERT</figcaption></figure></div><p>In the above image, the input fed to the BERT model consists of the mask token. The model trained on masked language modelling objectives predicts the token to fill the mask token.  </p><h2>Pretraining for Decoder-only Models </h2><p>Decoder-only models or <strong>Autoregressive models</strong> are pre-trained with the <strong>Casual Language modelling </strong>objective. Decoder-only models are best suited for language modelling tasks, i.e. text generation or next-word prediction. </p><p><strong>Casual Language Modelling</strong> learns to predict the next token by considering the left side context (or the context of the past tokens) only to predict the new token. This type of context is called a <strong>Uni-Directional context</strong>. This ensures that the model will learn to predict the next word by attending to the past tokens only. Some examples of pre-trained decoder-only models include the GPT series, Llama series and BLOOM. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!U2_L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U2_L!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 424w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 848w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 1272w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!U2_L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic" width="1088" height="738" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8c1964ec-c041-4386-8abb-540d4937abb6.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1088,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:49058,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!U2_L!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 424w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 848w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 1272w, https://substackcdn.com/image/fetch/$s_!U2_L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c1964ec-c041-4386-8abb-540d4937abb6.heic 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Casual Language Modelling with GPT2</figcaption></figure></div><p>In the above example, the GPT2 model generated the text using the next word prediction task for the input prompt provided. </p><h2>Pretraining for Encoder-Decoder Models</h2><p>The Encoder-decoder type models or sequence-to-sequence models usually perform tasks by taking in the input sequence and generating the output sequence. The objective of sequence-to-sequence models varies from the task and model being considered. </p><p>Some prominent use cases of sequence-to-sequence tasks are machine translation, summarization and question-answering. Commonly available models like T5 and BART are examples of the encoder-decoder types of models. </p><h2>Why is Pretraining essential?</h2><p>Usually, pretrained models are called foundation models as they consist of the basic information about the world, i.e. based on the dataset it is trained on. These models are very costly to train from scratch, i.e. regarding data, compute and total training time. Some examples of the training statistics of popular large language models available are:</p><ul><li><p><strong>GPT2</strong>: A 157M parameter decoder-only model trained on the BookCorpus dataset with over 7000 unpublished fiction books and a dataset of 8 million web pages with the casual language modelling objective. </p></li><li><p><strong>GPT4</strong>: A whooping 1.76 trillion parameter multi-modal large language model trained on 45 gigabytes of training data taking approximately 90-100 days for pretraining. </p></li><li><p><strong>BLOOM</strong>: A 178 billion parameter multi-lingual LLM trained on 350B tokens of 59 languages for 3.5 months. </p></li></ul><p>The above models are pretrained for vast amounts of data and time to enable further finetuning on specific tasks easier and faster through <strong>Transfer Learning</strong>. Performing finetuning on these open-source models will result in democratizing AI, as not all individuals and organizations will have the vast resources and data to perform pretraining from scratch. </p><h2>Summary</h2><p>To summarize, </p><ul><li><p>Pretraining is a technique in which LLMs are trained with millions of text data and files to learn and detect the patterns from the data.</p></li><li><p>Pretraining is the initial step performed in the LLM training process. Pretrained models are further finetuned with datasets related to specific tasks under consideration. </p></li><li><p>Pretraining is a type of self-supervised learning technique. </p></li><li><p>Masked Language modelling is the pretraining objective for encoder-based models.</p></li><li><p>Casual Language Modelling is the pretraining objective for decoder-based models. </p></li><li><p>Pretraining models are costly in terms of computing power, training data and training time required. </p></li><li><p>Performing finetuning on available pretrained models for our specific tasks and datasets via transfer learning will result in more accurate results than training the model from scratch. </p></li></ul><p>Thanks for reading!</p><div><hr></div><p></p><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Please subscribe to the newsletter for a deeper and more nuanced understanding of AI! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item><item><title><![CDATA[Prompt Engineering Demystified: Understanding LLM Inference ]]></title><description><![CDATA[#4 Generative AI with LLMs: Understanding LLM Inference - I]]></description><link>https://neuraforge.substack.com/p/prompt-engineering-demystified-understanding</link><guid isPermaLink="false">https://neuraforge.substack.com/p/prompt-engineering-demystified-understanding</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Thu, 12 Oct 2023 01:30:11 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>A large language model or LLM is a treasure trove and rich linguistic knowledge and information repository. Since these models are trained on petabytes of text data in different formats, these models possess a solid understanding of the patterns and the semantic details of the language it is pre-trained with, along with capabilities like multi-lingual understanding and contextual awareness. Therefore, specific techniques are developed to interact with the LLM so that we can extract the required information effectively. This process of engineering the input prompts to extract relevant and required answers is called Prompt Engineering. In this blog, let&#8217;s explore some of the best prompt engineering techniques. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="4076" height="2712" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2712,&quot;width&quot;:4076,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;person using MacBook Pro&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="person using MacBook Pro" title="person using MacBook Pro" srcset="https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHx0eXBlJTIwb24lMjBsYXB0b3B8ZW58MHx8fHwxNjk2OTM3MzY2fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@glenncarstenspeters">Glenn Carstens-Peters</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Prompt Engineering: Getting Started</h2><p><strong>Prompt Engineering</strong> is the process of designing input prompts and queries used to interact with Large language models like chatbots or domain-specific fine-tuned models. The main goal of prompt engineering is to elicit desired responses or behaviours from the LLM. Before we delve into different techniques of prompt engineering, let&#8217;s revisit the basic mechanism of LLM Inference. </p><p>Every model takes in a text called <strong>prompt</strong> as input. The LLM generates the required text called <strong>completion</strong> based on the input prompt. The length of the generated text or completion depends upon the context window of the LLM. <strong>The context window is the total number of tokens (or words) </strong>the model can process at a time. The context window of the model can be represented as follows,</p><blockquote><p>Context window of LLM = Length of input prompt + Length of completion </p></blockquote><p>Usually, the context window of LLMs is <strong>fixed</strong> up to 1000 or 2000 tokens, so we should define the input prompt effectively so that the LLM has enough context window to generate the required completion. </p><p>Let&#8217;s explore a few prompt engineering techniques for effective LLM inference.</p><h2>Prompt Engineering Techniques</h2><p>Some of the most widely used prompt engineering techniques are:</p><ul><li><p>Zero-Shot learning</p></li><li><p>In-Context Learning: One-shot and Few-shot learning. </p></li><li><p>Chain of thought prompting </p></li></ul><p>Let&#8217;s explore each of them in more detail. </p><h3>Zero-Shot Learning </h3><p><strong>Zero-shot learning</strong> or prompting is based on the principle that LLMs like GPT-3 and GPT-4 are finetuned to follow instructions and are usually trained on large amounts of data. Therefore, these LLMs can perform tasks &#8220;zero-shot&#8221; or tasks without additional fine-tuning. </p><p>For example, consider providing the following prompt to LLMs like GPT-3 or ChatGPT,</p><pre><code><strong>Prompt:</strong> 
Detect the sentence's sentiment as Positive, Negative or Neutral: 
I love the scenery of the place! 

Sentiment: 

<strong>Output:</strong> 
Sentiment: Positive</code></pre><p>We can see that the model correctly identifies the sentiment of the sentence and completes the prompt with the option <strong>Positive. </strong>This capability of the large language model to perform inference tasks without additional fine-tuning and just using the knowledge learnt during model training is called <strong>zero-shot  inference</strong>. Zero-shot inference best works for larger models, usually multi-billion parameter models like GPT-4, LLama, Falcon, etc. </p><p>But zero-shot inference is lacking in generating coherent completions when it comes to prompts or tasks that require more context, or it is a task not usually the LLM was trained upon directly. This is where we can use In-context learning techniques to customize the model generation by specifying additional context in the input prompt.  </p><h3>In-Context Learning</h3><p>In-context learning refers to learning from the context provided in the input prompt. One-shot learning and Few-shot learning are some techniques under In-context learning. </p><p>One-shot Learning or prompting enables the LLM to perform better by providing additional context in the input prompt. This is done by designing the prompt by including one example of the complex task that needs to be performed.  Consider the following example of performing One-shot inference,</p><pre><code><strong>Prompt:</strong>
Classify this review: I loved the cast and the story !
Sentiment: Positive 

Classify this review: I hated the performance of villian !
Sentiment:

<strong>Output:</strong>
Sentiment: Negative</code></pre><p>In the above example, we have provided an example of the prompt as well as the completion of a sentiment analysis task. Then, we have provided another example for the model to perform inference. By observing the context and structure of output provided in the initial prompt, the model generates completion for the later prompt in the required format. </p><p>This prompting method can be further improved by including multiple examples of tasks in the context. This prompting method is called <strong>Few-shot learning</strong>. Few-shot learning includes multiple examples of that complex task to be performed in the context of the prompt. Then, the model will generate the output based on the patterns and observations from the input prompt&#8217;s context. </p><p>Let&#8217;s explore another interesting prompting technique to improve text generation quality.</p><h3>Chain-of-Thought Prompting </h3><p>Chain-of-thought prompting technique enables complex reasoning capabilities in LLMs by providing intermediate reasoning steps in the context of the input prompt. This can be further improved by combining the few-shot prompting technique to get better results on a highly complex task involving much reasoning before responding. </p><p>To better illustrate the capabilities of Chain-of-thought prompting, consider the following example, </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!bCMT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!bCMT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 424w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 848w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 1272w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!bCMT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png" width="1456" height="673" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:673,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271966,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!bCMT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 424w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 848w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 1272w, https://substackcdn.com/image/fetch/$s_!bCMT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66b20765-42bf-4162-828e-3a9b0531ba1f_2082x962.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://arxiv.org/abs/2201.11903</figcaption></figure></div><p>We can observe that according to the above example, the model was able to answer the question better when additional reasoning was provided with the correct answer. This prompting technique works best when coupled with few-shot learning. </p><h2>Beyond Prompt Engineering?</h2><p>Using the above prompt engineering techniques, LLMs can generate coherent and accurate completions for a wide variety of tasks. But sometimes, LLMs tend to hallucinate while generating completion for a prompt or might not produce completions of good quality even after providing multiple examples in few-shot prompting.  </p><p>In this scenario, task-based fine-tuning of large language models is the best way to handle uncertainty in text generation. The pre-trained model is fine-tuned in this approach with a prompt-based task dataset created for the specific task. This will help the model adapt to the prompt structure easily compared to few-shot learning. </p><h2>Conclusion</h2><p>Large language models with billions of parameters trained with petabytes of data will have excellent natural language processing and understanding capabilities. These LLMs will also have a large memory to process and generate text related to any topic. However, this indefinite potential of knowledge and information possessed by LLMs can be utilized by effective querying techniques called prompt engineering. As and when the sizes of LLMs increase, it is necessary for us to use these prompt engineering techniques to query better and mine the knowledge from LLMs effectively.</p><h2>Summary</h2><p>To summarise, </p><ul><li><p>Large language models with multiple billions of parameters consist of rich representation and understanding of knowledge and information. </p></li><li><p>Prompt Engineering is the process of designing prompts to input to LLMs to extract required and meaningful completions. </p></li><li><p>Some popular prompt engineering techniques are Zero-shot prompting, In-context learning, and Chain-of-thought prompting.</p></li><li><p>Zero-shot prompting includes the task within the input prompt to be performed, i.e. for the model to generate completion. </p></li><li><p>In-context learning includes defining the context of the problem to be resolved in the input prompt. This can be done by providing one (one-shot learning) or more examples (few-shot learning) in the input prompt. </p></li><li><p>Chain-of-thought learning improves the completion quality by including the intermediate reasoning steps and the examples in the input prompt. This improves the model's reasoning capability and helps generate more coherent completion.</p></li><li><p>If the quality of the completion generated by the model doesn&#8217;t improve even after providing additional examples, i.e.few-shot prompting, model fine-tuning must be performed. </p></li></ul><p>Thanks for reading! </p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Step into a world of discovery with our technical newsletter. Delve deep into applied Machine Learning, Generative AI and advanced Deep Learning concepts as we unravel fundamental concepts and unveil the latest research trends. Embark on this exhilarating journey of learning and growth with us by subscribing to the newsletter! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><p></p><p></p><p></p>]]></content:encoded></item><item><title><![CDATA[Attention Is All You Need: Understanding the Transformer's Attention Mechanism]]></title><description><![CDATA[#3 Generative AI with LLMs: Understanding the Transformer Architecture - II]]></description><link>https://neuraforge.substack.com/p/attention-is-all-you-need-understanding</link><guid isPermaLink="false">https://neuraforge.substack.com/p/attention-is-all-you-need-understanding</guid><dc:creator><![CDATA[Narasimha Karthik J]]></dc:creator><pubDate>Sun, 08 Oct 2023 23:30:20 GMT</pubDate><enclosure url="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" length="0" type="image/jpeg"/><content:encoded><![CDATA[<h2>Introduction</h2><p>The Transformer architecture has revolutionized the landscape of natural language processing in different applications like machine translations, summarization or chatbots. This is primarily due to the power of the attention mechanism of the Transformer. In this blog, let&#8217;s understand more about the working of the attention mechanism and how it played a crucial role in Transformer&#8217;s success. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6115" height="4375" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4375,&quot;width&quot;:6115,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;a person holding a baseball bat&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="a person holding a baseball bat" title="a person holding a baseball bat" srcset="https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1641531105535-1ead3c1784ab?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw4fHxjb25jZW50cmF0aW9ufGVufDB8fHx8MTY5NjcwMTI2M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@medion4you">Norbert Braun</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div><h2>Components of the Transformer</h2><p>The transformer architecture comprises two main parts: the Encoder and the Decoder. The Transformer's architecture in the figure below has Encoder and Decoder blocks connected to perform sequence-to-sequence tasks like machine translation. Later research shows these blocks can be used as stand-alone models for classification or text generation tasks. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Q-qk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 424w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 848w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1272w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png" width="1000" height="1217" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1217,&quot;width&quot;:1000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:205249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Q-qk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 424w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 848w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1272w, https://substackcdn.com/image/fetch/$s_!Q-qk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d0e7243-cdb1-44e8-8889-553a872decfa_1000x1217.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png</figcaption></figure></div><h6> </h6><p>The transformer model architecture comprises the following components: </p><ol><li><p>Model Input/Output - Tokenization</p></li><li><p>Embeddings</p></li><li><p>Positional Encoding</p></li><li><p>Multi-headed Attention for the Encoder and masked Multi-Head Attention for the Decoder.</p></li><li><p>Layer addition and Normalization blocks. </p></li><li><p>Feedforward neural networks </p></li><li><p>Linear and Softmax layers</p></li></ol><p>Let&#8217;s have a brief overview of each of the components of the transformer architecture.</p><h3>Tokenization</h3><p>Models don't accept raw text as inputs. The raw input is broken down into smaller words or tokens using world-based or sub-word tokenization techniques. After Tokenization, the <strong>vocabulary</strong> is created with a list of all unique tokens from the input. </p><pre><code># Example of tokenization
input_text = "Tokenization converts text into tokens"
print(tokenizer.tokenize(input_text))

<strong>Out:</strong> ['token', '##ization', 'converts', 'text', 'into', 'token', '##s']</code></pre><p>We can see in the above code block example that the entire sentence is broken down into sub-words. The tokenizer used above is called the <strong>Sub-Word tokenizer, </strong>which splits the input sentence into possible smaller subwords or tokens. </p><p>After a vocabulary is created by combining all possible tokens, these tokens are encoded (or assigned) with a unique number for every token. This encoded output is fed as input to the model. </p><pre><code># Encode the text to get list of numbers
input_text = "Tokenization converts text into tokens"
print(tokenizer.encode(input_text))

<strong>Out:</strong> [101, 19204, 3989, 19884, 3793, 2046, 19204, 2015, 102]</code></pre><p>The input text is now broken down into tokens and then converted into numbers, ready to be fed into the model. This end-to-end process is called <strong>Tokenization</strong>. </p><p>Since the Encoder block has bi-directional attention, the entire input is fed into the model. But the Decoder block has casual or autoregressive attention, i.e. only the inputs before the target word is considered. </p><h3>Token Embedding Layer</h3><p>Further, these tokens are matched into a high-dimensional vector space called embeddings. <strong>Embeddings</strong> are high-order representations of every word or token in vector space. This will capture meaningful representations and relationships between words learnt during the model training. </p><p>The transformer model will convert the tokenized input tokens to embeddings. This ensures that knowledge representation about the input word is captured. </p><h3>Positional Encoding Layer</h3><p>Token embeddings are just higher-order representations of input words. But this doesn&#8217;t capture the order of words in the input sentence. Therefore, <strong>positional embeddings</strong> are added to the existing word embeddings. In this way, the model can preserve the word order information. </p><p>Positional embeddings are embedding representations of the index of the word as they occur in the input sequence. Therefore, when added with the word embeddings, the model will learn the order in which the word has happened in the sequence. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!cUxD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cUxD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png" width="1372" height="966" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:966,&quot;width&quot;:1372,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:42279,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cUxD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!cUxD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ea59245-2285-41ac-b618-364fde4f6b15_1372x966.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png</figcaption></figure></div><h6></h6><h3>Understanding Self Attention</h3><p>The token embedding (word embeddings) and the positional embeddings are passed to the attention layers of the Encoder/Decoder. </p><p>The attention layer will compute the self-attention weights for every word of the input to identify the relationship between the words. This is done by computing an attention map that will be learnt by the model during the training process and will define the context and dependence of every word with respect to other words in the input. </p><p>Here is an example of visualization of the self-attention mechanism in transformers created using the <strong><a href="https://github.com/jessevig/bertviz">Bertviz</a> </strong>library. </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!AYRs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AYRs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 424w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 848w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1272w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png" width="812" height="778" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:778,&quot;width&quot;:812,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108267,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AYRs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 424w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 848w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1272w, https://substackcdn.com/image/fetch/$s_!AYRs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12be73d0-1ac2-4ac6-9719-d71f3186eff7_812x778.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Example of the Self-Attention Mechanism </figcaption></figure></div><p>In the above example, the highlighted words on the left indicate the values of attention given to a corresponding word on the right. Therefore, the model can identify the reference of the word <strong>it</strong> in the input sequence and map it to the word <strong>monkey. </strong></p><p>This ability to provide attention to the words and their relationship with other words helps the model to capture the context and consider the long-term dependencies. </p><h3>Multi-Headed Attention &amp; Masked Multi-Headed Attention</h3><p>In the above example, we explored how one layer of attention is used to identify the relationship between a pronoun (i.e. <strong>it</strong>) and the subject (i.e. <strong>monkey</strong>) in the same sentence. This single layer of attention learning the pronoun-subject relationship is called an <strong>Attention head</strong>. </p><p>In the transformer architecture, there are usually multiple attention heads, i.e., 12-100 heads, present to learn and understand simultaneously different aspects and patterns of the language.  These attention heads will specialize in various tasks learned during the model training. This is known as <strong>Multi-Headed Self Attention</strong>.</p><p>The Encoder block of the transformer consists of the Multi-Headed Self-attention after the positional encoding layer to learn different features, aspects, patterns of the input sequence, and various dependencies of words within the text. Since the Encoder considers the entire input sequence, i.e. both the left and right side context of a word in the input sequence, to extract features, this type of attention is called <strong>Bi-Directional attention</strong>. </p><p>Since the Decoder block mainly performs autoregressive tasks like text generation, i.e. next-word prediction by considering the past input tokens, the entire context of the input must not be considered for training, and the right side context of the word must be hidden from the attention layers. </p><p>Therefore, the right side context of the input is masked during the training of the Multi-Head self-attention layers by using the mask token. This will prevent information about the future from leaking into the past. This type of attention is called <strong>Autogregressive Attention,</strong> and the <strong>Masked Multi-Headed Self Attention</strong> <strong>Layers</strong> are used for training the Decoder of the transformer model. </p><h3>Layer Addition and Normalization Layers</h3><p>The <strong>Layer Addition or Residual connections</strong> add the gradient output from the attention layers and the input to the attention layer before passing them to feedforward networks. This ensures that information is not lost during training and that the vanishing gradients problem is addressed. </p><p>The <strong>Normalization</strong> layers are introduced to reduce unnecessary variations in the hidden vectors. This is done by normalizing every layer by scaling the inputs by setting unit mean and standard deviation. This also helps in speeding up the training process significantly. </p><h3>Feed Forward Neural Networks</h3><p>After processing through the Residual and Normalization layers, the attention layer&#8217;s outputs are passed to the fully connected feedforward layers for further post-processing of the output vector. This is primarily used to introduce the non-linearities in the model architecture and enable the deep learning magic to identify hidden patterns from the input data. </p><p>Residual and Normalization operations are again applied to the outputs of the Feedforward layers before passing it through the linear and the softmax layers.</p><h3>Linear layers with Softmax activation</h3><p>Finally, the outputs are passed through the linear layers to obtain the logits for every token in the vocabulary. Further, these logits are passed through the softmax activation to convert and obtain the probabilities and then choose the final predicted word based on the highest probability. </p><h2>Encoder-Only Model Architecture </h2><p>The architecture of the Encoder-only model includes,</p><ul><li><p>Tokenization of entire inputs</p></li><li><p>Positional Encoding layer = Token embeddings + positional embeddings</p></li><li><p>Bi-directional or multi-headed attention layer</p></li><li><p>Addition and Normalization layer </p></li><li><p>Feed Forward Neural network </p></li><li><p>Linear and Softmax layers </p></li></ul><p>This model type is best suited for extracting features and patterns from the input text and performing tasks like keyword extraction, named entity recognition and sentiment analysis. Examples of Encoder-only models are BERT, RoBERTa, etc.</p><h2>Decoder-Only Model Architecture</h2><p>The architecture of the Decoder-only model includes,</p><ul><li><p>Tokenization of the right-shifted inputs </p></li><li><p>Positional Encoding layer = Token embeddings + positional embeddings</p></li><li><p>Autoregressive or Masked-headed attention</p></li><li><p>Addition and Normalization layer</p></li><li><p>Feed Forward Neural network </p></li><li><p>Linear and Softmax layers </p></li></ul><p>The Decoder model type is best suited for generating text, i.e. next word prediction task. Examples of Decoder-only models are GPT2, LLama and Falcon.</p><h2>Encoder-Decoder based Model Architecture</h2><p>The architecture of the Encoder-Decoder based model includes both the Encoder and the Decoder layers, as mentioned above. However, the outputs of the Encoder are fed into the second multi-headed attention block of the Decoder along with the past results of the multi-headed attention block, as per the architecture of transformers shown in the figure - I. </p><p>Encoder-Decoder model architecture best suits sequence-to-sequence tasks like answering questions and machine translation. Examples of encoder-decoder models are T5 and BART. </p><h2>Summary</h2><p>We have a brief overview of the architecture and building blocks of the transformer model. To summarise,</p><ul><li><p>Transformer architecture consists of two blocks: the Encoder and the Decoder blocks.</p></li><li><p>Converting the raw input text into tokens is known as tokenization. </p></li><li><p>Tokens are converted to high dimensional vector space called Embeddings, which better represent the inputs and relationships between the tokens.</p></li><li><p>Token embeddings are passed through the positional encoding layer to encode the positional information with token embeddings. </p></li><li><p>Multi-headed attention computes attention weights to learn different aspects of the language and the dependence of words with every word.</p></li><li><p>Multi-headed attention is present in the Encoder, whereas masked multi-headed attention is used in the Decoder.</p></li><li><p>Residual connections and layer normalization are performed to retain important information and speed up the training. </p></li><li><p>Output logits and probabilities are computed by passing through the linear layers and the softmax activation. Then, the final token is chosen based on the highest probability score. </p></li></ul><p>Thanks for reading! </p><div><hr></div><div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Step into a world of discovery with our technical newsletter. Delve deep into applied Machine Learning, Generative AI and advanced Deep Learning concepts as we unravel fundamental concepts and unveil the latest research trends. Embark on this exhilarating journey of learning and growth with us by subscribing to the newsletter! &#128640;&#129302;</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"><input type="submit" class="button primary" value="Subscribe"><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content:encoded></item></channel></rss>