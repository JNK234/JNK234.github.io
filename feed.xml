<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jnk234.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jnk234.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-07T05:41:42+00:00</updated><id>https://jnk234.github.io/feed.xml</id><title type="html">blank</title><subtitle>MS in AI at Northwestern University | Ex-Boeing Data Scientist | Building intelligent systems with focus on LLMs, NLP, and Reinforcement Learning. </subtitle><entry><title type="html">Reinforcement Learning Essentials: MDPs &amp;amp; Optimal Control</title><link href="https://jnk234.github.io/blog/2025/reinforcement-learning-essentials-mdps-optimal-control/" rel="alternate" type="text/html" title="Reinforcement Learning Essentials: MDPs &amp;amp; Optimal Control"/><published>2025-08-09T13:01:47+00:00</published><updated>2025-08-09T13:01:47+00:00</updated><id>https://jnk234.github.io/blog/2025/reinforcement-learning-essentials-mdps--optimal-control</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/reinforcement-learning-essentials-mdps-optimal-control/"><![CDATA[<p>Reinforcement Learning (RL) stands as a powerful paradigm for creating intelligent agents capable of autonomous decision-making in complex, dynamic environments. This comprehensive guide delves into the foundational concepts of RL, starting with the essential agent-environment interaction and progressing to the mathematical rigor of Markov Decision Processes (MDPs). We will explore how agents define strategies through policies and quantify future rewards with value functions, culminating in a detailed look at classic control algorithms like Policy and Value Iteration that drive optimal behavior. </p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" width="5434" height="3623" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3623,&quot;width&quot;:5434,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;silhouette of road signage during golden hour&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="silhouette of road signage during golden hour" title="silhouette of road signage during golden hour" srcset="https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1533073526757-2c8ca1df9f1c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxkaXJlY3Rpb25zfGVufDB8fHx8MTc1NDcxOTk5Nnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="https://unsplash.com/@soymeraki">Javier Allegue Barros</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h3>The Agent-Environment Interaction Loop</h3> <p>At the heart of any Reinforcement Learning system lies a fundamental, continuous interaction loop between two primary entities: the <strong>Agent</strong> and its <strong>Environment</strong>. The agent serves as the learner and decision-maker, actively selecting and executing an action at each step. This chosen action directly influences the environment, causing it to transition from its current configuration to a new state. Crucially, in response to the agent's action and the resulting state change, the environment provides vital feedback back to the agent, thereby completing the cycle. This iterative process of observation, action, and feedback is the bedrock upon which the agent's learning process is built.</p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div> <h3>Understanding States, Actions, and Rewards</h3> <p>The feedback provided by the environment to the agent manifests primarily in two critical forms: <strong>States</strong> and <strong>Rewards</strong>.</p> <ul><li><p>A <strong>State</strong> precisely describes the current configuration or condition of the environment. It provides the agent with all the necessary contextual information to make an informed decision at that particular moment. The state encapsulates the relevant aspects of the environment that the agent perceives.</p></li><li><p>An <strong>Action</strong> represents the specific operation, decision, or control signal that the agent performs within a given state. These actions are the agent's means of influencing the environment and progressing through its task.</p></li><li><p>Following an action, the environment issues a <strong>Reward</strong>. This is a scalar numerical value that quantifies the immediate desirability or undesirability of the agent's action and the resulting transition to a new state. A positive reward indicates a favorable outcome, while a negative reward (often termed a penalty) signifies an unfavorable one.</p></li></ul> <p>The overarching goal of the agent is not merely to maximize immediate rewards, but rather to maximize the <em>cumulative reward</em> over the long term. Through this iterative cycle of observing states, performing actions, and receiving rewards, the agent progressively learns which sequences of actions lead to the most favorable long-term outcomes, thereby discovering an optimal policy for navigating its environment.</p> <h2>II. From Multi-Arm Bandits to Contextual Challenges</h2> <p><strong>Sequential decision-making under uncertainty</strong> is a fundamental aspect of Reinforcement Learning. The Multi-Arm Bandit (MAB) problem serves as an accessible entry point to this domain, illustrating core concepts before progressing to more complex frameworks that address more intricate challenges.</p> <h3>Introduction to Multi-Arm Bandit (MAB) problems</h3> <p>The Multi-Arm Bandit (MAB) problem models a decision-maker faced with a finite set of choices, often referred to as "arms," each yielding a reward drawn from an unknown probability distribution. Analogous to a gambler selecting from multiple slot machines, where each machine (arm) has a distinct, unknown payout probability, the objective is to maximize the total cumulative rewards over a series of independent decisions.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!43pB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!43pB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 424w, https://substackcdn.com/image/fetch/$s_!43pB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 848w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg" width="280" height="280" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:280,&quot;width&quot;:280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:16991,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!43pB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 424w, https://substackcdn.com/image/fetch/$s_!43pB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 848w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!43pB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc89d78c2-c533-47c6-bc0b-754a43472e62_280x280.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Slot Machine or Bandir Machines</figcaption></figure></div> <p>The central challenge in MAB is the <strong>exploration-exploitation trade-off</strong>. Exploitation involves consistently choosing the arm that has historically provided the highest average reward, aiming for immediate gains based on current knowledge. Conversely, exploration entails trying less-chosen or untried arms to gather more information about their true reward distributions, with the potential of discovering a more lucrative long-term payout. This dilemma is inherent in scenarios where optimizing immediate reward competes with acquiring knowledge for future benefit. A key characteristic of the MAB problem is the <strong>assumption of a single, independent decision at each time step,</strong> where the <strong>environment does not change based on past actions</strong>, and there is no sequence of interconnected states. The reward received from pulling an arm does not influence the availability or characteristics of other arms or future states.</p> <h3>Understanding Contextual Multi-Arm Bandit (C-MAB)</h3> <p>Building upon the MAB framework, the Contextual Multi-Arm Bandit (C-MAB) problem introduces "context" or "state information" into the decision process. Unlike traditional MABs where rewards are independent of external factors, in a C-MAB, the optimal action (arm selection) is contingent on the current situation or observable features of the environment. For instance, when recommending an article to a user, the most suitable article (arm) may vary based on the user's browsing history, demographics, or the current time of day&#8212;this constitutes the context. The agent observes this context <em>before</em> making a decision at each step.</p> <p>This integration of state information renders the problem more realistic and powerful, as decisions are no longer isolated but are informed by the environment's current state. The objective in a C-MAB is to <strong>learn a policy that maps observed contexts to optimal actions</strong>, thereby maximizing cumulative reward. This fundamental difference means that while MAB assumes a static optimal arm (or set of arms) regardless of the situation, C-MAB acknowledges that the best action depends dynamically on the current context.</p> <h3>Limitations of Bandits for sequential decision-making</h3> <p>While MAB and C-MAB problems are valuable for understanding the exploration-exploitation trade-off and the role of context in decision-making, they present significant simplifications regarding temporal dynamics. They primarily focus on single-step or short-horizon decisions where the immediate reward is the primary concern. Bandit frameworks are insufficient for scenarios where an agent's actions have long-term consequences, and the environment dynamically evolves over time as a result of those actions.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!TZf6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!TZf6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 424w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 848w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1272w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png" width="1344" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1344,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34429,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!TZf6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 424w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 848w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1272w, https://substackcdn.com/image/fetch/$s_!TZf6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F821b8dfa-e000-40b9-8489-09f0de073f29_1344x726.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://images.app.goo.gl/AaAu4kvQt4pqDvSk8</figcaption></figure></div> <p>Bandit problems have certain limitations in their modeling approach:</p> <ul><li><p>Lack of Influence on Future States: Actions taken in the present do not impact future states or decision opportunities.</p></li><li><p>No State Transition or Delayed Reward: Traditional bandit formulations do not incorporate the concepts of state transitions or delayed rewards.</p></li><li><p>Static Environment: The environment&#8217;s state remains unchanged regardless of the agent&#8217;s actions, and future rewards are not influenced by past actions.</p></li></ul> <p>For scenarios that demand foresight, planning across interconnected states, and evaluating the long-term effects of current decisions, a more robust mathematical framework is necessary. This is where full Reinforcement Learning comes into play, offering comprehensive tools to address these complexities.</p> <h2>III. Markov Decision Processes (MDPs): The Foundation for Sequential Decisions</h2> <p>Markov Decision Processes (MDPs) provide a formal mathematical framework for modeling sequential decision-making problems. An MDP is typically characterized by a tuple (S, A, P, R, &#947;, H) , where each element plays a critical role in defining the problem. </p> <p>The Discount Factor (&#947;) is a key parameter in reinforcement learning that quantifies the present value of future rewards. It has a value ranging from 0 to 1 (inclusive) and plays a crucial role in balancing immediate gratification against long-term objectives.</p> <ul><li><p>High &#947; (closer to 1): Emphasizes long-term rewards, encouraging the agent to consider future consequences more heavily.</p></li><li><p>Low &#947; (closer to 0): Makes the agent more "myopic," prioritizing immediate rewards over future ones.</p></li></ul> <h3>Defining an MDP: States (S), Actions (A), Reward function (R), Transition matrix (P)</h3> <p>The fundamental components of an MDP define the environment and the agent's interaction within it:</p> <ul><li><p><strong>States (S)</strong> represent all possible configurations or observable conditions of the environment at any given time. While often considered finite for theoretical simplicity and tractability, state spaces can be countably infinite or even continuous in practical applications. In such cases, techniques like function approximation become essential to manage the immense or infinite number of states.</p></li><li><p><strong>Actions (A)</strong> constitute the set of choices available to the agent when in a particular state. These actions are the agent's means of influencing the environment's evolution, leading to new states and potentially new rewards.</p></li><li><p>The <strong>Transition Probability (P)</strong> defines the dynamics of the environment, capturing its inherent uncertainty. Specifically, <em><strong>P(s&#8217;|s, a)</strong></em> denotes the probability of transitioning to a next state s&#8217; from the current state s after the agent takes action a. This probabilistic nature is fundamental to modeling real-world environments where outcomes are not always deterministic.</p></li><li><p>The <strong>Reward Function (R)</strong> provides immediate scalar feedback to the agent, indicating the desirability of a specific state-action-next-state transition. <em><strong>R(s,a,s&#8217;) </strong></em>assigns a numerical value representing the immediate reward received by the agent for taking action a in state s and subsequently transitioning to state s&#8217;. While rewards can also be stochastic, the primary source of uncertainty in MDPs typically stems from the probabilistic state transitions.</p></li></ul> <h3>The concept of Horizon (H) in MDPs</h3> <p>The <strong>Horizon (H)</strong> specifies the total number of steps or time stages in an episode of the decision-making process. The horizon can be <strong>finite</strong>, as seen in games with a fixed number of turns, or infinite, which is common in continuous control tasks where the process continues indefinitely until a terminal condition is met or a steady state is achieved.</p> <h3>Understanding the Markov Property</h3> <p>A critical characteristic that underpins the power and analytical tractability of MDPs is the <strong>Markov Property</strong>. This property asserts that the <strong>future state and the reward depend </strong><em><strong>only</strong></em><strong> on the current state and the action taken</strong>, and are <strong>conditionally independent of all previous states and actions</strong>. Formally, this can be expressed as:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;P(S_{t+1}|S_t, A_t, S_{t-1}, A_{t-1}, \\dots) = P(S_{t+1}|S_t, A_t)&quot;,&quot;id&quot;:&quot;VMKOMEFNQF&quot;}" data-component-name="LatexBlockToDOM"></div> <p>This simplification is profoundly significant because it allows the agent to make optimal decisions based solely on the current state, without needing to recall or process the entire history of interactions. It implies that the current state encapsulates all necessary information from the past to predict the future. For many well-defined MDPs, the optimal policy itself is also Markov, meaning the optimal action in any state depends only on that state. However, it's important to note that for more complex scenarios, such as Partially Observable Markov Decision Processes (POMDPs) where the current state is not fully observable, or Multi-Agent Reinforcement Learning (MARL) where interactions are complex, policies may need to consider historical observations to infer a more complete understanding of the environment.</p> <p>This formalization of states, actions, transitions, and rewards, rigorously underpinned by the Markov Property, provides a robust mathematical foundation for understanding how agents learn to navigate and optimize their behavior in dynamic and uncertain environments. It enables the development of algorithms that can effectively derive optimal policies for sequential decision-making.</p> <h2>IV. Key Concepts: Environment Settings, Policy, and Value Functions</h2> <p>Reinforcement Learning (RL) agents operate within dynamic environments, learning optimal strategies through interaction. Understanding the fundamental concepts of these environments, the agent's strategy (policy), and how future rewards are estimated (value functions) is crucial for grasping how RL systems function. This section delves into these core components, including the critical role of the discount factor.</p> <h3>Different Environment Settings: Known (Planning), Unknown (Simulator, Online, Offline)</h3> <p>In Reinforcement Learning, the <strong>environment</strong> is everything an agent interacts with, providing states, rewards, and determining the consequences of actions. The nature of this environment significantly influences the learning approach. Environments can broadly be categorized based on whether the agent has a complete model of the world or not.</p> <p>When an agent operates in a <strong>known environment</strong>, it possesses a complete model of the environment's dynamics&#8212;meaning it knows exactly how actions will transition it between states and what rewards will be received. This allows for <strong>planning</strong>, where the agent can compute optimal strategies offline without direct interaction, often leveraging techniques like dynamic programming.</p> <p>Conversely, in <strong>unknown environments</strong>, the agent does not have a complete model of the world. Learning in such settings requires the agent to interact with the environment to gather information. These unknown environments can manifest in several forms:</p> <ul><li><p><strong>Simulator environments</strong> provide a digital replica of a real-world system. While the underlying model might be complex or unknown to the agent, the simulator allows for repeated, safe, and often accelerated interaction to collect data. The agent learns from this simulated experience, which can then be transferred or adapted to the real world.</p></li><li><p><strong>Online environments</strong> represent direct interaction with the real world. The agent performs actions and receives immediate feedback (new states and rewards) in real-time. Learning in an online setting is continuous and directly influenced by live experience, often requiring careful exploration strategies to discover optimal behaviors without causing significant negative consequences.</p></li><li><p><strong>Offline environments</strong> involve learning from a fixed dataset of previously collected interactions without any further interaction with the environment itself. The agent analyzes historical data to derive a policy. This approach is valuable when real-world interaction is costly, dangerous, or time-consuming, but it presents challenges in terms of data coverage and avoiding extrapolation beyond the observed data.</p></li></ul> <h3>Policy (&#960;): The Agent's Strategy </h3> <p>A <strong>policy</strong> (&#960;) defines the agent's behavior, acting as its strategy for choosing actions in any given state. It is the core of an RL agent, mapping observed states to actions. The ultimate goal of many RL algorithms is to discover an optimal policy that maximizes the agent's cumulative reward over time. Policies can be characterized in several ways:</p> <ul><li><p>A <strong>deterministic policy</strong> dictates a single, specific action for each state. If the agent is in a particular state, a deterministic policy will always prescribe the exact same action to be taken.</p></li><li><p>In contrast, a <strong>stochastic policy</strong> outputs a probability distribution over possible actions for each state. This means that for a given state, there might be multiple actions the agent could take, each with a certain probability. Stochastic policies are often useful for exploration, allowing the agent to try different actions and discover better strategies, especially in environments with inherent uncertainty.</p></li><li><p>A <strong>Markov policy</strong> (or memoryless policy) depends solely on the current state. It assumes that the current state provides all necessary information to make an optimal decision, satisfying the Markov property where future states depend only on the current state and action, not the entire history. Most standard RL algorithms assume a Markov policy.</p></li><li><p>A <strong>general policy</strong>, on the other hand, might depend on the entire history of states and actions encountered up to the current moment. While more complex, such policies can be necessary in environments where the Markov property does not hold, and past observations provide crucial context for future decisions.</p></li></ul> <h3>Value Function: Quantifying Future Rewards </h3> <p>Value functions are central to Reinforcement Learning, serving to estimate the "goodness" of states or state-action pairs in terms of future rewards. They provide a quantitative measure of how much cumulative reward an agent can expect to receive from a given point forward, following a specific policy.</p> <p>The foundation of value functions is the <strong>Return (Gt)</strong>, which represents the total discounted sum of future rewards from a particular time step t. It aggregates all rewards obtained from t onwards, with future rewards being progressively discounted.</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{H-1} r_{t+H-1}&quot;,&quot;id&quot;:&quot;HYTMFCXBKW&quot;}" data-component-name="LatexBlockToDOM"></div> <p>Building upon the concept of return, two primary types of value functions are used:</p> <ul><li><p>The <strong>state-value function (V(s))</strong>, often denoted as the V-value, provides an estimate of the expected return an agent can anticipate if it starts in a particular state s and then follows a given policy &#960; thereafter. In essence, V(s) tells us <strong>"how good it is to be in state s"</strong> under a specific strategy.</p></li></ul> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V^\\pi_h(s) = E^\\pi \\left[ \\sum_{h'=h}^H r_{h'}(S_{h'}, A_{h'}) \\mid S_h=s \\right]&quot;,&quot;id&quot;:&quot;POHMMDIDGU&quot;}" data-component-name="LatexBlockToDOM"></div> <ul><li><p>The <strong>state-action value function (Q(s,a))</strong>, known as the Q-value, estimates the expected return if the agent takes a specific action a in a particular state s, and then follows policy &#960; for all subsequent actions. </p></li></ul> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q^\\pi_h(s,a) = E^\\pi \\left[ \\sum_{h'=h}^H r_{h'}(S_{h'}, A_{h'}) \\mid S_h=s, A_h=a \\right]&quot;,&quot;id&quot;:&quot;UGHWGIJJXV&quot;}" data-component-name="LatexBlockToDOM"></div> <p>The Q-value answers the question, "how good it is to take action a in state s ?" This function is particularly useful for decision-making, as an agent can choose the action with the highest Q-value in any given state to maximize its expected future reward.</p> <p>Value functions are critical because they allow RL algorithms to evaluate different policies and learn which actions lead to long-term success, even if immediate rewards are small or negative.</p> <h2>V. The Objective of Reinforcement Learning: Finding Optimal Policies</h2> <p>With the fundamental concepts of environments, policies, and value functions established, the central objective of Reinforcement Learning becomes clear and actionable. The ultimate goal is to discover an <strong>optimal policy (&#960;*)</strong> that strategically directs the agent's actions, ensuring it maximizes its expected cumulative reward over the long term within a given environment.</p> <h3>Defining the Optimal Policy (&#960;*)</h3> <p>An optimal policy, denoted as &#960;*, signifies the best possible strategy an agent can employ. This policy dictates action choices that consistently lead to the highest attainable expected cumulative reward. Formally, this objective often translates to maximizing the value of the initial state, represented as V, assuming a fixed starting state . </p> <p>In more generalized scenarios where the initial state is not predetermined, the objective expands to maximizing the expected value across a distribution of potential initial states, . For many theoretical analyses and practical applications, a fixed initial state assumption simplifies the problem without loss of generality for the core concepts.</p> <h3>The Bellman Optimality Equations: Characterizing Optimal Value Functions</h3> <p>Identifying an optimal policy necessitates a precise method to characterize what 'optimal' truly means in terms of value. This is precisely where the <strong>Bellman Optimality Equations</strong> become indispensable. These equations provide a recursive relationship that optimal value functions must inherently satisfy, serving as the fundamental theoretical bedrock for finding optimal policies. They rigorously define the value of a state, or a state-action pair, under an optimal policy as the maximum possible expected return. This maximum is achieved by considering the immediate reward from an action and the optimal values of all possible successor states. </p> <h2>Policy Iteration for Optimal Policies</h2> <p>The objective of Reinforcement Learning is to discover an optimal policy&#8212;a strategy that maximizes long-term rewards. Dynamic programming provides powerful iterative methods to achieve this goal. Policy Iteration (PI) stands as a foundational algorithm for solving Markov Decision Processes (MDPs), systematically converging to an optimal policy for finite MDPs.</p> <p>Policy Iteration operates through a robust, cyclical process comprising two core phases: <strong>policy evaluation and policy improvement.</strong> The algorithm begins with any arbitrary policy, &#960;, and systematically refines it through these alternating steps until the optimal strategy is precisely identified. This iterative refinement is key to its power.</p> <h3>Step 1: Policy Evaluation</h3> <p>This initial phase focuses on understanding the current policy's effectiveness. In Policy Evaluation, the value function V_&#960;(s) for the <em>current</em> policy, &#960;, is accurately computed for every possible state s. This value quantifies the expected cumulative return an agent can anticipate from starting in state s and subsequently adhering strictly to policy &#960;. In essence, Policy Evaluation meticulously measures the "goodness" or long-term desirability of each state under the existing policy. For finite MDPs, this involves a thorough assessment to determine the precise state-value function for the given policy.</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V^\\pi_k(s) = \\sum_{a} \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^\\pi_{k-1}(s') \\right]&quot;,&quot;id&quot;:&quot;YTCBEKATPA&quot;}" data-component-name="LatexBlockToDOM"></div> <p></p> <h3>Step 2: Policy Improvement</h3> <p>With a complete understanding of the current policy's value function <em>V_&#960;(s)</em>, the Policy Improvement phase updates the policy itself. A <em>new</em> policy, <em>&#960;'</em>, is constructed to be "greedy" with respect to the recently evaluated value function. This is done by first computing the Q-value function for current policy &#960; which helps quantify the value of taking a specific a in state s and then following &#960;i:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q^{\\pi_i}(s,a) = R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^{\\pi_i}(s')&quot;,&quot;id&quot;:&quot;RJKPRUQCXM&quot;}" data-component-name="LatexBlockToDOM"></div> <p> The new policy <em>&#960;'(s)</em> is chosen by taking the argmax i.e. the action that maximizes this Q-value for each state s</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\pi_{i+1}(s) = \\text{argmax}_{a \\in A} Q^{\\pi_i}(s,a) \\quad \\forall s \\in S&quot;,&quot;id&quot;:&quot;ZAPEZBCBXT&quot;}" data-component-name="LatexBlockToDOM"></div> <p>This means that for every state <em>s</em>, the improved policy <em>&#960;'(s)</em> selects the action a that promises the highest expected return. This decision considers both the immediate reward gained by taking action <em>a</em> in state <em>s</em> and the anticipated value of the subsequent state <strong>s'</strong> according to the just-computed <em>V_&#960;(s')</em>. This strategic update guarantees that the revised policy is inherently better, or at least as good as, the previous one, leveraging the comprehensive evaluation from the prior step.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!P-7I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!P-7I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 424w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 848w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1272w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp" width="801" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:801,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:8976,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/170227495?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!P-7I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 424w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 848w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1272w, https://substackcdn.com/image/fetch/$s_!P-7I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90b5f7d6-cd0a-4b3f-80ca-cd14b1d79d97_801x400.webp 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: Policy Iteration - https://www.geeksforgeeks.org/data-science/what-is-the-difference-between-value-iteration-and-policy-iteration/</figcaption></figure></div> <h3>Understanding Monotonic Improvement in Policy Iteration</h3> <p>The powerful cycle of policy evaluation and policy improvement continues iteratively, driving the algorithm towards optimality. A fundamental and crucial characteristic of Policy Iteration is its guarantee of <em><strong>monotonic improvement</strong></em><strong>.</strong> This means that each successive iteration is guaranteed to yield a policy that performs at least as well as, and frequently strictly better than, its predecessor in terms of expected long-term return. This consistent, non-decreasing enhancement of the policy's value at every step is a cornerstone of Policy Iteration's reliability and effectiveness. </p> <p>Crucially, for finite state-action spaces, Policy Iteration is mathematically guaranteed to converge to an optimal policy within a finite number of steps. This explicit process of maintaining and continually refining the policy throughout its execution solidifies Policy Iteration as a robust and reliable method for discovering optimal control strategies.</p> <h2>Value Iteration for Optimal Policies</h2> <p>Value Iteration (VI) offers a distinct and powerful approach to solving Markov Decision Processes (MDPs) by directly computing the optimal value function, V*(s). Unlike Policy Iteration, VI does not explicitly maintain or iteratively improve a policy throughout its process. Instead, it focuses solely on refining the value estimates for each state until they converge to their optimal values, from which the optimal policy can then be readily derived.</p> <p>The algorithm begins by initializing an arbitrary value function, commonly setting V&#8320;(s) = 0 for all states s. In each subsequent iteration k+1, the value of every state s is updated by considering the maximum expected return achievable from that state across all possible actions. This critical update rule is encapsulated by the <strong>Bellman Optimality Equation</strong>, which serves as the core iterative principle:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V_{k+1}(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V_k(s') \\right]&quot;,&quot;id&quot;:&quot;IBZPNBDGRC&quot;}" data-component-name="LatexBlockToDOM"></div> <p>In this equation, R(s,a) represents the immediate reward received for taking action a in state s. The term &#947; &#931;_{s'} P(s'|s,a)V_k(s') accounts for the discounted expected value of the next state, s', weighted by its transition probability P(s'|s,a) and evaluated using the current value function V_k. The entire expression inside the maximization, R(s,a) + &#947; &#931;_{s'} P(s'|s,a)V_k(s'), effectively represents the Q-value of taking action a in state s and then proceeding optimally according to the current value function V_k. By selecting the action that maximizes this Q-value, Value Iteration iteratively builds up the optimal value function by considering increasingly longer planning horizons.</p> <p>The guaranteed convergence of Value Iteration to a unique optimal value function is attributed to a fundamental mathematical property of the Bellman optimal operator: it is a <strong>contraction mapping</strong>. When the discount factor &#947; is less than 1, applying this operator repeatedly reduces the "distance" between successive value functions. This contraction property ensures that the sequence of value functions, V&#8320;, V&#8321;, V&#8322;, ..., will converge to a single, unique fixed point, which is precisely the optimal value function V*(s). This theoretical soundness is crucial for Value Iteration, firmly establishing that the algorithm will arrive at the correct V*(s), without requiring formal proofs of this property during its conceptual application.</p> <p>Once the value function V*(s) has converged to a stable state, the optimal policy &#960;*(s) can be extracted directly. For each state s, the optimal policy simply dictates selecting the <strong>action a that maximizes the expected return</strong>, utilizing the converged optimal value function to evaluate future states:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\pi^*(s) = \\text{argmax}_{a} \\left[ R(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a)V^*(s') \\right]&quot;,&quot;id&quot;:&quot;DELNIUDSMZ&quot;}" data-component-name="LatexBlockToDOM"></div> <p>It is an important distinction that while the value function itself monotonically converges to the optimal one in Value Iteration, the policy extracted at intermediate steps does not necessarily guarantee monotonic improvement. This contrasts with Policy Iteration, where policy improvement is a guaranteed step in each iteration.</p> <h2>VI. Key Challenges in Reinforcement Learning</h2> <p>Reinforcement Learning (RL) faces several significant challenges, which are central to the study of its mathematical foundations and practical applications:</p> <p><strong>1. Function Approximation</strong></p> <p>Classical Reinforcement Learning models <strong>often rely on tabular methods</strong>, which assume a finite number of states with stored values and policies. However, real-world applications like large-scale video games, autonomous driving, or complex robotic systems involve millions, billions, or even continuous state and action spaces. Directly representing or tabulating values and policies for such vast spaces is computationally intractable and memory-prohibitive. Function approximation, using <strong>parameterized functions like neural networks or linear models,</strong> approximates the value function (Q-values or state-values) or the policy directly. This allows RL algorithms to generalize from limited states to unseen ones, enabling scalability to environments with immense or continuous state-action spaces. The challenge then shifts from storing every state-action pair to learning and optimizing the parameters of these complex functions.</p> <p><strong>2. Partial Observability (POMDPs)</strong></p> <p>In many practical settings, an agent does not have complete or perfect information about the environment's true underlying state. Instead, it receives only limited, noisy, or incomplete observations. This condition introduces the significant challenge of <strong>partial observability,</strong> where the agent must infer the underlying true state from its incomplete observation history. For example, a robot navigating a cluttered room might only perceive its immediate vicinity through its sensors, rather than possessing a complete, global map of the entire room layout. <strong>Partially Observable Markov Decision Processes (POMDPs)</strong> are the formal mathematical framework used to model and study such scenarios. In POMDPs, agents cannot simply react to the current observation; they are required to maintain a belief distribution over possible true states, continuously updating this belief based on new observations and past actions. This belief state, rather than the true state, then guides the agent's decision-making, significantly increasing the complexity of policy derivation and learning.</p> <p><strong>3. Multi-Agent RL (MARL)</strong></p> <p>Multi-Agent Reinforcement Learning (MARL) addresses the complexity of RL when multiple agents interact in a shared environment. Agents can collaborate to achieve a common goal (e.g., robots) or compete (e.g., players in a game). Each agent&#8217;s perspective introduces dynamic and non-stationary environments, as the optimal policy depends on the evolving policies of other agents. This non-stationarity violates the core Markovian assumption in single-agent RL, where the environment&#8217;s dynamics are static. MARL research often draws upon game-theoretical perspectives to design algorithms and establish theoretical guarantees for these intricate multi-agent systems. Simple Markov policies may be insufficient; more sophisticated general policies that incorporate the full history of observations and actions are necessary to account for complex interdependencies and evolving behaviors.</p> <p><strong>4. Sample and Computational Efficiency</strong></p> <p>Two critical practical concerns in deploying Reinforcement Learning systems are sample efficiency and computational efficiency. Modern RL algorithms require millions to hundreds of millions of interactions with the environment to learn effective policies, which is prohibitively expensive, time-consuming, and potentially unsafe in real-world applications like training autonomous vehicles or operating physical robots. Reducing sample complexity is a primary goal in RL research, while computational efficiency addresses the substantial processing power required for training advanced RL models, especially those using deep neural networks. Iterative algorithms, based on principles from dynamic programming, mitigate high computational costs by avoiding expensive operations like large matrix inversions, especially in environments with vast state spaces. Balancing data requirements with practical constraints remains a significant hurdle for real-world RL deployment.</p> <h2>Conclusion</h2> <p>This guide explains the core principles of Reinforcement Learning, from agent-environment interactions and Markov Decision Processes to powerful control algorithms like Policy and Value Iteration. It explores how policies, value functions, and the discount factor define optimal strategies in complex state spaces through simulation. Despite challenges like function approximation, partial observability, and multi-agent interactions, understanding these foundational principles is crucial for designing intelligent systems that learn and adapt in dynamic environments.</p> <div><hr/></div> <p>Thank you so much for taking the time to read through my thoughts. This newsletter is a small space where I share my learnings and explorations in RL, Generative AI, and beyond as I continue to study and grow. If you found value here, I&#8217;d be honored if you subscribed and shared it with friends who might enjoy it too. Your feedback means the world to me, and I genuinely welcome both your kind words and constructive critiques.</p> <p>With heartfelt gratitude,<br/>Thank you for being part of Neuraforge!<br/>Narasimha Karthik J</p> <p></p> <div class="subscription-widget-wrap-editor" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}" data-component-name="SubscribeWidgetToDOM"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">Thanks for reading NeuraForge: AI Unleashed! Subscribe for free to receive new posts and support my work.</p></div><form class="subscription-widget-subscribe"><input type="email" class="email-input" name="email" placeholder="Type your email&#8230;" tabindex="-1"/><input type="submit" class="button primary" value="Subscribe"/><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[A comprehensive guide to understanding Markov Decision Processes, Policy Iteration, Value Iteration, and achieving optimal behavior in RL.]]></summary></entry><entry><title type="html">Beyond Supervised Learning: Unlocking AIs Potential with Reinforcement Learning</title><link href="https://jnk234.github.io/blog/2025/beyond-supervised-learning-unlocking-ais-potential-with-reinforcement-learning/" rel="alternate" type="text/html" title="Beyond Supervised Learning: Unlocking AIs Potential with Reinforcement Learning"/><published>2025-07-14T12:02:02+00:00</published><updated>2025-07-14T12:02:02+00:00</updated><id>https://jnk234.github.io/blog/2025/beyond-supervised-learning-unlocking-ais-potential-with-reinforcement-learning</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/beyond-supervised-learning-unlocking-ais-potential-with-reinforcement-learning/"><![CDATA[<h2>Introduction</h2> <p>Imagine AI systems that learn not from pre-labeled data, but through iterative trial and error, autonomously mastering complex tasks by interacting with their environment. This is the essence of Reinforcement Learning (RL), a transformative paradigm driving the next wave of artificial intelligence. This deep dive will unravel the core principles of RL, explore how its powerful merger with deep neural networks has unlocked unprecedented capabilities, and showcase its diverse applications across industries, ultimately touching upon the frontier challenges shaping its future.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080" width="4000" height="6000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:6000,&quot;width&quot;:4000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;black and white robot illustration&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="black and white robot illustration" title="black and white robot illustration" srcset="https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1625314887424-9f190599bd56?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyfHxyb2JvdHxlbnwwfHx8fDE3NTI0MjgxOTl8MA&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Xu Haiwei</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h2>Understanding Reinforcement Learning: A Paradigm Shift in AI</h2> <p>Artificial Intelligence (AI) and Machine Learning (ML) are rapidly transforming various domains. While Supervised Learning has demonstrated remarkable success in tasks requiring extensive labeled datasets, a distinct paradigm, Reinforcement Learning (RL), provides a compelling alternative. RL empowers intelligent agents to acquire optimal behaviors through iterative trial and error within complex, dynamic environments. This section establishes the foundational concepts of Reinforcement Learning, highlighting its unique approach and differentiating it from traditional machine learning paradigms.</p> <h3>Supervised Learning vs. Reinforcement Learning: Key Differences and Limitations</h3> <p>Supervised Learning operates on the principle of learning from labeled examples. A model is trained to map input features to known output labels, minimizing a predefined error function. This approach excels in tasks such as image classification, natural language processing, and regression where ample ground truth data is available, allowing the model to learn a direct mapping between inputs and desired outputs.</p> <p>In contrast, Reinforcement Learning does not rely on pre-existing labeled datasets. Instead, an RL agent learns by interacting directly with an environment. The agent receives scalar feedback in the form of rewards or penalties for its actions, aiming to maximize a <em>cumulative reward</em> signal over time. This paradigm is particularly suited for sequential decision-making problems, where the optimal action depends on the current state and crucially influences future states and subsequent rewards. Limitations of supervised learning often arise in scenarios requiring strategic decision-making, where the "correct" answer for every possible state-action pair is not explicitly provided, or where the optimal behavior emerges from a sequence of interactions rather than isolated mappings. RL directly addresses these challenges by enabling autonomous learning through experience.</p> <h3>The Core Components of Reinforcement Learning</h3> <p>An RL system comprises several fundamental elements working in concert to facilitate learning and decision-making:</p> <ul><li><p><strong>Agent:</strong> The learner or decision-maker. The agent observes the environment's state and selects actions based on its learned policy.</p></li><li><p><strong>Environment:</strong> The external world with which the agent interacts. It encompasses the rules of interaction, the possible states the agent can be in, and the reward signals provided in response to the agent's actions.</p></li><li><p><strong>State:</strong> A complete, or sufficiently informative, description of the environment at a given moment. The agent's decision-making process is fundamentally based on its perception or representation of the current state.</p></li><li><p><strong>Action:</strong> A specific move or decision made by the agent that influences the environment's state. Actions are the means by which the agent interacts with and changes its surroundings.</p></li><li><p><strong>Reward:</strong> A scalar feedback signal provided by the environment to the agent after an action. This signal quantifies the immediate desirability of the agent's action in that particular state. The agent's primary objective is to maximize its total accumulated reward over the long term.</p></li><li><p><strong>Policy:</strong> The agent's strategy or behavior function. It defines how the agent maps observed states to actions. An optimal policy dictates the best action to take in any given state to maximize long-term cumulative reward.</p></li></ul> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ToXq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ToXq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 424w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 848w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1272w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png" width="1456" height="803" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:803,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:770936,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://neuraforge.substack.com/i/168227160?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ToXq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 424w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 848w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1272w, https://substackcdn.com/image/fetch/$s_!ToXq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa561374-3c78-4726-a9a9-e17c0e847b41_2118x1168.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p>Consider a robotic arm learning to pick up an object. The robotic arm itself is the <strong>agent</strong>. The physical world, including the object's position, the table, and gravity, constitutes the <strong>environment</strong>. The arm's joint angles, the gripper's status, and the object's coordinates represent the current <strong>state</strong>. Moving a joint or opening/closing the gripper are <strong>actions</strong>. A positive <strong>reward</strong> is received when the object is successfully grasped, while a negative reward might be given if the object is dropped or if the arm collides with an obstacle. The arm's learned strategy for moving its joints and operating its gripper to consistently grasp objects is its <strong>policy</strong>.</p> <h3>The Credit Assignment Problem: Learning from Delayed Rewards</h3> <p>A central challenge in Reinforcement Learning is the <em>credit assignment problem</em>. Unlike supervised learning, where feedback (the correct label or value) is immediate for each prediction, RL agents often receive rewards that are delayed and sparse. An action taken at a specific time step might only contribute to a significant positive (or negative) reward much later in the sequence of interactions. Determining which past actions were truly responsible for a future outcome becomes a non-trivial task.</p> <p>The agent's goal is to maximize the <em>cumulative reward</em> over an entire episode or its lifetime, not just immediate rewards. This necessitates that the agent learns the long-term consequences of its actions, effectively assigning credit or blame to actions that contributed to these delayed outcomes. Overcoming the credit assignment problem is fundamental to developing effective RL policies, enabling agents to learn complex, optimal behaviors even when feedback is sparse and arrives long after the causative actions.</p> <h2>The Rise of Deep Reinforcement Learning: Combining Data and Optimization</h2> <p>The previous section established the foundational concepts of Reinforcement Learning (RL), differentiating it from supervised learning and outlining its core components. This section explores the pivotal advancements that led to the emergence of Deep Reinforcement Learning (Deep RL), a powerful paradigm that combines the perceptual capabilities of deep neural networks with the decision-making frameworks of classical RL, fundamentally transforming the field of AI.</p> <h3>The Revolution of Deep Learning: Feature Extraction and Generative Models</h3> <p>Deep Learning fundamentally transformed the field of Artificial Intelligence by enabling models to learn intricate, hierarchical representations directly from raw, high-dimensional data. Prior to the Deep Learning revolution, extracting meaningful features from raw inputs&#8212;a crucial step for any AI model&#8212;was often a manual and laborious process known as feature engineering. This reliance on human expertise and domain-specific knowledge presented a significant bottleneck for scalability and generalization.</p> <p>Deep neural networks, particularly convolutional neural networks (CNNs) for image data and recurrent neural networks (RNNs) for sequential data, demonstrated unprecedented capabilities in automatically learning and extracting abstract features from high-dimensional inputs without explicit human programming. This allowed models to identify complex patterns and abstract representations from data like pixels or raw audio, a capability indispensable for an RL agent operating in realistic environments. Furthermore, advancements in generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), showcased deep learning's power in synthesizing realistic data, a capability that indirectly contributes to robust training through data augmentation or the development of more accurate environment models in complex RL scenarios. These breakthroughs in automatic perception and representation learning laid the essential groundwork for integrating deep neural networks into RL agents, enabling them to process the complex sensory information of the real world.</p> <h3>Historical Roots of Modern Reinforcement Learning: Classical RL and Control Optimization</h3> <p>Reinforcement Learning boasts a rich history that significantly predates the Deep Learning revolution. Classical RL algorithms, such as <strong>Q-learning</strong> and <strong>SARSA</strong>, provided foundational theoretical frameworks for how an agent could learn to maximize cumulative rewards through iterative trial and error. These methods focused on learning optimal value functions (estimating the goodness of states or state-action pairs) or policies (direct mappings from states to actions) primarily in discrete, often finite, state-action spaces. They effectively addressed challenges like the credit assignment problem, where an agent must determine which past actions were responsible for a delayed reward.</p> <p>Concurrently, the mature field of optimal control theory, deeply rooted in mathematics and engineering, developed robust methods for designing controllers that optimize system behavior over time. Techniques like dynamic programming provided rigorous frameworks for sequential decision-making, often assuming a complete and accurate model of the environment. While highly effective in well-defined, lower-dimensional problems with clear state representations, these classical approaches faced significant scalability challenges. They struggled immensely when confronted with high-dimensional state spaces (e.g., raw pixel inputs), continuous action spaces (e.g., robotic joint torques), or complex, unknown environments where a perfect model was unavailable or intractable to define. This <em>"curse of dimensionality"</em> was a primary barrier to applying RL to real-world complexities.</p> <h3>Deep Reinforcement Learning (Deep RL): Merging Perception with Decision-Making</h3> <p>Deep Reinforcement Learning emerged as a powerful paradigm from the synergistic combination of Deep Learning's unparalleled representation learning capabilities and Reinforcement Learning's robust decision-making frameworks. This merger directly addressed the critical scalability limitations that plagued classical RL algorithms. By employing deep neural networks as highly flexible and powerful function approximators, Deep RL agents can effectively handle high-dimensional, raw sensory inputs such as pixels from a camera, raw audio signals, or complex sensor readings.</p> <p>In this integrated architecture, the deep neural network serves as the "perception" layer. It processes the raw observations, automatically extracting meaningful, abstract features and representations that are crucial for understanding the environment's state. Subsequently, the classical RL algorithms leverage these rich, learned representations to learn optimal "decision-making" strategies. The deep network can approximate complex non-linear mappings from states to actions, value functions, or even environment models, allowing the agent to generalize from a limited set of experiences. This profound integration allowed RL to move far beyond tabular methods and simple, hand-engineered state representations, enabling agents to learn directly from raw sensor data and operate effectively in highly complex, realistic, and previously intractable environments.</p> <h3>Pioneering Achievements: From AlphaGo's 'Move 37' to Emergent Behaviors and 'The Bitter Lesson'</h3> <p>The advent of Deep RL ushered in a series of groundbreaking achievements that not only captured widespread public attention but also profoundly demonstrated the paradigm's immense potential. A landmark success was DeepMind's Deep Q-Network (DQN), which achieved human-level performance across a diverse suite of Atari 2600 video games by learning directly from raw pixel inputs, showcasing the power of end-to-end learning.</p> <p>A more profound milestone, however, was AlphaGo's historic victory over the world champion Go player, Lee Sedol. Notably, <strong>Move 37</strong> in Game 2, an unconventional and seemingly counter-intuitive move, demonstrated a level of strategic intuition and creativity previously thought exclusive to human masters. This achievement highlighted Deep RL's capacity for complex, long-term planning and its ability to discover novel, highly effective strategies in domains with immense state spaces. </p> <p>Beyond specific games, Deep RL has enabled the emergence of sophisticated and often surprising behaviors in diverse domains, including complex robotic control, multi-agent coordination, and intricate simulations. These successes frequently involve agents discovering non-intuitive or highly optimized solutions that surpass human-designed approaches. These groundbreaking achievements collectively underscore "The Bitter Lesson," a concept emphasizing that general methods that leverage massive amounts of computation and data scale more effectively and ultimately outperform human-designed knowledge or handcrafted features, reinforcing the power of end-to-end learning in complex domains.</p> <h2>RL in Action: Diverse Applications Across Industries</h2> <p>The previous sections established the foundational principles of Reinforcement Learning (RL) and explored the transformative impact of Deep Learning on its capabilities. With a robust understanding of how RL agents learn optimal behaviors through interaction and cumulative reward, this section now shifts focus to the practical deployment of RL across various industries.</p> <p>Reinforcement Learning's inherent ability to navigate complex, dynamic environments and solve sequential decision-making problems makes it uniquely suited for a wide array of real-world applications. Its versatility allows it to not only optimize existing processes and discover novel strategies but also to enable autonomous agents in scenarios where traditional, rule-based methods fall short due to the sheer complexity or uncertainty of the environment. RL's capacity for adaptive learning from experience is key to its success in these diverse domains.</p> <p>The following areas represent significant domains where Reinforcement Learning is actively being applied:</p> <h3>Game Playing and Robotics: Mastering Complex Control and Emergent Behaviors</h3> <p>Reinforcement Learning has demonstrated remarkable success in mastering complex games, often surpassing human capabilities. These environments provide structured, yet highly dynamic, scenarios with clear reward signals, allowing agents to learn optimal policies for navigating vast state-action spaces. Similarly, in robotics, RL enables agents to learn intricate motor control and develop sophisticated behaviors directly from interaction with physical or simulated environments. This includes tasks ranging from manipulation and locomotion to complex navigation, where the agent learns to adapt its actions based on real-time sensory input to achieve desired objectives.</p> <h3>Healthcare and Finance: Optimizing Decisions in Critical Domains</h3> <p>In critical sectors such as healthcare and finance, RL offers powerful tools for optimizing decision-making processes under uncertainty. In healthcare, potential applications include personalizing treatment recommendations by adapting to patient responses over time, optimizing drug discovery pipelines through iterative experimentation, and managing chronic diseases with dynamic intervention strategies. In finance, RL can be applied to develop adaptive trading strategies, optimize portfolio allocation, and enhance risk management by learning from market dynamics and making sequential decisions that aim to maximize long-term returns while mitigating exposure. The ability of RL to account for long-term consequences and stochastic environments is particularly valuable here.</p> <h3>Recommender Systems and Resource Management: Personalized Experiences and Efficiency</h3> <p>RL's capacity for sequential decision-making extends to enhancing user experiences and optimizing resource allocation. In recommender systems, RL agents can learn to suggest items that not only satisfy immediate user preferences but also maximize long-term user engagement and satisfaction by understanding the evolving user journey and predicting future interactions. For resource management, RL can optimize complex systems such as energy consumption in smart grids, traffic flow in urban networks, or logistical operations in supply chains. By making adaptive decisions based on real-time data and environmental feedback, RL agents can improve efficiency, reduce waste, and enhance system performance dynamically.</p> <h3>Beyond Traditional Fields: New Frontiers of Reinforcement Learning Application</h3> <p>The applicability of Reinforcement Learning continues to expand beyond these established domains, demonstrating its profound versatility. New frontiers are constantly being explored, showcasing RL's potential to address novel challenges in areas ranging from accelerated scientific discovery and materials design to intelligent infrastructure management and smart cities. In these emerging fields, RL's core principle of learning optimal actions through interaction&#8212;even when the optimal path is unknown or the environment is highly complex&#8212;remains a powerful paradigm for solving increasingly intricate problems and driving innovation.</p> <h2>Beyond the Basics: Advanced Concepts and Future Challenges in Deep Reinforcement Learning</h2> <p>The previous sections established the foundational principles and diverse applications of Reinforcement Learning (RL) and Deep Reinforcement Learning (Deep RL). While Deep RL has achieved remarkable successes, its deployment in complex, real-world scenarios still presents significant challenges. This section explores advanced concepts and addresses key limitations, outlining the future trajectory of Deep RL and its role in the pursuit of Artificial General Intelligence (AGI).</p> <h3>Addressing Limitations: Sparse Rewards and Sample Efficiency</h3> <p>A primary challenge in many complex RL environments is the issue of <strong>sparse rewards</strong>. In such scenarios, positive feedback is infrequent or only occurs at the culmination of a long sequence of actions, making it difficult for an agent to learn which actions contributed to the eventual success. For example, in a complex robotic assembly task, a reward might only be given upon successful completion of the entire assembly, rather than for each intermediate step. This sparsity significantly complicates the <strong>credit assignment problem</strong>, where the agent struggles to attribute success or failure to specific preceding actions. Without clear, frequent signals, the learning process can be extremely slow or even fail to converge on an effective policy.</p> <p>Closely related is <strong>sample efficiency</strong>. Deep RL algorithms often require an immense number of interactions with the environment to learn an effective policy. This extensive data requirement can be prohibitive in real-world applications where interactions are costly, time-consuming, or unsafe (e.g., in robotics for physical damage or in healthcare for patient safety). Research in this area focuses on developing methods that enable agents to learn effectively from limited experience, such as experience replay enhancements, model-based RL, and curriculum learning, although specific methodologies are beyond the scope of this discussion. Improving sample efficiency is crucial for making Deep RL practical for real-world deployment.</p> <h3>Learning from Others: Imitation Learning and Inverse Reinforcement Learning</h3> <p>When direct reward engineering is challenging or expert demonstrations are readily available, alternative paradigms like <strong>Imitation Learning (IL)</strong> and <strong>Inverse Reinforcement Learning (IRL)</strong> become valuable. <strong>Imitation Learning</strong> allows an agent to learn a policy by observing and mimicking an expert's behavior. Conceptually, this treats the problem as a supervised learning task where the expert's observations are inputs and their corresponding actions are the desired outputs (labels). This approach bypasses the need for a manually designed reward function altogether, directly learning a behavioral policy from demonstrations.</p> <p><strong>Inverse Reinforcement Learning</strong> takes a different, more profound approach. Instead of learning a policy from a predefined reward, IRL aims to infer the underlying reward function that best explains an observed expert's behavior. The premise is that the expert is acting optimally with respect to some unknown reward function. Once this implicit reward function is inferred, standard RL algorithms can then be applied to optimize a policy for that learned reward. Both IL and IRL offer powerful ways to leverage human expertise to accelerate and simplify the learning process for complex tasks, especially where specifying explicit rewards is impractical.</p> <h3>Adapting and Transferring Knowledge: Transfer Learning and Meta-learning ('Learning to Learn')</h3> <p>For RL agents to be truly versatile and efficient, they must possess the ability to adapt to new tasks and environments without starting from scratch. <strong>Transfer Learning</strong> in RL involves leveraging knowledge acquired from solving one task or in one environment to improve learning performance on a different, but related, task or environment. This can manifest as pre-training a policy or value function on a simpler or related task, or transferring learned features and representations that are broadly applicable. The goal is to reduce the training time and data required for new tasks by capitalizing on previously gained insights.</p> <p><strong>Meta-learning</strong>, often referred to as "learning to learn," pushes this concept further. A meta-learning agent is trained across a distribution of tasks such that it can quickly adapt to a new, unseen task with minimal additional training data or interactions. This involves learning general learning strategies, optimal initialization parameters, or efficient optimization procedures that enable rapid adaptation to novel situations. Unlike traditional transfer learning which reuses learned <em>knowledge</em>, meta-learning focuses on learning <em>how to learn</em> effectively across a family of tasks. These approaches are critical for achieving greater generalization and reducing the prohibitive sample complexity often associated with training agents for diverse real-world applications.</p> <h3>The Path to General Intelligence: Reinforcement Learning with Human Feedback (RLHF) and Current Challenges</h3> <p>Reinforcement Learning plays a pivotal role in the ongoing pursuit of Artificial General Intelligence (AGI). A key development in this direction, particularly for large language models (LLMs), is <strong>Reinforcement Learning with Human Feedback (RLHF)</strong>. RLHF involves fine-tuning a model (e.g., an LLM) using human preferences as a reward signal. Humans provide feedback by rating the quality, helpfulness, or safety of different model outputs. An RL agent then learns a policy that aligns the model's behavior more closely with human values and intentions, effectively "teaching" the model what constitutes a good response. This approach has been instrumental in making LLMs more helpful, harmless, and honest, demonstrating RL's power in aligning complex AI systems with human objectives.</p> <p>Despite these advancements, significant conceptual and practical challenges remain on the path to AGI. These include developing agents that can robustly handle open-ended, ill-defined problems, exhibit common-sense reasoning, and possess true understanding beyond mere pattern matching or statistical correlation. While RL provides a powerful framework for learning complex behaviors, further research is essential to overcome current limitations and unlock its full potential in creating adaptable, truly intelligent systems capable of tackling the most complex real-world problems.</p> <h2>Conclusion</h2> <p>Reinforcement Learning, especially Deep RL, has emerged as a transformative force in AI, empowering intelligent agents to learn optimal behaviors through iterative interaction and cumulative reward in complex, dynamic environments. Its fusion with deep neural networks has overcome previous limitations, leading to groundbreaking achievements across diverse domains from gaming and robotics to healthcare and finance. While challenges like sample efficiency and sparse rewards remain, ongoing research into advanced techniques such as transfer learning and RLHF is continually expanding its capabilities. As we push these frontiers, RL is set to be a cornerstone in developing more adaptable, truly intelligent systems and advancing the path towards Artificial General Intelligence.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Understand the fundamental shift RL offers, its core components, real-world applications, and the advanced challenges driving the future of intelligent systems.]]></summary></entry><entry><title type="html">Implementing GPT-Style Attention: A Step-by-Step Guide with PyTorch</title><link href="https://jnk234.github.io/blog/2025/implementing-gpt-style-attention-a-step-by-step-guide-with-pytorch/" rel="alternate" type="text/html" title="Implementing GPT-Style Attention: A Step-by-Step Guide with PyTorch"/><published>2025-01-21T07:15:20+00:00</published><updated>2025-01-21T07:15:20+00:00</updated><id>https://jnk234.github.io/blog/2025/implementing-gpt-style-attention-a-step-by-step-guide-with-pytorch</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/implementing-gpt-style-attention-a-step-by-step-guide-with-pytorch/"><![CDATA[<h2>Introduction</h2> <p>Attention mechanisms have revolutionized the field of natural language processing (NLP) in recent years, enabling models to effectively capture long-range dependencies and achieve state-of-the-art performance on a wide range of tasks. At the heart of modern language models like the Transformer and GPT series lies the self-attention mechanism, a powerful tool for relating different positions within an input sequence.</p> <p>In this blog post, we'll explore the inner workings of attention, starting from the limitations of traditional approaches and building up to the efficient multi-head attention used in today's cutting-edge models. You can follow this blog post along with this <a href="https://colab.research.google.com/drive/1fi1YFBixhPjConeVDogSxcwiPSSrvkZX?usp=sharing">Colab notebook</a>.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3956" height="2220" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2220,&quot;width&quot;:3956,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;cable network&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="cable network" title="cable network" srcset="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1558494949-ef010cbdcc31?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwyM3x8bmV1cmFsJTIwbmV0d29ya3N8ZW58MHx8fHwxNzM3NDE2NzI0fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Taylor Vick</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h2>Recap: The Problem with Modeling Long Sequences</h2> <p>Recurrent Neural Networks (RNNs) and encoder-decoder models have been widely used for processing sequential data in natural language processing tasks. However, these architectures face several challenges when dealing with long sequences:</p> <ul><li><p><strong>Information Bottleneck Problem</strong>: RNNs and encoder-decoder models compress the entire input sequence into a fixed-size hidden state vector. As the sequence length grows, it becomes increasingly difficult to pack all the necessary information into this fixed-size representation. Important details from earlier parts of the sequence can be lost or overwritten as the hidden state is updated at each step, making it challenging to capture long-range dependencies.</p></li><li><p><strong>Vanishing or Exploding Gradient Problem</strong>: During training, as the gradient signal is backpropagated through time, it can either decay exponentially (vanishing) or grow exponentially (exploding).</p><ul><li><p>Vanishing gradients make it difficult for the model to learn long-range dependencies, as the gradient signal becomes too weak to effectively update earlier parts of the network.</p></li><li><p>Exploding gradients can cause the model to become unstable and diverge during training.</p></li></ul></li></ul> <div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!25PW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png" width="375" height="134" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:134,&quot;width&quot;:375,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3488,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!25PW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 424w, https://substackcdn.com/image/fetch/$s_!25PW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 848w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1272w, https://substackcdn.com/image/fetch/$s_!25PW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb74762d9-bf7e-4bf8-8170-1b7edb2cb9ab_375x134.png 1456w" sizes="100vw"/></picture><div></div></div></a><figcaption class="image-caption">Source: https://github.com/sooftware/seq2seq</figcaption></figure></div> <p>In encoder-decoder models, the decoder has limited access to the context from the input sequence. The fixed-size hidden state from the encoder is the only information available to the decoder at each generation step, which can be insufficient for capturing all the relevant context. This is particularly problematic for tasks like machine translation, where understanding the entire source sequence is crucial for generating accurate translations.</p> <p>Furthermore, RNNs and encoder-decoder models process the input sequence sequentially, one token at a time. This sequential processing can be time-consuming, especially for long sequences, as the computation cannot be easily parallelized. Each hidden state update depends on the previous hidden state, creating an inherent sequential dependency that limits the ability to take advantage of modern hardware like GPUs that excel at parallel processing.</p> <p>These limitations motivated the development of attention mechanisms, which allow the model to selectively focus on different parts of the input sequence during processing. By enabling the model to access and utilize relevant information from the entire sequence, attention mechanisms can effectively capture long-range dependencies and overcome the limitations of fixed-size hidden state representations.</p> <h2>Capturing Dependencies with Attention Mechanisms</h2> <p>To address the limitations of RNNs and encoder-decoder models, researchers introduced attention mechanisms. Attention allows the model to selectively focus on different parts of the input sequence during processing, enabling it to capture long-range dependencies more effectively.</p> <p>One of the first attention mechanisms proposed was the <em><strong>Bahdanau</strong></em> attention, introduced in 2014 for neural machine translation. In this approach, the decoder can attend to relevant parts of the source sequence at each generation step, rather than relying solely on the fixed-size hidden state from the encoder. This is achieved by computing attention weights that determine the importance of each source token for the current decoding step.</p> <p>The attention mechanism works by calculating a compatibility score between the current decoder hidden state and each encoder hidden state. These scores are then normalized using a softmax function to obtain attention weights. The weighted sum of the encoder hidden states, based on the attention weights, forms the context vector that provides relevant information to the decoder at each step.</p> <p>By allowing the decoder to access and utilize information from the entire source sequence, the Bahdanau attention mechanism enables the model to capture long-range dependencies and generate more accurate translations. This approach laid the foundation for subsequent developments in attention mechanisms.</p> <p>Building upon this idea, the Transformer architecture, introduced in the influential paper <em><strong>"Attention Is All You Need" by Vaswani et al. in 2017</strong></em>, took attention to the next level with the self-attention mechanism. Self-attention extends the attention concept to capture dependencies within a single sequence, rather than just between the encoder and decoder.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!WXL9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png" width="380" height="560" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:560,&quot;width&quot;:380,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:71606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WXL9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 424w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 848w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1272w, https://substackcdn.com/image/fetch/$s_!WXL9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ec88d5-9b55-4034-96be-f138ab6a831b_380x560.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Transformer Architecture proposed by Vasvani et al in 2017</a></figcaption></figure></div> <p>In self-attention, each token in the input sequence attends to all other tokens in the sequence, allowing the model to capture rich, context-dependent representations. This mechanism enables the model to directly learn the relationships and dependencies between different positions in the sequence, without relying on recurrent or convolutional operations.</p> <p>The self-attention mechanism forms the core of the Transformer architecture and has revolutionized natural language processing. It has led to the development of powerful language models like BERT, GPT, and their variants, which have achieved state-of-the-art performance on a wide range of tasks, including language understanding, generation, and translation.</p> <p>By leveraging attention mechanisms, particularly self-attention, models can effectively capture long-range dependencies, handle variable-length sequences, and process information in parallel. This has greatly enhanced the ability of models to understand and generate coherent and contextually relevant language.</p> <p>In the following sections, we will dive deeper into the details of self-attention and explore its implementation in modern language models.</p> <h2>Simplified Self-Attention Mechanism</h2> <p>To gain a better understanding of how self-attention works, let's start with a simplified version of the mechanism and walk through the computation step by step. Consider an input sequence <code>X</code> of length 6, where each token is represented by a 3-dimensional embedding vector. The goal of self-attention is to compute a new representation for each token that incorporates information from the entire sequence. This is achieved by calculating attention weights between pairs of tokens and using these weights to compute weighted sums of the input embeddings.</p> <p>First, we compute the dot product between each pair of token embeddings. The dot product serves as a measure of similarity between tokens, indicating how much they should attend to each other. In PyTorch, we can compute the dot products efficiently using matrix multiplication:</p> <pre><code><code>import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

attn_scores = inputs @ inputs.T
</code></code></pre> <p>In the code above, <code>attn_scores[i][j]</code> represents the dot product between the embeddings of tokens i and j. The resulting <code>attn_scores</code> matrix captures the similarity scores between all pairs of tokens.</p> <p>Next, we apply the softmax function to each row of the <code>attn_scores</code> matrix to obtain the attention weights. The softmax function normalizes the scores, converting them into probabilities that sum up to 1. This ensures that the attention weights can be interpreted as the relative importance of each token for a given token:</p> <pre><code><code>attn_weights = torch.softmax(attn_scores, dim=-1)</code></code></pre> <p>After applying the softmax function, <code>attn_weights[i][j]</code> represents the normalized attention weight indicating how much token i attends to token j.</p> <p>Finally, we compute the self-attended representations by taking a weighted sum of the input embeddings using the attention weights:</p> <pre><code><code>context_vecs = attn_weights @ inputs</code></code></pre> <p>The resulting <code>context_vecs</code> matrix contains the self-attended representations for each token. Each row in <code>context_vecs</code>is a weighted sum of the input embeddings, where the weights are determined by the attention weights. This allows each token to incorporate information from the entire sequence, weighted by the relevance of each token.</p> <p>This simplified version of self-attention demonstrates the core idea of allowing tokens to attend to each other and compute new representations based on the entire sequence. However, in practice, the self-attention mechanism used in Transformer models includes additional components, such as trainable weight matrices and scaling factors, which we will explore in the next section.</p> <h2>Math Behind Self-Attention</h2> <p>In the previous section, we explored a simplified version of self-attention that directly used the input embeddings to compute attention scores and context vectors. However, in practice, the self-attention mechanism used in Transformer models incorporates trainable weight matrices to project the inputs into query, key, and value representations before computing the attention scores.</p> <p>In self-attention, each input vector <strong>xi</strong> is projected onto three distinct vectors: query <strong>qi</strong>, key <strong>ki</strong>, and value <strong>vi</strong>. </p> <p>These projections are performed via learnable weight matrices <strong>Wq</strong>, <strong>Wk</strong>, and <strong>Wv</strong>, resulting in:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;q_i = x_i W_Q, \\quad k_i=x_i W_K, \\quad v_i=x_i W_V&quot;,&quot;id&quot;:&quot;OTDSYPPSNS&quot;}" data-component-name="LatexBlockToDOM"></div> <p>These weight matrices are initialized randomly and optimized during training.</p> <p>The simplified matrix representation, where the query, key, and value matrices are computed as a single operation, is given by:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;\\text{attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V&quot;,&quot;id&quot;:&quot;IYMDIGSHSI&quot;}" data-component-name="LatexBlockToDOM"></div> <p>The working of the above attention calculation is explained in the next section.</p> <h2>Adding Trainable Weights to Self-Attention</h2> <p>The query, key, and value matrices (Q, K, V) are obtained by multiplying the input embedding matrix <code>X</code> with learned weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>, respectively:</p> <pre><code><code>import torch.nn as nn

d_in = 3
d_out = 2

W_query = nn.Linear(d_in, d_out)
W_key   = nn.Linear(d_in, d_out)
W_value = nn.Linear(d_in, d_out)

print(W_query)

# Calculate queries, keys and values
queries = W_query(inputs)
keys    = W_key(inputs)
values  = W_value(inputs)</code></code></pre> <p>Here, <code>d_in</code> represents the input embedding dimension, and <code>d_out</code> represents the output dimension of the projected queries, keys, and values. The weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code> are initialized randomly and learned during the training process.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8H5b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png" width="418" height="641" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:641,&quot;width&quot;:418,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8H5b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 424w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 848w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1272w, https://substackcdn.com/image/fetch/$s_!8H5b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cd76d9c-ee16-4d13-9b21-f65500e6bf42_418x641.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Image Source: <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">Scaled Dot-Product Attention</a></figcaption></figure></div> <p>By projecting the inputs into separate query, key, and value spaces, the model can learn to capture different aspects of the input embeddings that are relevant for computing attention. This allows for more expressive and flexible representations compared to directly using the input embeddings.</p> <p>After obtaining the queries, keys, and values, the attention scores are computed as the scaled dot product between the queries and keys:</p> <pre><code><code>attn_scores = queries @ keys.T / keys.shape[-1]**0.5
print(attn_scores)</code></code></pre> <p>The scaling factor (the square root of the key dimension) is introduced to mitigate the effect of large magnitudes in the dot products, which can lead to extremely small gradients when passed through the <strong>softmax</strong> function. This scaling helps stabilize the training process and improve convergence.</p> <p>Once the attention scores are computed, the rest of the self-attention mechanism remains the same as in the simplified version. The attention weights are obtained by applying the <strong>softmax</strong> function to the scores, and the context vectors are computed as the weighted sum of the values using the attention weights.</p> <p>To encapsulate this computation in a more compact and reusable form, we can define a Python class that implements the self-attention mechanism with trainable weights:</p> <pre><code><code>class SelfAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()

        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

    def forward(self, x):
        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vecs = attn_weights @ values

        return context_vecs
</code></code></pre> <p>The <code>SelfAttention</code> class uses PyTorch's <code>nn.Linear</code> module to define the trainable weight matrices <code>W_query</code>, <code>W_key</code>, and <code>W_value</code>. The <code>forward</code> method performs the self-attention computation, taking the input embeddings <code>x</code> and returning the context vectors.</p> <p>By incorporating trainable weights into the self-attention mechanism, Transformer models can learn to adapt the attention computation to the specific requirements of the task at hand. This flexibility and expressiveness have contributed to the success of Transformer-based models in various natural language processing tasks.</p> <h2>Causal Attention: Masking Future Tokens</h2> <p>In certain tasks, such as language modeling or text generation, it's crucial to prevent the self-attention mechanism from accessing information from future tokens. This is where causal attention, also known as masked attention, comes into play.</p> <p>Causal attention restricts the self-attention computation to only consider the tokens up to the current position in the sequence. In other words, when computing the attention scores for a given token, only the tokens that appear before it in the sequence are considered.</p> <p>To achieve this, we modify the attention weight matrix by applying a mask that sets the upper triangular part of the matrix to negative infinity. This effectively prevents the model from attending to future tokens.</p> <p>Here's an example of how to create the mask and apply it to the attention scores:</p> <pre><code><code>import torch

def create_mask(context_length):
    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
    return mask.bool()

context_length = attn_scores.shape[0]
mask = create_mask(context_length)
attn_scores = attn_scores.masked_fill(mask, -torch.inf)
print(attn_scores)
</code></code></pre> <p>In the code above, we create a mask using PyTorch's <code>torch.triu</code> function, which sets the elements above the main diagonal to 1 and the rest to 0. We then convert the mask to a boolean tensor and use it to fill the upper triangular part of the <code>attn_scores</code> matrix with negative infinity.</p> <p><strong>By setting the masked positions to negative infinity, we ensure that the softmax function will assign zero attention weights to those positions, effectively ignoring the future tokens.</strong></p> <p>After applying the mask, we proceed with the rest of the self-attention computation as usual, applying the softmax function to obtain the attention weights and computing the context vectors.</p> <pre><code><code>attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
print(attn_weights)</code></code></pre> <p>It's worth noting that causal attention is particularly important in autoregressive models like GPT, where the model generates tokens sequentially and should only have access to the previously generated tokens at each step.</p> <h3>Adding Dropout</h3> <p>In addition to masking future tokens, we can also incorporate dropout regularization to the attention weights. Dropout helps prevent overfitting by randomly setting a fraction of the attention weights to zero during training. This encourages the model to rely on a broader set of tokens and reduces the risk of memorizing specific patterns.</p> <p>Here's an example of how to apply dropout to the attention weights:</p> <pre><code><code>dropout = nn.Dropout(p=0.1)
attn_weights = dropout(attn_weights)</code></code></pre> <p>In the code above, we create an instance of PyTorch's <code>nn.Dropout</code> module with a dropout probability of 0.1. We then apply the dropout to the <code>attn_weights</code> matrix, randomly setting 10% of the attention weights to zero and increasing the remaining values in the matrix by 10%.</p> <p>By incorporating causal attention and dropout regularization, we can effectively mask future tokens and improve the generalization ability of our self-attention-based models.</p> <p>To encapsulate the causal attention mechanism, we can define a <code>CausalAttention</code> class that inherits from the <code>SelfAttention</code> class and adds the masking and dropout functionality:</p> <pre><code><code>import torch.nn as nn

class CausalAttention(nn.Module):
    """
    Implements causal attention with dropout and masking.

    Args:
        d_in (int): Input embedding dimension.
        d_out (int): Output embedding dimension.
        context_length (int): Length of the context (number of tokens).
        dropout (float): Dropout rate. Default is 0.1.
        qkv_bias (bool): If True, adds a learnable bias to the Q, K, V projections. Default is False.
    """
    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):
        super().__init__()

        # Linear layers for K, Q, V projections
        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
            'mask',
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        """
        Forward pass for causal attention.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_tokens, d_in).

        Returns:
            torch.Tensor: Context vectors of shape (batch_size, num_tokens, d_out).
        """

        b, num_tokens, d_in = x.shape

        # Compute keys, queries, and values
        keys = self.W_keys(x)
        query = self.W_query(x)
        values = self.W_value(x)

        # Calculate attention scores
        att_scores = query @ keys.transpose(1, 2)  # Transpose to get (batch_size, num_tokens, num_tokens)

        # Apply mask to prevent attending to future tokens
        att_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)

        # Compute attention weights
        attn_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)

        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights)

        # Compute context vectors
        context_vec = attn_weights @ values

        return context_vec</code></code></pre> <p>The <code>CausalAttention</code> class takes additional arguments <code>context_length</code> and <code>dropout</code> to specify the maximum sequence length and the dropout probability, respectively. It also registers the mask as a buffer to ensure it is properly moved to the appropriate device along with the model.</p> <p>In the <code>forward</code> method, we apply the mask to the attention scores using <code>masked_fill</code>, ensuring that future tokens are ignored. We then apply the <strong>softmax</strong> function, perform dropout regularization, and compute the context vectors as before.</p> <p>By using the <code>CausalAttention</code> class, we can easily incorporate causal attention and dropout regularization into our self-attention-based models, enabling them to handle tasks that require masking future tokens.</p> <h2>Multi-Head Attention</h2> <p>Multi-head attention is an extension of the self-attention mechanism that allows the model to attend to different parts of the input sequence in multiple ways simultaneously. Instead of performing a single attention operation, multi-head attention splits the input embeddings into multiple smaller matrices (heads) and applies self-attention to each head independently. The results from all heads are then concatenated and linearly transformed to produce the final output.</p> <p>The motivation behind multi-head attention is to enable the model to capture different types of relationships and dependencies within the input sequence. Each head can focus on different aspects of the input, allowing the model to learn a more diverse and nuanced representation.</p> <p>Here's a step-by-step breakdown of the multi-head attention process:</p> <ul><li><p><strong>Splitting the Input Embeddings</strong>: The input embeddings are split into multiple smaller matrices, each representing a different head. The number of heads is a hyperparameter that can be tuned based on the specific task and model architecture. If the input embeddings have dimension <code>d_out</code> and there are <code>num_heads</code> heads, each head will have a dimension of <code>d_head = d_out // num_heads</code>.</p></li><li><p><strong>Applying Self-Attention to Each Head</strong>: For each head, the input embeddings are projected into query, key, and value matrices using separate linear transformations. The self-attention mechanism is then applied to each head independently, computing the attention scores, attention weights, and context vectors for each head.</p></li><li><p><strong>Concatenating the Head Outputs</strong>: The context vectors from all heads are concatenated along the embedding dimension to form a single matrix. This concatenated matrix has a dimension of <code>d_model</code>, which is the same as the original input embeddings.</p></li><li><p><strong>Linear Transformation</strong>: The concatenated matrix is passed through a final linear transformation to produce the output of the multi-head attention mechanism. This linear transformation allows the model to combine and mix the information from different heads.</p></li></ul> <pre><code><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, num_heads, context_length, dropout):
        super().__init__()
        
        assert (d_out % num_heads == 0), "d_out must be divisible by num_heads" 
        
        self.d_in = d_in
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        # Linear layers for Q, K, V projections
        self.W_query = nn.Linear(d_in, d_out)
        self.W_key = nn.Linear(d_in, d_out)
        self.W_value = nn.Linear(d_in, d_out)

&#9;&#9;# Linear layer to combine head outputs
        self.out_proj = nn.Linear(d_out, d_out)

&#9;&#9;# Dropout layer
        self.dropout = nn.Dropout(dropout)

&#9;&#9;# Upper triangular mask to prevent attending to future tokens
        self.register_buffer(
&#9;&#9;    "mask", 
&#9;&#9;&#9;torch.triu(torch.ones(context_length, context_length), diagonal=1)
&#9;&#9;)
        
    def forward(self, x):
    
        batch_size, num_tokens, d_in = x.shape

&#9;&#9;# Compute keys, queries, and values
        keys = self.W_key(x)      # (batch_size, num_tokens, d_out)
        queries = self.W_query(x) # (batch_size, num_tokens, d_out)
        values = self.W_value(x)  # (batch_size, num_tokens, d_out)


&#9;&#9;# Reshape to (batch_size, num_tokens, num_heads, head_dim)
        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)
        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)

&#9;&#9;# Transpose to (batch_size, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

&#9;&#9;# Calculate attention scores
        attn_scores = queries @ keys.transpose(2, 3) 
        # Shape: (batch_size, num_heads, num_tokens, num_tokens)

&#9;&#9;# Apply mask to prevent attending to future tokens
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)

&#9;&#9;# Compute attention weights
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        
        # Apply dropout to attention weights
        attn_weights = self.dropout(attn_weights) 


&#9;&#9;# Compute context vectors
&#9;&#9;context_vec = attn_weights @ values 
&#9;&#9;# Shape: (batch_size, num_heads, num_tokens, head_dim)

        # Reshape and combine heads
        context_vec = context_vec.transpose(1, 2) # (batch_size, num_tokens, num_heads, head_dim)
        context_vec = context_vec.contiguous().view(batch_size, num_tokens, -1)
        # (batch_size, num_tokens, d_out)
        
        # Apply final linear projection
&#9;&#9;context_vec = self.out_proj(context_vec) # (batch_size, num_tokens, d_out)

&#9;&#9;return context_vec
</code></code></pre> <p>The <code>__init__</code> method initializes the necessary parameters and modules for multi-head attention:</p> <ul><li><p>It takes the input dimension <code>d_in</code>, output dimension <code>d_out</code>, number of heads <code>num_heads</code>, context length <code>context_length</code>, and dropout probability <code>dropout</code>.</p></li><li><p>It asserts that <code>d_out</code> is divisible by <code>num_heads</code> to ensure proper splitting of dimensions.</p></li><li><p>It initializes the linear transformations for the query, key, and value matrices (<code>self.W_query</code>, <code>self.W_key</code>, <code>self.W_value</code>) and an additional output projection matrix (<code>self.out_proj</code>).</p></li><li><p>It also registers the causal mask as a buffer using <code>self.register_buffer()</code>.</p></li></ul> <p>The <code>forward</code> method performs the multi-head attention computation:</p> <ol><li><p>It applies the linear transformations to the input <code>x</code> to obtain the query, key, and value matrices (<code>queries</code>, <code>keys</code>, <code>values</code>).</p></li><li><p>It splits the matrices into multiple heads by reshaping and transposing the tensors. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>, where <code>head_dim</code> is <code>d_out // num_heads</code>.</p></li><li><p>It computes the attention scores by performing matrix multiplication between the queries and keys using the <code>@</code>operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, num_tokens)</code>.</p></li><li><p>It applies the causal mask to the attention scores using <code>masked_fill_()</code>, setting the upper triangular part to negative infinity. This ensures that each token can only attend to the tokens that appear before it in the sequence.</p></li><li><p>It applies the softmax function to the masked attention scores to obtain the attention weights. The scaling factor <code>keys.shape[-1]**0.5</code> is used to stabilize the gradients.</p></li><li><p>It computes the context vectors by multiplying the attention weights with the values using the <code>@</code> operator. The resulting shape is <code>(batch_size, num_heads, num_tokens, head_dim)</code>.</p></li><li><p>It transposes and reshapes the context vectors to <code>(batch_size, num_tokens, d_out)</code> to combine the outputs from all heads.</p></li><li><p>It applies the output projection matrix (<code>self.out_proj</code>) to the combined context vectors to obtain the final output.</p></li></ol> <p>The <code>MultiHeadAttention</code> class efficiently implements multi-head attention by performing the computations in a single pass. It takes advantage of tensor operations and reshaping to parallelize the computations across multiple heads.</p> <p>By using this efficient implementation, the model can capture different types of relationships and dependencies within the input sequence, allowing it to learn more expressive and nuanced representations for various natural language processing tasks.</p> <p>The <code>MultiHeadAttention</code> class can be used as a building block in larger models, such as the Transformer architecture, to leverage the power of multi-head attention in a computationally efficient manner.</p> <h2>The GPT Architecture: Harnessing the Power of Multi-Head Attention</h2> <p>The GPT (Generative Pre-trained Transformer) architecture, which includes models like GPT-2 and GPT-3, has revolutionized the field of natural language processing. At the core of the GPT architecture lies the multi-head attention mechanism, which enables the model to capture rich linguistic patterns and generate coherent and contextually relevant text.</p> <p>For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention heads and a context vector embedding size of 768. The largest GPT-2 model (1.5 billion parameters) has 25 attention heads and a context vector embedding size of 1,600. The embedding sizes of the token inputs and context embeddings are the same in GPT models (d_in = d_out).</p> <p>Here's a code snippet showcasing the initialization of the multi-head attention module in the GPT-2 architecture:</p> <pre><code><code>torch.manual_seed(123)

# Sample inputs for num_heads = 12, d_in = d_out = 768, context_length = 1024
batch = torch.rand(2, 1024, 768) 
# Two inputs with 1024 tokens each; each token has embedding dimension 768.

print(batch.shape)

batch_size, context_length, d_in = batch.shape
d_out = 768
num_heads = 12

mha = MultiHeadAttention(d_in, d_out, num_heads, context_length, 0.0)
context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
</code></code></pre> <h2>Conclusion</h2> <p>In this blog post, we have explored the concept of attention mechanisms and their significant impact on natural language processing tasks. Starting with a simplified version of self-attention and progressing to the more advanced and efficient multi-head attention, we have seen how attention allows models to selectively focus on relevant parts of the input sequence and capture long-range dependencies effectively. Self-attention enables each token to attend to every other token in the sequence, facilitating the learning of rich, context-dependent representations. Multi-head attention takes this a step further by allowing models to capture different types of relationships and dependencies simultaneously, enhancing their expressive power and ability to understand and generate natural language.</p> <p>Congrats on sticking with the blog and understanding the importance of attention in coding the GPT-style models from scratch. Implementing attention mechanisms, especially multi-head attention, is crucial for building the Transformer architecture. By delving into the details and implementing it efficiently, you&#8217;ve gained valuable insights into the core component that powers many top-notch NLP models.</p> <div><hr/></div> <p>Thanks for reading NeuraForge: AI Unleashed!</p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p> <p>Connect with me on <a href="https://www.linkedin.com/in/narasimhakarthik/">LinkedIn</a>. </p> <p></p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p> <p></p> <p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learn how to build and optimize attention mechanisms for transformer models, from basic self-attention to the multi-head attention architecture used in state-of-the-art language models]]></summary></entry><entry><title type="html">Example Jupyter Notebook Post - ML Model Training</title><link href="https://jnk234.github.io/blog/2025/example-jupyter-notebook/" rel="alternate" type="text/html" title="Example Jupyter Notebook Post - ML Model Training"/><published>2025-01-20T14:00:00+00:00</published><updated>2025-01-20T14:00:00+00:00</updated><id>https://jnk234.github.io/blog/2025/example-jupyter-notebook</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/example-jupyter-notebook/"><![CDATA[<p>This post demonstrates how to embed Jupyter notebooks directly into your blog posts. Perfect for sharing ML experiments, data analysis, and technical tutorials with executable code!</p> <h2 id="embedding-a-jupyter-notebook">Embedding a Jupyter Notebook</h2> <p>Below is an example Jupyter notebook embedded directly in this post:</p> <p>Note: Create your notebook at assets/jupyter/ml-example.ipynb to see it rendered here.</p> <h2 id="benefits-of-notebook-blog-posts">Benefits of Notebook Blog Posts</h2> <p>Using Jupyter notebooks in your blog posts allows you to:</p> <ol> <li><strong>Show code and results together</strong> - Readers see both input and output</li> <li><strong>Include visualizations</strong> - Plots and charts render beautifully</li> <li><strong>Maintain reproducibility</strong> - Share the exact notebook you used</li> <li><strong>Support both themes</strong> - Notebooks adapt to light/dark mode</li> </ol> <h2 id="how-to-add-your-own-notebooks">How to Add Your Own Notebooks</h2> <ol> <li>Create your notebook in Jupyter/Google Colab</li> <li>Save it to <code class="language-plaintext highlighter-rouge">assets/jupyter/your-notebook.ipynb</code></li> <li>Reference it in your blog post using the embedding code</li> <li>Build and preview your site</li> </ol> <p>For detailed instructions, see the README in the <code class="language-plaintext highlighter-rouge">_posts</code> folder.</p> <hr/> <p><em>This is an example post. Replace it with your own notebook-based content!</em></p>]]></content><author><name></name></author><category term="technical"/><category term="jupyter"/><category term="machine-learning"/><category term="tutorial"/><summary type="html"><![CDATA[Demonstrating how to embed Jupyter notebooks in blog posts]]></summary></entry><entry><title type="html">Welcome to NeuraForge - My AI/ML Journey</title><link href="https://jnk234.github.io/blog/2025/example-first-post/" rel="alternate" type="text/html" title="Welcome to NeuraForge - My AI/ML Journey"/><published>2025-01-15T14:00:00+00:00</published><updated>2025-01-15T14:00:00+00:00</updated><id>https://jnk234.github.io/blog/2025/example-first-post</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/example-first-post/"><![CDATA[<h2 id="welcome">Welcome!</h2> <p>This is an example first post for your blog. Feel free to edit or delete this file.</p> <p>Im excited to share my journey in AI and Machine Learning through this blog. Here, Ill be writing about:</p> <ul> <li><strong>LLM Development</strong>: Insights from working with GPT, Claude, and other language models</li> <li><strong>Research Updates</strong>: Findings from my work at Northwesterns CCL Lab</li> <li><strong>Technical Tutorials</strong>: Practical guides on implementing AI solutions</li> <li><strong>Industry Perspectives</strong>: Lessons learned from my time at Boeing and upcoming internship at Relativity</li> </ul> <h2 id="what-to-expect">What to Expect</h2> <p>Ill be sharing:</p> <ul> <li>Code snippets and implementations</li> <li>Research paper summaries</li> <li>Project walkthroughs</li> <li>Tips for AI/ML practitioners</li> </ul> <p>Stay tuned for more content!</p> <hr/> <p><em>Feel free to modify or remove this example post when youre ready to add your own content.</em></p>]]></content><author><name></name></author><category term="personal"/><category term="introduction"/><category term="AI"/><category term="machine-learning"/><summary type="html"><![CDATA[Introduction to my blog about AI, Machine Learning, and building intelligent systems]]></summary></entry><entry><title type="html">The Ultimate Guide to Preparing Text Data for Language Modeling with PyTorch</title><link href="https://jnk234.github.io/blog/2025/the-ultimate-guide-to-preparing-text-data-for-language-modeling-with-pytorch/" rel="alternate" type="text/html" title="The Ultimate Guide to Preparing Text Data for Language Modeling with PyTorch"/><published>2025-01-06T20:58:53+00:00</published><updated>2025-01-06T20:58:53+00:00</updated><id>https://jnk234.github.io/blog/2025/the-ultimate-guide-to-preparing-text-data-for-language-modeling-with-pytorch</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/the-ultimate-guide-to-preparing-text-data-for-language-modeling-with-pytorch/"><![CDATA[<h2>Introduction</h2> <p>When working with large language models (LLMs), one of the most crucial steps is preparing the textual data in a format that these models can understand and learn from. This process involves converting raw text into numerical vectors, known as embeddings, as LLMs cannot directly process plain text.</p> <p>In this post, we'll take a deep dive into the techniques and best practices for text preprocessing and embedding generation using PyTorch, a popular deep learning framework. We'll cover everything from basic tokenization to implementing advanced algorithms like Byte Pair Encoding (BPE), creating efficient data sampling techniques, and building embedding layers from scratch. By the end, you'll have a solid understanding of how to prepare text data for training powerful language models. To explore the concepts further and see the code in action, check out the accompanying Colab notebook <a href="https://colab.research.google.com/drive/1n6UjZRTRP0yRdcvJLHK2CRfNL8Gj_-IL?usp=sharing">here</a> and follow along with the step-by-step examples.</p> <p>Let's get started!</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="5472" height="3648" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3648,&quot;width&quot;:5472,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;open book lot&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="open book lot" title="open book lot" srcset="https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1457369804613-52c61a468e7d?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw3fHxyYW5kb218ZW58MHx8fHwxNzM2MTM1NDgyfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Patrick Tomasso</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h2>Understanding Embeddings: The Bridge Between Text and Mathematics</h2> <p>Before we dive into the technical implementation details, let's understand what embeddings are and why they're crucial for language models. Think of embeddings as a way to translate words into numbers &#8211; but not just any numbers. They're carefully crafted numerical representations that capture the meaning, relationships, and context of words in a way that computers can process.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!m1KP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png" width="1100" height="566" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:566,&quot;width&quot;:1100,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:113900,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m1KP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 424w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 848w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1272w, https://substackcdn.com/image/fetch/$s_!m1KP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc041df20-ad0b-472e-b911-cf0a96bc095d_1100x566.png 1456w" sizes="100vw"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">Embedding representations</a></em></figcaption></figure></div> <h3>What Are Embeddings and Why Do We Need Them?</h3> <p>At their core, embeddings are dense vectors (arrays of numbers) that represent words or tokens in a continuous vector space. When you feed the word "cat" to a computer, you can't just use the letters "c-a-t" - computers need numbers to perform calculations. An embedding transforms "cat" into a vector like [0.2, -0.5, 0.8, ...], where each number helps represent different aspects of the word's meaning.</p> <p>What makes embeddings powerful is their ability to capture semantic relationships. Words with similar meanings end up having similar numerical representations. For example, the embeddings for "cat" and "kitten" would be more similar to each other than to the embedding for "submarine". This similarity can be measured mathematically, allowing models to understand relationships between words.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!UPsT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:269534,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UPsT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!UPsT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c1fdb5-5837-4b36-8112-35290ab448ad_1920x1080.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: <a href="https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings">Representation of embedding vectors in 2D</a></figcaption></figure></div> <p>Modern embedding systems typically represent words in high-dimensional spaces. For example:</p> <ul><li><p>GPT-2 uses 768-dimensional embeddings for its smallest model</p></li><li><p>GPT-3's largest model uses 12,288-dimensional embeddings</p></li><li><p>BERT-base uses 768-dimensional embeddings</p></li></ul> <p>The real power of embeddings comes from their ability to learn from data. During model training, these embeddings are automatically adjusted to capture relationships present in the training data, adapting to specific domains and discovering nuanced patterns that might not be obvious to human designers.</p> <h2>Text Tokenization and Preprocessing Techniques</h2> <p>The first step in preparing text for LLMs is <strong>tokenization</strong> - breaking down raw text into smaller units called tokens. Tokens can be individual words, subwords, or even characters. The goal is to create a finite set of meaningful units that the model can learn from.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!rt5h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png" width="862" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:862,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109042,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rt5h!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 424w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 848w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1272w, https://substackcdn.com/image/fetch/$s_!rt5h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aace375-487b-42f7-8af5-32fd77a9111a_862x529.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></figcaption></figure></div> <p>However, raw text often contains noise and inconsistencies that can hinder the tokenization process. These include:</p> <ul><li><p>Inconsistent casing (e.g., "Hello" vs "hello")</p></li><li><p>Punctuation attached to words (e.g., "world!")</p></li><li><p>Special characters and contractions (e.g., "don't", "U.S.A.")</p></li><li><p>Unknown or rare words</p></li></ul> <p>To handle these issues and perform effective tokenization, we can use a combination of text preprocessing techniques and regular expressions in Python. Here's an example code snippet that demonstrates this:</p> <pre><code><code>import re

UNK = '&lt;unk&gt;'  # Token for unknown words
EOS = '&lt;eos&gt;'  # Token for end of text

def tokenize(text, known_words):
    # Lowercase the text
    text = text.lower()

    # Split on whitespace and punctuation using regular expressions
    tokens = re.findall(r"\w+|[^\w\s]", text)

    # Replace unknown words with &lt;unk&gt; token
    tokens = [t if t in known_words else UNK for t in tokens]

    # Append &lt;eos&gt; token to the end of the text
    tokens.append(EOS)

    return tokens

# Example usage
text = "Hello, world! This is a sample sentence."
known_words = {'this', 'is', 'a', 'sample', 'sentence'}

print(tokenize(text, known_words))</code></code></pre> <pre><code>['&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', 'this', 'is', 'a', 'sample', 'sentence', '&lt;unk&gt;', '&lt;eos&gt;']</code></pre> <p>Let's break down the tokenization process step by step:</p> <ol><li><p>First, we convert the entire text to lowercase using <code>text.lower()</code>. This ensures consistent casing across all words.</p></li><li><p>Next, we use a regular expression <code>r"\w+|[^\w\s]"</code> to split the text on whitespace and punctuation. The regex pattern <code>\w+</code> matches one or more word characters , while <code>[^\w\s]</code> matches any single character that is not a word character or whitespace. This effectively separates words and punctuation into individual tokens.</p></li><li><p>We then replace any unknown words (i.e., words not in the <code>known_words</code> set) with a special <code>&lt;unk&gt;</code> token. This helps the model handle out-of-vocabulary words gracefully during training and inference.</p></li><li><p>Finally, we append an <code>&lt;eos&gt;</code> token to the end of the tokenized text to mark the end of the sequence. This is useful for the model to learn when a text or document ends.</p></li></ol> <h2>Understanding and Implementing Byte Pair Encoding</h2> <p>While the tokenization approach we discussed so far works well for many cases, it has some limitations. One major drawback is the handling of unknown or rare words. Replacing all uncommon words with a generic <code>&lt;unk&gt;</code> token can lead to loss of information and hinder the model's ability to understand the nuances of the text.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Z7y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png" width="602" height="279" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:279,&quot;width&quot;:602,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:40598,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Z7y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 424w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 848w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1272w, https://substackcdn.com/image/fetch/$s_!_Z7y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b715e8-c5bc-4e8c-aa00-8096e1037e83_602x279.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em>...</figcaption></figure></div> <p>This is where Byte Pair Encoding (BPE) comes into play. BPE is a <strong>subword tokenization algorithm</strong> that iteratively builds a vocabulary of subword units based on their frequency in the training corpus. It starts with individual characters and progressively merges them into larger subword units until a desired vocabulary size is reached. This allows BPE to effectively handle out-of-vocabulary words by representing them as combinations of subword units.</p> <p>Let's walk through a step-by-step example to better understand how BPE constructs its vocabulary. Imagine we have the following list of words:</p> <pre><code><code>['low', 'lower', 'newest', 'widest']</code></code></pre> <ul><li><p>Step 1: Initialization</p></li></ul> <p>BPE begins by splitting each word into individual characters and appending a special end-of-word symbol, typically denoted by <code>&lt;/w&gt;</code>, to mark the end of each word. This initial segmentation looks like this:</p> <pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre> <ul><li><p>Step 2: Frequency Counting</p></li></ul> <p>Next, BPE counts the frequency of each character pair in the corpus. In this example, the most frequent pair is <code>e</code> followed by <code>&lt;/w&gt;</code>, as it appears in two words: <code>lower</code> and <code>newest</code>:</p> <ul><li><p>Step 3: Merging</p></li></ul> <p>BPE merges the most frequent pair into a new subword unit. In our example, <code>e&lt;/w&gt;</code> becomes a single unit i.e. considered as single token in vocabulary:</p> <pre><code><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w e s t&lt;/w&gt;', 'w i d e s t&lt;/w&gt;']</code></code></pre> <ul><li><p>Step 4: Iteration</p></li></ul> <p>The process of frequency counting and merging is repeated iteratively. In the next iteration, the most frequent pair is <code>es</code> followed by <code>&lt;/w&gt;</code>, so they get merged:</p> <pre><code>['l o w&lt;/w&gt;', 'l o w e r&lt;/w&gt;', 'n e w es&lt;/w&gt;', 'w i d es&lt;/w&gt;']</code></pre> <p>This iterative process continues until one of two conditions is met:</p> <ol><li><p>A desired vocabulary size is reached (e.g., 10,000 subword units).</p></li><li><p>No more frequent pairs are found (i.e., all possible merges have been performed).</p></li></ol> <p>The resulting set of subword units, along with their frequencies, forms the final BPE vocabulary. To further illustrate how BPE handles out-of-vocabulary words, let's consider an example. Suppose we have a BPE vocabulary that includes the subword units <code>low</code>, <code>est</code>, and <code>&lt;/w&gt;</code>, but not the word <code>lowest</code>. When encountering <code>lowest</code>, BPE would break it down into the known subword units:</p> <pre><code>['low', 'est', '&lt;/w&gt;']</code></pre> <p>By representing <code>lowest</code> as a combination of subword units, BPE enables the model to process and generate words it hasn't seen during training.</p> <h3><br/>Byte Pair Encoding in Python</h3> <p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library. <code>tiktoken</code> is an existing Python open source library (<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>), which implements the BPE algorithm very efficiently based on source code in Rust. It can be installed as follows:</p> <p>Now, let's see how we can implement BPE in Python using the <code>tiktoken</code> library:</p> <pre><code>pip install tiktoken</code></pre> <pre><code><code>import tiktoken

# Load the BPE tokenizer 
bpe_tokenizer = tiktoken.get_encoding("gpt2")

text = "This is an example of byte pair encoding! xhsbfubs"

# Tokenize the text using BPE
tokens = bpe_tokenizer.encode(text) 
decoded = bpe_tokenizer.decode(tokens) 

print(f"Encoded tokens: {tokens}") 
print(f"Decoded text: {decoded}")

# Encoded tokens: [1212, 318, 281, 1672, 286, 18022, 5166, 21004, 0, #  # 2124, 11994, 19881, 23161] 
# Decoded text: This is an example of byte pair encoding! xhsbfubs
</code></code></pre> <p>The <code>tiktoken</code> library provides an efficient implementation of the BPE algorithm used by OpenAI's GPT models. We first load the BPE tokenizer with <code>tiktoken.get_encoding("gpt2")</code>, which gives us access to the same tokenizer used by the GPT-2 model.</p> <p>We then encode our text using <code>bpe_tokenizer.encode(text)</code>, which applies the BPE algorithm and returns a list of token IDs. These IDs correspond to the subwords in the BPE vocabulary.</p> <p>Finally, we can decode the token IDs back into the original text using <code>bpe_tokenizer.decode(tokens)</code>. This demonstrates that BPE can effectively tokenize and reconstruct the text without losing information.</p> <p>The real power of BPE lies in its ability to handle out-of-vocabulary words. Since it breaks down words into subwords, even if a word is not explicitly present in the vocabulary, it can still be represented by a combination of subwords. This allows the model to understand and generate words it hasn't seen during training. By understanding and implementing Byte Pair Encoding, you can take your text preprocessing to the next level and build more powerful and versatile language models.</p> <h2>Creating and Managing Sampling Windows</h2> <p>Now that we have our text data tokenized into a sequence of token IDs, the next step is to prepare it for training our language model. But how exactly do we feed this data to the model?</p> <p>To answer that, let's first understand how language models like GPT learn. During training, the model tries to predict the next token in a sequence given the tokens that come before it. For example, if the input is "The cat sat on the", the model learns to predict the next most likely token, such as "mat" or "couch".</p> <p>To facilitate this learning process, we need to create input-target pairs from our tokenized text. The input will be a sequence of tokens, and the target will be the next token that follows this sequence. We can generate these pairs using a technique called <strong>sampling windows</strong>. The sampling window is popularly also known as <strong>context length</strong>.</p> <p>Imagine our tokenized text as a long ribbon. We take a small window of a fixed size (say, 50 tokens) and slide it over the ribbon. At each step, the tokens inside the window become our input, and the token immediately following the window becomes the target. We keep sliding the window until we reach the end of the ribbon.</p> <p>Here's a visual representation:</p> <pre><code><code>[The, cat, sat, on, the, mat, ., &lt;eos&gt;]
 |   window 1    |
      |   window 2    |
           |   window 3    |</code></code></pre> <p>In window 1, the input is <code>[The, cat, sat, on, the]</code> and the target is <code>mat</code>. In window 2, the input is <code>[cat, sat, on, the, mat]</code> and the target is <code>.</code>. And so on.</p> <p>By creating these sampling windows, we break down our long text into manageable sequences that the model can learn from. The size of the window is a hyperparameter that we can tune. A larger window allows the model to learn from more context, but it also increases the computational complexity.</p> <p>Now, let's see how we can implement this in Python. We'll use PyTorch's <code>Dataset</code> and <code>DataLoader</code> classes to create an efficient data pipeline.</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, tokens, window_size):
        # Store the tokenized text
        self.tokens = tokens  
        # Store the size of the sampling window
        self.window_size = window_size  

    def __len__(self):
        # Return the total number of sampling windows
        return len(self.tokens) - self.window_size

    def __getitem__(self, idx):
        # Get the input-target pair for the given index
        input_seq = self.tokens[idx:idx+self.window_size]  # Input sequence
        target_seq = self.tokens[idx+1:idx+self.window_size+1]  # Target sequence (shifted by 1)
        return torch.tensor(input_seq), torch.tensor(target_seq)

# Example usage
tokens = [1212, 318, 281, 1672, 286, 2419, 683, 26254, 0] # Tokenized text
dataset = TextDataset(tokens, window_size=5)  # Create a TextDataset with window size of 5
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # Create a DataLoader with batch size of 2 and shuffling enabled

for inputs, targets in dataloader:
    print(inputs)  # Print the input sequences
    print(targets)  # Print the corresponding target sequences
    break  # Break after the first batch (for demonstration purposes)
</code></code></pre> <pre><code>tensor([[ 1672, 286, 2419, 683, 26254], 
&#9;&#9;[ 1212, 318, 281, 1672, 286]]) 

tensor([[ 286, 2419, 683, 26254, 0], 
&#9;&#9;[ 318, 281, 1672, 286, 2419]])</code></pre> <p>Let's break this down step by step:</p> <ol><li><p>We define a custom <code>TextDataset</code> class that inherits from PyTorch's <code>Dataset</code> class. This class takes the tokenized text and the window size as input.</p></li><li><p>The <code>__len__</code> method returns the total number of sampling windows we can create from the tokenized text. We subtract the window size to avoid going out of bounds.</p></li><li><p>The <code>__getitem__</code> method is the heart of the dataset. It takes an index <code>idx</code> and returns the input-target pair for the corresponding sampling window. The input is <code>tokens[idx:idx+window_size]</code> and the target is <code>tokens[idx+1:idx+window_size+1]</code>, i.e., the input sequence shifted by one token.</p></li><li><p>We then create an instance of the <code>TextDataset</code> with our tokenized text and a window size of 5.</p></li><li><p>We wrap the dataset in a <code>DataLoader</code>, which allows us to batch the data and shuffle it for training. Here, we use a batch size of 2.</p></li><li><p>Finally, we loop over the dataloader to get batches of input-target pairs. Each input is a tensor of shape <code>(batch_size, window_size)</code>, and each target is a tensor of shape <code>(batch_size, window_size)</code>.</p></li></ol> <p>Now consider the following code which provides the best practice for creating datasets and dataloaders:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class GPTDataset(Dataset):
    def __init__(self, text, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []
        
        # Tokenize entire text
        token_ids = tokenizer.encode(text)
        
        # Create overlapping sequences
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1:i + max_length + 1]
            
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))
            
    def __len__(self):
        return len(self.input_ids)
        
    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader(text, batch_size=4, max_length=256, stride=128):
    """Create an efficient data loader for training"""
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDataset(text, tokenizer, max_length, stride)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    return dataloader
</code></code></pre> <h2>Building Token Embeddings from Scratch</h2> <p>So far, we've seen how to preprocess text data and convert it into sequences of token IDs using techniques like tokenization and Byte Pair Encoding. The next crucial step is to transform these discrete token IDs into continuous vector representations, known as <strong>embeddings</strong>.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8wV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png" width="1372" height="966" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:966,&quot;width&quot;:1372,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41167,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8wV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 424w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 848w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1272w, https://substackcdn.com/image/fetch/$s_!8wV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f00bd2b-79da-43c5-85a2-d45a168ac154_1372x966.png 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://machinelearningmastery.com/wp-content/uploads/2022/01/PE6.png</figcaption></figure></div> <p>Embeddings are dense, low-dimensional vectors that capture semantic and syntactic information about the tokens. By representing tokens as embeddings, we enable the language model to learn meaningful relationships and patterns in the text data.</p> <p>In PyTorch, we can create embeddings using the <code>nn.Embedding</code> layer. This layer maps each token ID to a corresponding vector of a specified size.</p> <p>Here's an example of how to create an embedding layer in PyTorch:</p> <pre><code><code>import torch
import torch.nn as nn

vocab_size = 10000  # Size of the vocabulary (number of unique tokens)
embed_size = 128  # Dimensionality of the embedding vectors

embedding_layer = nn.Embedding(vocab_size, embed_size)
</code></code></pre> <p>In this code snippet, we define an embedding layer with a vocabulary size of 10,000 and an embedding size of 128. This means that each token ID will be mapped to a 128-dimensional vector.</p> <p>To use the embedding layer, we simply pass the token IDs through it:</p> <pre><code><code>token_ids = torch.tensor([1, 2, 3, 4])  # Example token IDs
embeddings = embedding_layer(token_ids)

print(embeddings.shape)  

# Output: torch.Size([4, 128])
</code></code></pre> <p>Here, we pass a tensor of token IDs through the embedding layer, and it returns the corresponding embeddings. The resulting <code>embeddings</code> tensor has a shape of <code>(4, 128)</code>, indicating that we have 4 tokens, each represented by a 128-dimensional vector.</p> <p>But how does the embedding layer know what values to assign to each token's embedding vector? Initially, the embedding layer is <strong>randomly</strong> initialized. During the training process, the language model learns to adjust these embeddings based on the patterns and relationships in the text data.</p> <p>However, the token embeddings each word independently, regardless of its position. This is where positional embeddings come in &#8211; they help the model understand where each word appears in the sequence.</p> <h3>Adding Positional Embeddings</h3> <p>The self-attention mechanism in transformer models is inherently position-agnostic. When looking at token embeddings alone, the model has no way to know whether "cat" appears at the beginning, middle, or end of the sentence. Positional embeddings solve this by adding position-specific information to each token embedding.</p> <p>Think of it this way: if token embeddings tell us "what" the word is, positional embeddings tell us "where" it appears. When we combine them, the model gets both pieces of information simultaneously.</p> <h4>Implementing Positional Embeddings</h4> <p>Let's implement a complete embedding system that combines both token and positional embeddings. We choose <code>max_sequence_length</code> based on how long our input sequences might be:</p> <pre><code><code># Define max_sequence_length as 512
max_sequence_length&nbsp;=&nbsp;512&nbsp;

# Create positional embedding layer
position_embedding&nbsp;=&nbsp;nn.Embedding(max_sequence_length,&nbsp;embed_size)
</code></code></pre> <p>Now, generate position indices for our sequence:</p> <pre><code><code># If our token_ids has length 4, we need positions [0, 1, 2, 3]&nbsp;
positions&nbsp;=&nbsp;torch.arange(len(token_ids))&nbsp;
print(f"Position indices:&nbsp;{positions}")

# Output: Position indices: tensor([0, 1, 2, 3])</code></code></pre> <p>Now get the embeddings for these positions or indices:</p> <pre><code><code># Get embeddings from position_embeddings layer
position_embeddings&nbsp;=&nbsp;position_embedding(positions)&nbsp;
print(f"Position embedding shape:&nbsp;{position_embeddings.shape}")

# Output: Position embedding shape: torch.Size([4, 128])</code></code></pre> <p>Now combine both token embeddings and positional embeddings:</p> <pre><code><code>combined_embeddings = embeddings + position_embeddings
print(f"Combined embedding shape: {combined_embeddings.shape}")

# Output: Combined embedding shape: torch.Size([4, 128])</code></code></pre> <h3>Implementing Embeddings (Best Practise)</h3> <p>Let's implement a complete embedding system that combines both token and positional embeddings:</p> <pre><code><code># Best practices implementation
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_sequence_length):
        super().__init__()
        
        # Initialize embeddings with proper scaling
        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.position_embedding = nn.Embedding(max_sequence_length, embedding_dim)
        
        # Initialize weights using normal distribution
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.position_embedding.weight, std=0.02)
        
    def forward(self, token_ids):
        # Apply scaling to token embeddings
        token_embeddings = self.token_embedding(token_ids) * self.scale
        
        # Create and cache position indices
        if not hasattr(self, '_position_ids'):
            self._position_ids = torch.arange(
                token_ids.size(1), 
                device=token_ids.device
            )
        
        # Add positional embeddings
        return token_embeddings + self.position_embedding(self._position_ids)</code></code></pre> <p>Let's break down how this works:</p> <ol><li><p><strong>Token Embeddings</strong>: Each word gets transformed into a dense vector through the <code>token_embedding</code> layer, just as we discussed earlier.</p></li><li><p><strong>Position Numbers</strong>: We create a sequence of position indices (0, 1, 2, ...) for each position in our input sequence.</p></li><li><p><strong>Position Embeddings</strong>: These indices get transformed into position-specific vectors through the <code>position_embedding</code> layer.</p></li><li><p><strong>Combination</strong>: We add the token and positional embeddings together. This addition operation preserves both the meaning of the word (from token embeddings) and its position (from positional embeddings).</p></li></ol> <h2>Wrapping Up</h2> <p>In this comprehensive guide, we've explored the fundamental building blocks of text preprocessing for language modeling. We started by diving into tokenization techniques, learning how to break down raw text into meaningful units while handling challenges like punctuation, casing, and special characters. Next, we discovered the power of Byte Pair Encoding (BPE) for creating subword vocabularies that effectively handle rare and unknown words. We then learned how to construct efficient sampling windows to prepare tokenized text for training, and finally, we built token embeddings from scratch using PyTorch, incorporating positional information to capture word order and context.</p> <p>Remember, the techniques and concepts we've discussed are not just theoretical - they have practical applications in a wide range of natural language processing tasks, such as language translation, text summarization, sentiment analysis, and more. By mastering these fundamentals, you'll be equipped to tackle complex language modeling challenges and build impressive AI systems.</p> <div><hr/></div> <p>Thanks for reading NeuraForge: AI Unleashed!</p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p> <p></p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p> <p></p> <h1></h1> <p></p> <p></p> <p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Master tokenization, Byte Pair Encoding, Sampling windows, and Embeddings]]></summary></entry><entry><title type="html">PyTorch in Practice: Essential Building Blocks for Modern Deep Learning</title><link href="https://jnk234.github.io/blog/2025/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning/" rel="alternate" type="text/html" title="PyTorch in Practice: Essential Building Blocks for Modern Deep Learning"/><published>2025-01-02T03:49:50+00:00</published><updated>2025-01-02T03:49:50+00:00</updated><id>https://jnk234.github.io/blog/2025/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning</id><content type="html" xml:base="https://jnk234.github.io/blog/2025/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning/"><![CDATA[<h4>From Tensors to Neural Networks: Understanding Core Components</h4> <h3>Introduction</h3> <p>As deep learning continues to advance artificial intelligence applications, PyTorch has established itself as a fundamental framework powering everything from computer vision systems to large language models. Originally developed by Metas AI Research lab, PyTorch combines Pythons flexibility with deep learning capabilities through a powerful, intuitive interface.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MjgGftC7-d5x2mD7"/><figcaption>Photo by <a href="https://neuraforge.substack.com/p/true">Iker Urteaga</a> on<a href="https://unsplash.com/">Unsplash</a></figcaption></figure> <h4>Core Components ofPyTorch</h4> <p>PyTorchs architecture rests on three key components that work together to enable efficient deep learning development:</p> <ol><li><strong>Dynamic TensorLibrary</strong></li></ol> <ul><li>Extends NumPys array programming capabilities</li><li>Provides seamless CPU and GPU acceleration</li><li>Implements efficient mathematical operations for deep learning computations</li></ul> <p>2. <strong>Automatic Differentiation Engine (Autograd)</strong></p> <ul><li>Computes gradients automatically through computational graphs</li><li>Manages backpropagation for neural networktraining</li></ul> <p>3. <strong>Deep Learning Framework</strong></p> <ul><li>Delivers modular neural network components</li><li>Implements optimized loss functions and optimizers</li></ul> <h3>Getting Started withPyTorch</h3> <h4>Installation andSetup</h4> <p>PyTorch can be installed directly using pip, Pythons package installer:</p> <pre>pip install torch</pre> <p>However, for optimal performance, its recommended to install the version specifically compatible with your systems hardware. Visit <a href="https://pytorch.org/">pytorch.org</a> to get the appropriate installation command based onyour:</p> <ul><li>Operating system</li><li>Package manager preference (pip/conda)</li><li>CUDA version (for GPUsupport)</li><li>Python version</li></ul> <h4>GPU Support and Compatibility</h4> <p>PyTorch seamlessly integrates with NVIDIA GPUs through CUDA. To verify GPU availability in your environment:</p> <pre><br /># Check GPU availability<br />gpu_available = torch.cuda.is_available()<br />print(f&quot;GPU Available: {gpu_available}&quot;)<br /><br /># Get GPU device count if available<br />if gpu_available:<br />    print(f&quot;Number of GPUs: {torch.cuda.device_count()}&quot;)</pre> <p>If a GPU is detected, you can move tensors and models to GPU memoryusing:</p> <pre># Create a tensor<br />tensor = torch.tensor([1.0, 2.0, 3.0])<br /><br /># Move to GPU if available<br />device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;<br />tensor = tensor.to(device)</pre> <h4>Apple SiliconSupport</h4> <p>For users with Apple M1/M2/M3 chips, PyTorch provides acceleration through the Metal Performance Shaders (MPS) backend. Verify MPS availability:</p> <pre><br /># Check MPS (Metal Performance Shaders) availability<br />mps_available = torch.backends.mps.is_available()<br />print(f&quot;MPS Available: {mps_available}&quot;)<br /><br /># If MPS is available, you can use it as device<br />if mps_available:<br />    device = torch.device(&quot;mps&quot;)<br />    # Move tensors/models to MPS device<br />    tensor = tensor.to(device)</pre> <p>For ease of usage, I recommend using <a href="https://colab.research.google.com/">Google Colab</a> i.e. a popular jupyter notebooklike environment, which provides time-limited access toGPUs.</p> <h3>Understanding Tensors</h3> <h4>What AreTensors?</h4> <p>Tensors are mathematical objects that generalize vectors and matrices to higher dimensions. In PyTorch, tensors serve as fundamental data containers that hold and process multi-dimensional arrays of numerical values. These containers enable efficient computation and automatic differentiation, making them essential for deep learning operations. PyTorch tensors are similar to Numpy arrays in basicsense.</p> <h4>Scalers, Vectors, Matrices andTensors</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/0*HVGh8DKM7kouoN-m.png"/></figure> <p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorchs `Tensor` class using the `torch.tensor` function as shown in the following listing.</p> <pre>import torch<br /><br /># Scalar (0-dimensional tensor)<br />scalar = torch.tensor(1)     <br /><br /># Vector (1-dimensional tensor)<br />vector = torch.tensor([1, 2, 3])    <br /><br /># Matrix (2-dimensional tensor)<br />matrix = torch.tensor([[1, 2], <br />                      [3, 4]])     <br /><br /># 3-dimensional tensor<br />tensor3d = torch.tensor([[[1, 2], [3, 4]], <br />                        [[5, 6], [7, 8]]])</pre> <p>Each tensor type maintains its specific dimensionality, accessible through the<strong>.shape</strong> attribute:</p> <pre>print(f&quot;Scalar shape: {scalar.shape}&quot;)      # torch.Size([])<br />print(f&quot;Vector shape: {vector.shape}&quot;)      # torch.Size([3])<br />print(f&quot;Matrix shape: {matrix.shape}&quot;)      # torch.Size([2, 2])<br />print(f&quot;3D tensor shape: {tensor3d.shape}&quot;) # torch.Size([2, 2, 2])</pre> <h4>Tensor Data Types and Precision</h4> <p>PyTorch supports various data types with different precision levels, optimized for different computational needs:</p> <p>Some of the common torch datatypes available with torch are float32, float64, float16, bfloat16, int8, uint8, int16, int32,int64.</p> <p>The choice of precision impacts both memory usage and computational efficiency:</p> <ul><li>float32: Standard for most deep learningtasks</li><li>float16: Reduced precision, useful for memory optimization</li><li>bfloat16: Brain Floating Point, balances precision andrange</li></ul> <h4>Floating DataTypes</h4> <p>PyTorch supports various floating-point precisions for tensors, each serving different computational needs:</p> <ul><li>torch.float32 (default): 32-bit precision offering 6-9 decimal places, optimal for most deep learningtasks</li><li>torch.float64: 64-bit double precision with 15-17 decimal places, suitable for high-precision numerical computations</li><li>torch.float16: 16-bit half precision with 3-4 decimal places, useful for memory-efficient operations</li><li>torch.bfloat16: Brain floating point format with 2-3 decimal precision, balancing range and precision</li></ul> <pre>import torch<br /><br />float32_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  <br />float64_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)  <br />float16_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16)  <br />bfloat16_tensor = torch.tensor([1.0, 2.0], dtype=torch.bfloat16)</pre> <h4>Integer Types</h4> <p>PyTorch supports various integer data types, each with specific memory allocations and valueranges:</p> <ul><li>int8: 8-bit signed integers (-128 to127)</li><li>uint8: 8-bit unsigned integers (0 to255)</li><li>int16: 16-bit signed integers (-32768 to32767)</li><li>int32: 32-bit signed integers (-2^31 to2^31-1)</li><li>int64: 64-bit signed integers (-2^63 to 2^63-1), default integer type inPyTorch</li></ul> <pre>int8_tensor = torch.tensor([1, 2], dtype=torch.int8)     <br />uint8_tensor = torch.tensor([1, 2], dtype=torch.uint8)   <br />int16_tensor = torch.tensor([1, 2], dtype=torch.int16)   <br />int32_tensor = torch.tensor([1, 2], dtype=torch.int32)   <br />int64_tensor = torch.tensor([1, 2], dtype=torch.int64)</pre> <h4>Datatype Conversion</h4> <p>We can convert tensors from one datatype to another using the.tomethod.</p> <pre># Converting between data types<br />tensor = torch.tensor([1, 2, 3])<br /><br />float_tensor = tensor.to(torch.float32) # Convert from int64 to float32<br />int_tensor = tensor.to(torch.int32)     # Convert from float32 to int32</pre> <h3>Common Tensor Operations</h3> <p>PyTorch provides several fundamental tensor operations essential for deep learning computations. Here are the key operations with their implementations and specific usecases.</p> <h4>1. Tensor Creation and Shape Manipulation</h4> <p>Creating tensors and understanding their shape are fundamental operations inPyTorch:</p> <pre>import torch<br /><br /># Create 2D tensor<br />tensor2d = torch.tensor([[1, 2, 3], <br />                        [4, 5, 6]])<br /><br /># Check tensor shape<br />shape = tensor2d.shape  <br /># Returns: torch.Size([2, 3])</pre> <p>For the above tensor, the shape if 2 x 3 i.e. 2 rows and 3 columns. We can change the shape of the array by maintaining the total size of the array using reshapemethod.</p> <h4>2. Reshaping Operations</h4> <p>PyTorch offers two methods for tensor reshaping:</p> <pre># Reshape tensor from (2,3) to (3,2)<br />reshaped_tensor = tensor2d.reshape(3, 2)<br /><br /># Alternative using view<br />viewed_tensor = tensor2d.view(3, 2)</pre> <p><strong>Technical Note</strong>:.view() and.reshape() differ in memory handling:</p> <ul><li>.view(): Requires contiguous memorylayout</li><li>.reshape(): Works with any memory layout, performs copy if necessary</li></ul> <h4>3. Matrix Operations</h4> <p>PyTorch implements efficient matrix operations essential for linear algebra computations:</p> <pre># Transpose operation<br />transposed = tensor2d.T<br /><br /># Matrix multiplication methods<br />result1 = tensor2d.matmul(tensor2d.T)  # Using matmul<br />result2 = tensor2d @ tensor2d.T        # Using @ operator</pre> <p>Output shapes for a 2x3 inputtensor:</p> <ul><li>Transpose: 3x2</li><li>Matrix multiplication with transpose: 2x2</li></ul> <p>These operations form the foundation for neural network computations and linear algebra operations in deep learning models. For an exhaustive list of tensor operations, refer to the <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>.</p> <h3>Automatic Differentiation</h3> <h4>Understanding Computational Graphs</h4> <p>PyTorch builds computational graphs that track operations performed on tensors. These graphs enable automatic differentiation through the autogradsystem, making gradient computation efficient and programmatic.</p> <p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural networkwe will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p> <p>Consider the following example of a single layer neural network performing logistic regression with single weight andbias.</p> <pre>import torch<br />import torch.nn.functional as F<br /><br /># Initialize inputs and parameters<br />y = torch.tensor([1.0])           # Target<br />x1 = torch.tensor([1.1])          # Input<br />w1 = torch.tensor([2.2], <br />                  requires_grad=True)  # Weight<br />b = torch.tensor([0.0], <br />                 requires_grad=True)   # Bias<br /><br /># Forward pass computation<br />z = x1 * w1 + b                   # Linear computation<br />a = torch.sigmoid(z)              # Activation<br />loss = F.binary_cross_entropy(a, y)    # Loss computation</pre> <p>We have used the torch.nn.functional module from torch which provides many utility functions like loss functions, activations etc required to write and train deep neural networks.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*e_agaazNkZeE48Ie"/></figure> <p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs fromScratch</em></a></p> <h4>Gradient Computation withAutograd</h4> <p>To train the above model, we have to compute the gradients of loss w.r.t w1 and bwhich will be further used to update the existing weights iteratively. This is where PyTorch makes our life easier by automatically calculating them using the autogradengine.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cA24CuRSLTFFncr7"/></figure> <p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs fromScratch</em></a></p> <p>PyTorchs autograd system automatically computes gradients for all tensors with requires_grad=True. Here&#39;s how to compute gradients:</p> <pre>from torch.autograd import grad<br /><br /># Manual gradient computation<br />grad_L_w1 = grad(loss, w1, retain_graph=True)<br />grad_L_b = grad(loss, b, retain_graph=True)<br /><br /># Alternative using backward()<br />loss.backward()<br />print(w1.grad)    # Access gradient for w1<br />print(b.grad)     # Access gradient for b</pre> <p><strong>Technical Note</strong>: When using backward():</p> <ul><li>Gradients accumulate bydefault</li><li>Use zero_grad() before each backward pass in trainingloops</li><li>retain_graph=True allows multiple backwardpasses</li></ul> <p>The grad function is used to get gradients manually and it is useful for debugging and demonstration purposes. Using the backward() function automatically calculates for all the tensors which has requires_grad=True set and gradients will be stored inside.grad property.</p> <h3>Building Neural Networks withPyTorch</h3> <p>Next, we focus on PyTorch as a library for implementing deep neural networks. While our previous example demonstrated a single neuron for classification, practical applications require complex architectures like transformers and ResNets that process multiple inputs through various hidden layers to produce outputs. Manually calculating and updating individual weights becomes impractical at this scale. PyTorch provides a structured approach through its neural network modules, enabling efficient implementation of sophisticated architectures.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/860/0*USeS0rqcvEQiAYsC"/></figure> <p><em>Source: </em><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main"><em>LLMs fromScratch</em></a></p> <h4>Introduction to torch.nn.Module</h4> <p>The torch.nn.Module serves as PyTorch&#39;s foundational class for neural networks, providing a systematic way to define and manage model architectures, parameters, and computations. This base class handles essential functionalities including parameter management, device placement, and training behaviors.</p> <p>The subclass has the following components:</p> <ul><li>__init__: We define the layers of neural networks in the constructor of the subclass defined and how the layers interact during forward propagation.</li><li>forward: The forward method describes how the input data passes through the network and comes together as a computation graph.</li></ul> <h4>Creating Custom Neural Network Architectures</h4> <p>Complex neural networks require multiple layers with specific activation functions. Heres a practical implementation of a multi-layer neuralnetwork:</p> <pre>class DeepNetwork(nn.Module):<br />    def __init__(self, num_inputs, num_outputs):<br />        super().__init__()<br />        <br />        self.layers = nn.Sequential(<br />            nn.Linear(num_inputs, 30),     # First hidden layer<br />            nn.ReLU(),                     # Activation function<br />            nn.Linear(30, 20),             # Second hidden layer<br />            nn.ReLU(),                     <br />            nn.Linear(20, num_outputs)      # Output layer<br />        )<br /><br />    def forward(self, x):<br />        return self.layers(x)</pre> <p>nn.Sequential provides a container for stacking layers in a specific order, streamlining the forward pass implementation.</p> <h4>Model Parameters and Initialization</h4> <p>PyTorch automatically handles parameter initialization, but you can access and modify parameters:</p> <pre>model = DeepNetwork(50, 3)<br /><br /># Count trainable parameters<br />num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)<br />print(f&quot;Trainable parameters: {num_params}&quot;)<br /><br /># Access layer parameters<br />for name, param in model.named_parameters():<br />    print(f&quot;Layer: {name} | Size: {param.size()}&quot;)<br /><br /># Custom initialization<br />def init_weights(m):<br />    if isinstance(m, nn.Linear):<br />        torch.nn.init.xavier_uniform_(m.weight)<br />        m.bias.data.fill_(0.01)<br /><br />model.apply(init_weights)</pre> <p>Each parameter for which requires_grad=True counts as a trainable parameter and will be updated during training. In the above code, this referes to the weights initialized in torch.nn.Linear layers.</p> <h4>Forward Propagation Implementation</h4> <p>Forward propagation defines how input data flows through the network. Lets initialise random values and pass it through themodel.</p> <pre># Sample forward pass<br />model = DeepNetwork(50, 3)<br />batch_size = 32<br />input_features = torch.randn(batch_size, 50)<br /><br />with torch.no_grad():<br />    outputs = model(input_features)<br /><br />print(f&quot;Output shape: {outputs.shape}&quot;)</pre> <h4>Training Mode vs. Evaluation Mode</h4> <p>PyTorch models have distinct training and evaluation modes that affect certain layers behavior:</p> <pre>model = DeepNetwork(50, 3)<br /><br /># Training mode<br />model.train()<br />training_output = model(input_features)  <br /># Layers like Dropout and BatchNorm active<br />print(training_output)<br /><br /># Evaluation mode<br />model.eval()<br />with torch.no_grad():<br />    eval_output = model(input_features)  # Deterministic behavior<br />    print(eval_output)</pre> <p>PyTorch models operate in two distinctmodes:</p> <ol><li>Training Mode (model.train()):</li></ol> <ul><li>Activates Dropout and BatchNorm layers</li><li>Enables gradient computation andtracking</li><li>Maintains computational graph for backpropagation</li></ul> <p>2. Evaluation Mode (model.eval() with torch.no_grad()):</p> <ul><li>Disables Dropout and freezes BatchNorm statistics</li><li>Prevents gradient computation andtracking</li><li>Optimizes memory usage by eliminating gradientstorage</li><li>Reduces computational overhead during inference</li></ul> <p>This mode management ensures efficient resource utilization while maintaining appropriate model behavior for both training and inference phases.</p> <h3>Efficient DataHandling</h3> <p>Efficient data handling is crucial for developing robust deep learning models. PyTorch provides two primary tools for data management: the Dataset and DataLoader classes.</p> <h4>1. Dataset and DataLoader Overview</h4> <p>PyTorchs data handling framework consistsof</p> <ul><li>Dataset: Defines data access and preprocessing</li><li>DataLoader: Handles batch creation, shuffling, and parallelloading</li></ul> <p>Lets implement a simple classification dataset to demonstrate these concepts:</p> <pre>import torch<br />from torch.utils.data import Dataset, DataLoader<br /><br /># Training classification data<br />X_train = torch.tensor([<br />    [-1.2, 3.1],<br />    [-0.9, 2.9],<br />    [-0.5, 2.6],<br />    [2.3, -1.1],<br />    [2.7, -1.5]<br />])<br /><br />y_train = torch.tensor([0, 0, 0, 1, 1])<br /><br /># Testing dataset<br />X_test = torch.tensor([<br />    [-0.8, 2.8],<br />    [2.6, -1.6],<br />])<br /><br />y_test = torch.tensor([0, 1])</pre> <h4>2. Creating Custom DatasetObjects</h4> <p>Next, we create a custom dataset class, SampleDataset, by subclassing from PyTorchs Dataset parent class. It has following properties:</p> <ul><li>__init__: Initialize dataset attributes.</li><li>__getitem__: Define data access for individual samples</li><li>__len__: Return total number ofsamples</li></ul> <pre>from torch.utils.data import Dataset<br /><br />class SampleDataset(Dataset):<br />    def __init__(self, X, y):<br />    &quot;&quot;&quot;Initialize the dataset with features and labels&quot;&quot;&quot;<br />        self.features = X<br />        self.labels = y<br />    def __getitem__(self, index):<br />    &quot;&quot;&quot;Retrieve a single example and its label&quot;&quot;&quot;     <br />        one_x = self.features[index]     <br />        one_y = self.labels[index]       <br />        return one_x, one_y              <br />    def __len__(self):<br />    &quot;&quot;&quot;Get the total number of examples in the dataset&quot;&quot;&quot;<br />        return self.labels.shape[0]     <br /><br />train_ds = SampleDataset(X_train, y_train)<br />test_ds = SampleDataset(X_test, y_test)</pre> <h4>3. Implementing DataLoader</h4> <p>DataLoaders handle the heavy lifting of batching, shuffling, and parallel data loading. Now we can create DataLoaders from the SampleDataset object created. This can be done asfollows:</p> <pre># Create DataLoader with specific configurations<br />train_loader = DataLoader(<br />    dataset=train_ds,     # Dataset Instance<br />    batch_size=2,         # Number of samples per batch<br />    shuffle=True,         # Shuffle the training data<br />    num_workers=0         # Number of parallel workers<br />    drop_last=True.       # Drop incomplete batch<br />)<br /><br />test_loader = DataLoader(<br />    dataset=test_ds,<br />    batch_size=2,<br />    shuffle=False,        # No need to shuffle test data<br />    num_workers=0<br />)</pre> <p>Some key parameters of Dataloaders class are asfollows:</p> <ul><li>dataset: The Dataset instance to load datafrom</li><li>batch_size: Number of samples perbatch</li><li>shuffle: Whether to shuffle data betweenepochs</li><li>num_workers: Number of subprocesses for dataloading</li><li>drop_last: Whether to drop the last incomplete batch</li><li>pin_memory: Pin memory for faster data transfer toGPU</li></ul> <h4>Complete Example with Best Practices (Best Practice)</h4> <p>Heres a comprehensive implementation incorporating all concepts:</p> <pre>import torch<br />from torch.utils.data import Dataset, DataLoader<br /><br />class SampleDataset(Dataset):<br />    def __init__(self, X, y, transform=None):<br />        self.features = X<br />        self.labels = y<br />        self.transform = transform # Input transformations if required<br />    <br />    def __getitem__(self, index):<br />        x = self.features[index]<br />        y = self.labels[index]<br />        <br />        if self.transform:<br />            x = self.transform(x)<br />            <br />        return x, y<br />    <br />    def __len__(self):<br />        return len(self.labels)<br /><br /># Configuration for optimal performance<br />def create_data_loader(dataset, batch_size, is_training=True):<br />    return DataLoader(<br />        dataset=dataset,<br />        batch_size=batch_size,<br />        shuffle=is_training,<br />        num_workers=4 if is_training else 2,<br />        pin_memory=torch.cuda.is_available(),<br />        drop_last=is_training,<br />        persistent_workers=True<br />    )<br /><br /><br /># Usage example<br />if __name__ == &quot;__main__&quot;:<br />    # Create dataset<br />    dataset = SampleDataset(X_train, y_train)<br />    <br />    # Create data loader<br />    train_loader = create_data_loader(<br />        dataset=dataset,<br />        batch_size=32,<br />        is_training=True<br />    )<br />    <br />    # Training loop example<br />    for epoch in range(num_epochs):<br />        for batch_idx, (features, labels) in enumerate(train_loader):<br />            # Training operations here<br />            pass</pre> <p>This implementation provides a robust foundation for handling data in PyTorch, incorporating best practices for memory management and parallel processing. Adjust the configurations based on your specific use case and available computational resources.</p> <h3>Implementing Training Loops inPyTorch</h3> <p>A PyTorch training loop consists of several key components:</p> <ul><li>model initialization,</li><li>optimizer configuration,</li><li>loss function definition, and</li><li>the iterative trainingprocess.</li></ul> <p>Heres a structured implementation showcasing these elements.</p> <h4>Basic Training Loop Structure (Best Practice)</h4> <pre>import torch<br />import torch.nn as nn<br />from torch.optim import Adam<br />from torch.utils.data import DataLoader<br /><br />def train_model(<br />    model: nn.Module,<br />    train_loader: DataLoader,<br />    val_loader: DataLoader,<br />    num_epochs: int,<br />    learning_rate: float,<br />    device: str = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;<br />) -&gt; dict:<br />    &quot;&quot;&quot;<br />    Trains a PyTorch neural network model using the provided data loaders.<br />    <br />    This function implements a standard training loop with validation after each epoch.<br />    It supports both CPU and GPU training and tracks various metrics during training.<br />    <br />    Args:<br />        model (nn.Module): The PyTorch model to train<br />        train_loader (DataLoader): DataLoader for training data<br />        val_loader (DataLoader): DataLoader for validation data<br />        num_epochs (int): Number of training epochs<br />        learning_rate (float): Learning rate for the optimizer<br />        device (str): Device to train on (&#39;cuda&#39; or &#39;cpu&#39;), defaults to GPU if available<br />        <br />    Returns:<br />        dict: Training history containing:<br />            - train_loss: List of training losses per epoch<br />            - val_loss: List of validation losses per epoch<br />            - val_accuracy: List of validation accuracies per epoch<br />    &quot;&quot;&quot;<br />    <br />    # Initialize Adam optimizer and Cross Entropy Loss criterion<br />    optimizer = Adam(model.parameters(), lr=learning_rate)<br />    criterion = nn.CrossEntropyLoss()<br />    <br />    # Move model to appropriate device (GPU/CPU)<br />    model = model.to(device)<br />    <br />    # Dictionary to store training metrics<br />    history = {<br />        &#39;train_loss&#39;: [],<br />        &#39;val_loss&#39;: [],<br />        &#39;val_accuracy&#39;: []<br />    }<br />    <br />    # Main training loop over epochs<br />    for epoch in range(num_epochs):<br />        # Set model to training mode (enables dropout, batch norm, etc.)<br />        model.train()<br />        train_loss = 0.0<br />        <br />        # Iterate over training batches<br />        for batch_idx, (features, labels) in enumerate(train_loader):<br />            # Transfer batch data to appropriate device<br />            features = features.to(device)<br />            labels = labels.to(device)<br />            <br />            # Forward propagation<br />            outputs = model(features)<br />            loss = criterion(outputs, labels)<br />            <br />            # Backward propagation and optimization<br />            optimizer.zero_grad()  # Clear previous gradients<br />            loss.backward()        # Compute gradients<br />            optimizer.step()       # Update model parameters<br />            <br />            # Accumulate batch loss<br />            train_loss += loss.item()<br />            <br />            # Print progress every 100 batches<br />            if batch_idx % 100 == 0:<br />                print(f&#39;Epoch: {epoch+1}/{num_epochs} | &#39;<br />                      f&#39;Batch: {batch_idx}/{len(train_loader)} | &#39;<br />                      f&#39;Loss: {loss.item():.4f}&#39;)<br />        <br />        # Calculate average training loss for the epoch<br />        train_loss = train_loss / len(train_loader)<br />        history[&#39;train_loss&#39;].append(train_loss)<br />        <br />        # Validation phase<br />        model.eval()  # Set model to evaluation mode<br />        val_loss = 0.0<br />        correct = 0<br />        total = 0<br />        <br />        # Disable gradient computation for validation<br />        with torch.no_grad():<br />            for features, labels in val_loader:<br />                features = features.to(device)<br />                labels = labels.to(device)<br />                <br />                # Forward pass only<br />                outputs = model(features)<br />                loss = criterion(outputs, labels)<br />                <br />                # Accumulate validation metrics<br />                val_loss += loss.item()<br />                _, predicted = torch.max(outputs.data, 1)  # Get predicted class<br />                total += labels.size(0)                    # Count total samples<br />                correct += (predicted == labels).sum().item()  # Count correct predictions<br />        <br />        # Calculate validation metrics<br />        val_loss = val_loss / len(val_loader)<br />        val_accuracy = 100 * correct / total<br />        <br />        # Store validation metrics<br />        history[&#39;val_loss&#39;].append(val_loss)<br />        history[&#39;val_accuracy&#39;].append(val_accuracy)<br />        <br />        # Print epoch summary<br />        print(f&#39;Epoch: {epoch+1}/{num_epochs} | &#39;<br />              f&#39;Train Loss: {train_loss:.4f} | &#39;<br />              f&#39;Val Loss: {val_loss:.4f} | &#39;<br />              f&#39;Val Accuracy: {val_accuracy:.2f}%&#39;)<br />    <br />    return history<br /><br /><br /># Example Usage<br />def main():<br />    &quot;&quot;&quot;<br />    Example implementation of the training pipeline.<br />    <br />    This function demonstrates how to use the train_model function<br />    with a custom model and configuration.<br />    &quot;&quot;&quot;<br />    # Initialize model (assumed to be defined elsewhere)<br />    model = DeepNetwork()<br />    <br />    # Define training hyperparameters<br />    config = {<br />        &#39;num_epochs&#39;: 10,     # Number of training epochs<br />        &#39;learning_rate&#39;: 0.001,  # Learning rate for optimization<br />    }<br />    <br />    # Train the model with specified configuration<br />    history = train_model(<br />        model=model,<br />        train_loader=train_loader,  # Assumed to be defined elsewhere<br />        val_loader=test_loader,     # Assumed to be defined elsewhere<br />        num_epochs=config[&#39;num_epochs&#39;],<br />        learning_rate=config[&#39;learning_rate&#39;]<br />    )</pre> <p>The training loop does the following gradient descent as following:</p> <ul><li>The training process involves passing logits to the cross_entropy loss function, which internally applies softmax for optimized performance and numerical stability.</li><li>The loss.backward() call computes gradients through PyTorch&#39;s computational graph</li><li>The optimizer.step() step updates the model parameters using these gradients</li><li>The optimizer.zero_grad() must be called every training iteration to reset gradients, preventing unintended accumulation that could distort the optimization process.</li></ul> <h3>Model Persistence in PyTorch: Saving andLoading</h3> <p>PyTorch provides efficient mechanisms for model persistence through its state dictionary system. The state dictionary (state_dict) maintains a mapping between layer identifiers and their corresponding parameters (weights andbiases).</p> <h3>Basic Model Persistence</h3> <h4>Saving Models</h4> <p>After training the model, it is necessary to save the model weights to reuse later for further training or deployment. Save a models learned parameters using the state dictionary:</p> <pre>import torch<br /><br /># Save model parameters<br />torch.save(model.state_dict(), &quot;model_parameters.pth&quot;)</pre> <h4>Loading Models</h4> <p>The torch.load(&quot;model_parameters.pth&quot;) function reads the file &quot;model_parameters.pth&quot; and reconstructs the Python dictionary object containing the models parameters while model.load_state_dict() applies these parameters to the model, effectively restoring its learned state from when we savedit.</p> <p>We need the instance of the model in memory to apply the saved parameters. Here, the NeuralNetwork(2, 2) architecture needs to match the original saved modelexactly.</p> <pre># Initialize model architecture<br />model = NeuralNetwork(num_inputs=2, num_outputs=2)<br /><br /># Load saved parameters<br />model.load_state_dict(torch.load(&quot;model_parameters.pth&quot;))</pre> <h3>Comprehensive Model Persistence (Best Practice)</h3> <p>For production scenarios, save additional information alongside model parameters:</p> <pre># Save complete model state<br />checkpoint = {<br />    &#39;model_state_dict&#39;: model.state_dict(),<br />    &#39;optimizer_state_dict&#39;: optimizer.state_dict(),<br />    &#39;epoch&#39;: epoch,<br />    &#39;loss&#39;: loss,<br />    &#39;model_config&#39;: {<br />        &#39;num_inputs&#39;: 2,<br />        &#39;num_outputs&#39;: 2<br />    }<br />}<br />torch.save(checkpoint, &quot;model_checkpoint.pth&quot;)<br /><br /><br /># Load complete model state<br />checkpoint = torch.load(&quot;model_checkpoint.pth&quot;)<br />model = NeuralNetwork(**checkpoint[&#39;model_config&#39;])<br />model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])<br />optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])<br />epoch = checkpoint[&#39;epoch&#39;]<br />loss = checkpoint[&#39;loss&#39;]</pre> <h3>Conclusion</h3> <p>PyTorchs architecture provides a robust foundation for deep learning development through its integrated components: tensor computations, automatic differentiation, and neural network modules. The frameworks design enables efficient model implementation through dynamic computation graphs, GPU acceleration, and intuitive APIs for data processing and model construction.</p> <p>For continued learning and implementation guidance, refer to PyTorchs official documentation which provides comprehensive updates on best practices, optimizations, and emerging capabilities. This ensures your deep learning applications remain aligned with current framework standards and performance benchmarks.</p> <p>Want more AI/ML deep dives? Subscribe <a href="https://neuraforge.substack.com">here</a>.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=06b66a651f1f" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">PyTorch in Practice: Essential Building Blocks for Modern Deep Learning</title><link href="https://jnk234.github.io/blog/2024/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning/" rel="alternate" type="text/html" title="PyTorch in Practice: Essential Building Blocks for Modern Deep Learning"/><published>2024-12-30T17:39:16+00:00</published><updated>2024-12-30T17:39:16+00:00</updated><id>https://jnk234.github.io/blog/2024/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning</id><content type="html" xml:base="https://jnk234.github.io/blog/2024/pytorch-in-practice-essential-building-blocks-for-modern-deep-learning/"><![CDATA[<h1>Introduction</h1> <p>As deep learning continues to advance artificial intelligence applications, PyTorch has established itself as a fundamental framework powering everything from computer vision systems to large language models. Originally developed by Meta&#8217;s AI Research lab, PyTorch combines Python's flexibility with deep learning capabilities through a powerful, intuitive interface.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6000" height="4000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4000,&quot;width&quot;:6000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;blue building block lot&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="blue building block lot" title="blue building block lot" srcset="https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1493217465235-252dd9c0d632?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw2fHxiYXNpYyUyMGJsb2NrfGVufDB8fHx8MTczNTUxNTk4MHww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Iker Urteaga</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h3>Core Components of PyTorch</h3> <p>PyTorch's architecture rests on three key components that work together to enable efficient deep learning development:</p> <ol><li><p><strong>Dynamic Tensor Library</strong></p><ul><li><p>Extends NumPy's array programming capabilities</p></li><li><p>Provides seamless CPU and GPU acceleration</p></li><li><p>Implements efficient mathematical operations for deep learning computations</p></li></ul></li><li><p><strong>Automatic Differentiation Engine (Autograd)</strong></p><ul><li><p>Computes gradients automatically through computational graphs</p></li><li><p>Manages backpropagation for neural network training</p></li></ul></li><li><p><strong>Deep Learning Framework</strong></p><ul><li><p>Delivers modular neural network components</p></li><li><p>Implements optimized loss functions and optimizers</p></li></ul></li></ol> <h2>Getting Started with PyTorch</h2> <h3>Installation and Setup</h3> <p>PyTorch can be installed directly using pip, Python's package installer:</p> <pre><code><code>pip install torch</code></code></pre> <p>However, for optimal performance, it's recommended to install the version specifically compatible with your system's hardware. Visit&nbsp;<a href="https://pytorch.org/">pytorch.org</a>&nbsp;to get the appropriate installation command based on your:</p> <ul><li><p>Operating system</p></li><li><p>Package manager preference (pip/conda)</p></li><li><p>CUDA version (for GPU support)</p></li><li><p>Python version</p></li></ul> <h3>GPU Support and Compatibility</h3> <p>PyTorch seamlessly integrates with NVIDIA GPUs through CUDA. To verify GPU availability in your environment:</p> <pre><code>import torch

# Check GPU availability
gpu_available = torch.cuda.is_available()
print(f"GPU Available: {gpu_available}")

# Get GPU device count if available
if gpu_available:
    print(f"Number of GPUs: {torch.cuda.device_count()}")</code></pre> <p>If a GPU is detected, you can move tensors and models to GPU memory using:</p> <pre><code># Create a tensor
tensor = torch.tensor([1.0, 2.0, 3.0])

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
tensor = tensor.to(device)</code></pre> <h3>Apple Silicon Support</h3> <p>For users with Apple M1/M2/M3 chips, PyTorch provides acceleration through the Metal Performance Shaders (MPS) backend. Verify MPS availability:</p> <pre><code>import torch

# Check MPS (Metal Performance Shaders) availability
mps_available = torch.backends.mps.is_available()
print(f"MPS Available: {mps_available}")

# If MPS is available, you can use it as device
if mps_available:
    device = torch.device("mps")
    # Move tensors/models to MPS device
    tensor = tensor.to(device)</code></pre> <p>For ease of usage, I recommend using <a href="https://colab.research.google.com/">Google Colab</a> i.e. a popular jupyter notebook&#8211;like environment, which provides time-limited access to GPUs.</p> <h2>Understanding Tensors</h2> <h3>What Are Tensors?</h3> <p>Tensors are mathematical objects that generalize vectors and matrices to higher dimensions. In PyTorch, tensors serve as fundamental data containers that hold and process multi-dimensional arrays of numerical values. These containers enable efficient computation and automatic differentiation, making them essential for deep learning operations. PyTorch tensors are similar to Numpy arrays in basic sense. </p> <h3>Scalers, Vectors, Matrices and Tensors</h3> <div class="image-gallery-embed" data-attrs="{&quot;gallery&quot;:{&quot;images&quot;:[{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png&quot;}],&quot;caption&quot;:&quot;Source: https://dev.to/mmithrakumar/scalars-vectors-matrices-and-tensors-with-tensorflow-2-0-1f66&quot;,&quot;alt&quot;:&quot;Tensors&quot;,&quot;staticGalleryImage&quot;:{&quot;type&quot;:&quot;image/png&quot;,&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37a50016-1053-498d-b0a7-ea6cb4eae0bb_800x284.png&quot;}},&quot;isEditorNode&quot;:true}"></div> <p>As mentioned earlier, PyTorch tensors are data containers for array-like structures. A scalar is a zero-dimensional tensor (for instance, just a number), a vector is a one-dimensional tensor, and a matrix is a two-dimensional tensor. There is no specific term for higher-dimensional tensors, so we typically refer to a three-dimensional tensor as just a 3D tensor, and so forth. We can create objects of PyTorch&#8217;s&nbsp;`Tensor`&nbsp;class using the&nbsp;`torch.tensor`&nbsp;function as shown in the following listing.</p> <pre><code>import torch

# Scalar (0-dimensional tensor)
scalar = torch.tensor(1)     

# Vector (1-dimensional tensor)
vector = torch.tensor([1, 2, 3])    

# Matrix (2-dimensional tensor)
matrix = torch.tensor([[1, 2], 
                      [3, 4]])     

# 3-dimensional tensor
tensor3d = torch.tensor([[[1, 2], [3, 4]], 
                        [[5, 6], [7, 8]]])</code></pre> <p>Each tensor type maintains its specific dimensionality, accessible through the&nbsp;<strong>.shape</strong>&nbsp;attribute:</p> <pre><code>print(f"Scalar shape: {scalar.shape}")      # torch.Size([])
print(f"Vector shape: {vector.shape}")      # torch.Size([3])
print(f"Matrix shape: {matrix.shape}")      # torch.Size([2, 2])
print(f"3D tensor shape: {tensor3d.shape}") # torch.Size([2, 2, 2])</code></pre> <h3>Tensor Data Types and Precision</h3> <p>PyTorch supports various data types with different precision levels, optimized for different computational needs:</p> <p>Some of the common torch datatypes available with torch are <code>float32</code>, <code>float64</code>, <code>float16</code>, <code>bfloat16</code>, <code>int8</code>, <code>uint8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code>.</p> <p>The choice of precision impacts both memory usage and computational efficiency:</p> <ul><li><p><code>float32</code>: Standard for most deep learning tasks</p></li><li><p><code>float16</code>: Reduced precision, useful for memory optimization</p></li><li><p><code>bfloat16</code>: Brain Floating Point, balances precision and range</p></li></ul> <h3>Floating Data Types</h3> <p>PyTorch supports various floating-point precisions for tensors, each serving different computational needs:</p> <ul><li><p><code>torch.float32</code> (default): 32-bit precision offering 6-9 decimal places, optimal for most deep learning tasks</p></li><li><p><code>torch.float64</code>: 64-bit double precision with 15-17 decimal places, suitable for high-precision numerical computations</p></li><li><p><code>torch.float16</code>: 16-bit half precision with 3-4 decimal places, useful for memory-efficient operations</p></li><li><p><code>torch.bfloat16</code>: Brain floating point format with 2-3 decimal precision, balancing range and precision</p></li></ul> <pre><code><code>import torch

float32_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  
float64_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)  
float16_tensor = torch.tensor([1.0, 2.0], dtype=torch.float16)  
bfloat16_tensor = torch.tensor([1.0, 2.0], dtype=torch.bfloat16)</code></code></pre> <h3>Integer Types</h3> <p>PyTorch supports various integer data types, each with specific memory allocations and value ranges:</p> <ul><li><p><code>int8</code>: 8-bit signed integers (-128 to 127)</p></li><li><p><code>uint8</code>: 8-bit unsigned integers (0 to 255)</p></li><li><p><code>int16</code>: 16-bit signed integers (-32768 to 32767)</p></li><li><p><code>int32</code>: 32-bit signed integers (-2^31 to 2^31-1)</p></li><li><p><code>int64</code>: 64-bit signed integers (-2^63 to 2^63-1), default integer type in PyTorch</p></li></ul> <pre><code><code>import torch

int8_tensor = torch.tensor([1, 2], dtype=torch.int8)     
uint8_tensor = torch.tensor([1, 2], dtype=torch.uint8)   
int16_tensor = torch.tensor([1, 2], dtype=torch.int16)   
int32_tensor = torch.tensor([1, 2], dtype=torch.int32)   
int64_tensor = torch.tensor([1, 2], dtype=torch.int64)
</code></code></pre> <h3>Datatype Conversion</h3> <p>We can convert tensors from one datatype to another using the <code>.to</code> method.</p> <pre><code><code># Converting between data types
tensor = torch.tensor([1, 2, 3])
float_tensor = tensor.to(torch.float32) # Convert from int64 to float32
int_tensor = tensor.to(torch.int32)     # Convert from float32 to int32
</code></code></pre> <h3>Common Tensor Operations</h3> <p>PyTorch provides several fundamental tensor operations essential for deep learning computations. Here are the key operations with their implementations and specific use cases.</p> <h4>1. Tensor Creation and Shape Manipulation</h4> <p>Creating tensors and understanding their shape are fundamental operations in PyTorch:</p> <pre><code><code>import torch

# Create 2D tensor
tensor2d = torch.tensor([[1, 2, 3], 
                        [4, 5, 6]])

# Check tensor shape
shape = tensor2d.shape  
# Returns: torch.Size([2, 3])
</code></code></pre> <p>For the above tensor, the shape if 2 x 3 i.e. 2 rows and 3 columns. We can change the shape of the array by maintaining the total size of the array using reshape method.</p> <h4>2. Reshaping Operations</h4> <p>PyTorch offers two methods for tensor reshaping:</p> <pre><code><code># Reshape tensor from (2,3) to (3,2)
reshaped_tensor = tensor2d.reshape(3, 2)

# Alternative using view
viewed_tensor = tensor2d.view(3, 2)
</code></code></pre> <p><strong>Technical Note</strong>: <code>.view()</code> and <code>.reshape()</code> differ in memory handling:</p> <ul><li><p><code>.view()</code>: Requires contiguous memory layout</p></li><li><p><code>.reshape()</code>: Works with any memory layout, performs copy if necessary</p></li></ul> <h4>3. Matrix Operations</h4> <p>PyTorch implements efficient matrix operations essential for linear algebra computations:</p> <pre><code><code># Transpose operation
transposed = tensor2d.T

# Matrix multiplication methods
result1 = tensor2d.matmul(tensor2d.T)  # Using matmul
result2 = tensor2d @ tensor2d.T        # Using @ operator
</code></code></pre> <p>Output shapes for a 2x3 input tensor:</p> <ul><li><p>Transpose: 3x2</p></li><li><p>Matrix multiplication with transpose: 2x2</p></li></ul> <p>These operations form the foundation for neural network computations and linear algebra operations in deep learning models. For an exhaustive list of tensor operations, refer to the <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch documentation</a>.</p> <h2>Automatic Differentiation</h2> <h3>Understanding Computational Graphs</h3> <p>PyTorch builds computational graphs that track operations performed on tensors. These graphs enable automatic differentiation through the <code>autograd</code> system, making gradient computation efficient and programmatic.</p> <p>A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network&#8212;we will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.</p> <p>Consider the following example of a single layer neural network performing logistic regression with single weight and bias.</p> <pre><code><code>import torch
import torch.nn.functional as F

# Initialize inputs and parameters
y = torch.tensor([1.0])           # Target
x1 = torch.tensor([1.1])          # Input
w1 = torch.tensor([2.2], 
                  requires_grad=True)  # Weight
b = torch.tensor([0.0], 
                 requires_grad=True)   # Bias

# Forward pass computation
z = x1 * w1 + b                   # Linear computation
a = torch.sigmoid(z)              # Activation
loss = F.binary_cross_entropy(a, y)    # Loss computation
</code></code></pre> <p>We have used the <code>torch.nn.functional</code> module from <code>torch</code> which provides many utility functions like loss functions, activations etc required to write and train deep neural networks.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!6wqh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic" width="1156" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:1156,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26016,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6wqh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 424w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 848w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1272w, https://substackcdn.com/image/fetch/$s_!6wqh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa38187f3-6922-499d-8e02-bf56b8d3c0b8_1156x400.heic 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p> <h3>Gradient Computation with Autograd</h3> <p>To train the above model, we have to compute the gradients of loss w.r.t <code>w1</code> and <code>b</code> which will be further used to update the existing weights iteratively. This is where PyTorch makes our life easier by automatically calculating them using the <code>autograd</code> engine.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!D2gj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic" width="1120" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68642,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!D2gj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 424w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 848w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1272w, https://substackcdn.com/image/fetch/$s_!D2gj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a7b77bd-937b-4fff-9151-57bd01b243ef_1120x726.heic 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p> <p>PyTorch's autograd system automatically computes gradients for all tensors with <code>requires_grad=True</code>. Here's how to compute gradients:</p> <pre><code><code>from torch.autograd import grad

# Manual gradient computation
grad_L_w1 = grad(loss, w1, retain_graph=True)
grad_L_b = grad(loss, b, retain_graph=True)

# Alternative using backward()
loss.backward()
print(w1.grad)    # Access gradient for w1
print(b.grad)     # Access gradient for b
</code></code></pre> <p><strong>Technical Note</strong>: When using <code>backward()</code>:</p> <ul><li><p>Gradients accumulate by default</p></li><li><p>Use <code>zero_grad()</code> before each backward pass in training loops</p></li><li><p><code>retain_graph=True</code> allows multiple backward passes</p></li></ul> <p>The <code>grad</code> function is used to get gradients manually and it is useful for debugging and demonstration purposes. Using the <code>backward()</code> function automatically calculates for all the tensors which has <code>requires_grad=True</code> set and gradients will be stored inside <code>.grad</code> property.</p> <h2>Building Neural Networks with PyTorch</h2> <p>Next, we focus on PyTorch as a library for implementing deep neural networks. While our previous example demonstrated a single neuron for classification, practical applications require complex architectures like transformers and ResNets that process multiple inputs through various hidden layers to produce outputs. Manually calculating and updating individual weights becomes impractical at this scale. PyTorch provides a structured approach through its neural network modules, enabling efficient implementation of sophisticated architectures.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!AV-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic" width="860" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:860,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:90385,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AV-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 424w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 848w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1272w, https://substackcdn.com/image/fetch/$s_!AV-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55d81b02-6e3d-412a-baaf-2a0779d6f80d_860x760.heic 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div> <p><em>Source: <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main">LLMs from Scratch</a></em></p> <h3>Introduction to torch.nn.Module</h3> <p>The <code>torch.nn.Module</code> serves as PyTorch's foundational class for neural networks, providing a systematic way to define and manage model architectures, parameters, and computations. This base class handles essential functionalities including parameter management, device placement, and training behaviors.</p> <p>The subclass has the following components:</p> <ul><li><p><code>__init__</code>: We define the layers of neural networks in the constructor of the subclass defined and how the layers interact during forward propagation.</p></li><li><p><code>forward</code>: The forward method describes how the input data passes through the network and comes together as a computation graph.</p></li></ul> <h3>Creating Custom Neural Network Architectures</h3> <p>Complex neural networks require multiple layers with specific activation functions. Here's a practical implementation of a multi-layer neural network:</p> <pre><code><code>class DeepNetwork(nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(num_inputs, 30),     # First hidden layer
            nn.ReLU(),                     # Activation function
            nn.Linear(30, 20),             # Second hidden layer
            nn.ReLU(),                     
            nn.Linear(20, num_outputs)      # Output layer
        )

    def forward(self, x):
        return self.layers(x)
</code></code></pre> <p><code>nn.Sequential</code> provides a container for stacking layers in a specific order, streamlining the forward pass implementation.</p> <h3>Model Parameters and Initialization</h3> <p>PyTorch automatically handles parameter initialization, but you can access and modify parameters:</p> <pre><code><code>model = DeepNetwork(50, 3)

# Count trainable parameters
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Trainable parameters: {num_params}")

# Access layer parameters
for name, param in model.named_parameters():
    print(f"Layer: {name} | Size: {param.size()}")

# Custom initialization
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

model.apply(init_weights)</code></code></pre> <p>Each parameter for which <code>requires_grad=True</code> counts as a trainable parameter and will be updated during training. In the above code, this referes to the weights initialized in <code>torch.nn.Linear</code> layers.</p> <h3>Forward Propagation Implementation</h3> <p>Forward propagation defines how input data flows through the network. Let's initialise random values and pass it through the model.</p> <pre><code><code># Sample forward pass
model = DeepNetwork(50, 3)
batch_size = 32
input_features = torch.randn(batch_size, 50)

with torch.no_grad():
    outputs = model(input_features)

print(f"Output shape: {outputs.shape}")</code></code></pre> <h3>Training Mode vs. Evaluation Mode</h3> <p>PyTorch models have distinct training and evaluation modes that affect certain layers' behavior:</p> <pre><code><code>model = DeepNetwork(50, 3)

# Training mode
model.train()
training_output = model(input_features)  # Layers like Dropout and BatchNorm active

print(training_output)

# Evaluation mode
model.eval()
with torch.no_grad():
    eval_output = model(input_features)  # Deterministic behavior
    print(eval_output)</code></code></pre> <p>PyTorch models operate in two distinct modes:</p> <ol><li><p>Training Mode (<code>model.train()</code>):</p><ul><li><p>Activates Dropout and BatchNorm layers</p></li><li><p>Enables gradient computation and tracking</p></li><li><p>Maintains computational graph for backpropagation</p></li></ul></li><li><p>Evaluation Mode (<code>model.eval()</code> with <code>torch.no_grad()</code>):</p><ul><li><p>Disables Dropout and freezes BatchNorm statistics</p></li><li><p>Prevents gradient computation and tracking</p></li><li><p>Optimizes memory usage by eliminating gradient storage</p></li><li><p>Reduces computational overhead during inference</p></li></ul></li></ol> <p>This mode management ensures efficient resource utilization while maintaining appropriate model behavior for both training and inference phases.</p> <h2>Efficient Data Handling</h2> <p>Efficient data handling is crucial for developing robust deep learning models. PyTorch provides two primary tools for data management: the <code>Dataset</code> and <code>DataLoader</code> classes.</p> <h3>1. Dataset and DataLoader Overview</h3> <p>PyTorch's data handling framework consists of</p> <ul><li><p><code>Dataset</code>: Defines data access and preprocessing</p></li><li><p><code>DataLoader</code>: Handles batch creation, shuffling, and parallel loading</p></li></ul> <p>Let's implement a simple classification dataset to demonstrate these concepts:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

# Training classification data
X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])

# Testing dataset
X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])
</code></code></pre> <h3>2. Creating Custom Dataset Objects</h3> <p>Next, we create a custom dataset class, <code>SampleDataset</code>, by subclassing from PyTorch&#8217;s <code>Dataset</code> parent class. It has following properties:</p> <ul><li><p><code>__init__</code>: Initialize dataset attributes.</p></li><li><p><code>__getitem__</code>: Define data access for individual samples</p></li><li><p><code>__len__</code>: Return total number of samples</p></li></ul> <pre><code><code>from torch.utils.data import Dataset

class SampleDataset(Dataset):
    def __init__(self, X, y):
    """Initialize the dataset with features and labels"""
        self.features = X
        self.labels = y

    def __getitem__(self, index):
    """Retrieve a single example and its label"""     
        one_x = self.features[index]     
        one_y = self.labels[index]       
        return one_x, one_y              

    def __len__(self):
    """Get the total number of examples in the dataset"""
        return self.labels.shape[0]     

train_ds = SampleDataset(X_train, y_train)
test_ds = SampleDataset(X_test, y_test)
</code></code></pre> <h3>3. Implementing DataLoader</h3> <p>DataLoaders handle the heavy lifting of batching, shuffling, and parallel data loading. Now we can create <code>DataLoaders</code> from the <code>SampleDataset</code> object created. This can be done as follows:</p> <pre><code><code># Create DataLoader with specific configurations
train_loader = DataLoader(
    dataset=train_ds,     # Dataset Instance
    batch_size=2,         # Number of samples per batch
    shuffle=True,         # Shuffle the training data
    num_workers=0         # Number of parallel workers
    drop_last=True.       # Drop incomplete batch
)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,        # No need to shuffle test data
    num_workers=0
)</code></code></pre> <p>Some key parameters of Dataloaders class are as follows:</p> <ul><li><p><code>dataset</code>: The Dataset instance to load data from</p></li><li><p><code>batch_size</code>: Number of samples per batch</p></li><li><p><code>shuffle</code>: Whether to shuffle data between epochs</p></li><li><p><code>num_workers</code>: Number of subprocesses for data loading</p></li><li><p><code>drop_last</code>: Whether to drop the last incomplete batch</p></li><li><p><code>pin_memory</code>: Pin memory for faster data transfer to GPU</p></li></ul> <h3>Complete Example with Best Practices (Best Practice)</h3> <p>Here's a comprehensive implementation incorporating all concepts:</p> <pre><code><code>import torch
from torch.utils.data import Dataset, DataLoader

class SampleDataset(Dataset):
    def __init__(self, X, y, transform=None):
        self.features = X
        self.labels = y
        self.transform = transform # Input transformations if required
    
    def __getitem__(self, index):
        x = self.features[index]
        y = self.labels[index]
        
        if self.transform:
            x = self.transform(x)
            
        return x, y
    
    def __len__(self):
        return len(self.labels)

# Configuration for optimal performance
def create_data_loader(dataset, batch_size, is_training=True):
    return DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=is_training,
        num_workers=4 if is_training else 2,
        pin_memory=torch.cuda.is_available(),
        drop_last=is_training,
        persistent_workers=True
    )

# Usage example
if __name__ == "__main__":
    # Create dataset
    dataset = SampleDataset(X_train, y_train)
    
    # Create data loader
    train_loader = create_data_loader(
        dataset=dataset,
        batch_size=32,
        is_training=True
    )
    
    # Training loop example
    for epoch in range(num_epochs):
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Training operations here
            pass
</code></code></pre> <p>This implementation provides a robust foundation for handling data in PyTorch, incorporating best practices for memory management and parallel processing. Adjust the configurations based on your specific use case and available computational resources.</p> <h2>Implementing Training Loops in PyTorch</h2> <p>A PyTorch training loop consists of several key components:</p> <ul><li><p>model initialization,</p></li><li><p>optimizer configuration,</p></li><li><p>loss function definition, and</p></li><li><p>the iterative training process.</p></li></ul> <p>Here's a structured implementation showcasing these elements.</p> <h3>Basic Training Loop Structure (Best Practice)</h3> <pre><code><code>import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader

def train_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    num_epochs: int,
    learning_rate: float,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
) -&gt; dict:
    
    # Initialize optimizer and loss function
    optimizer = Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    # Move model to device
    model = model.to(device)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_accuracy': []
    }
    
    # Training loop
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            # Move data to device
            features = features.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Accumulate loss
            train_loss += loss.item()
            
            # Optional: Print batch progress
            if batch_idx % 100 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs} | '
                      f'Batch: {batch_idx}/{len(train_loader)} | '
                      f'Loss: {loss.item():.4f}')
        
        # Calculate average training loss
        train_loss = train_loss / len(train_loader)
        history['train_loss'].append(train_loss)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(device)
                labels = labels.to(device)
                
                # Forward pass
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                # Accumulate validation metrics
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        # Calculate validation metrics
        val_loss = val_loss / len(val_loader)
        val_accuracy = 100 * correct / total
        
        # Store validation metrics
        history['val_loss'].append(val_loss)
        history['val_accuracy'].append(val_accuracy)
        
        # Print epoch summary
        print(f'Epoch: {epoch+1}/{num_epochs} | '
              f'Train Loss: {train_loss:.4f} | '
              f'Val Loss: {val_loss:.4f} | '
              f'Val Accuracy: {val_accuracy:.2f}%')
    
    return history

# Example Usage
def main():
    # Assume we have model and data loaders defined
    model = DeepNetwork()
    
    # Training configuration
    config = {
        'num_epochs': 10,
        'learning_rate': 0.001,
    }
    
    # Train model
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=test_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )
</code></code></pre> <p>The training loop does the following gradient descent as following:</p> <ul><li><p>The training process involves passing logits to the <code>cross_entropy</code> loss function, which internally applies <code>softmax</code> for optimized performance and numerical stability.</p></li><li><p>The <code>loss.backward()</code> call computes gradients through PyTorch's computational graph</p></li><li><p>The <code>optimizer.step()</code> step updates the model parameters using these gradients</p></li><li><p>The <code>optimizer.zero_grad()</code> must be called every training iteration to reset gradients, preventing unintended accumulation that could distort the optimization process.</p></li></ul> <h2>Model Persistence in PyTorch: Saving and Loading</h2> <p>PyTorch provides efficient mechanisms for model persistence through its state dictionary system. The state dictionary (<code>state_dict</code>) maintains a mapping between layer identifiers and their corresponding parameters (weights and biases).</p> <h3>Basic Model Persistence</h3> <h4>Saving Models</h4> <p>After training the model, it is necessary to save the model weights to reuse later for further training or deployment. Save a model's learned parameters using the state dictionary:</p> <pre><code><code>import torch

# Save model parameters
torch.save(model.state_dict(), "model_parameters.pth")</code></code></pre> <h3>Loading Models</h3> <p>The <code>torch.load("model_parameters.pth")</code> function reads the file <code>"model_parameters.pth"</code> and reconstructs the Python dictionary object containing the model&#8217;s parameters while <code>model.load_state_dict()</code> applies these parameters to the model, effectively restoring its learned state from when we saved it.</p> <p>We need the instance of the model in memory to apply the saved parameters. Here, the <code>NeuralNetwork(2,</code> <code>2)</code> architecture needs to match the original saved model exactly.</p> <pre><code><code># Initialize model architecture
model = NeuralNetwork(num_inputs=2, num_outputs=2)

# Load saved parameters
model.load_state_dict(torch.load("model_parameters.pth"))
</code></code></pre> <h2>Comprehensive Model Persistence (Best Practice)</h2> <p>For production scenarios, save additional information alongside model parameters:</p> <pre><code><code># Save complete model state
checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
    'model_config': {
        'num_inputs': 2,
        'num_outputs': 2
    }
}
torch.save(checkpoint, "model_checkpoint.pth")

# Load complete model state
checkpoint = torch.load("model_checkpoint.pth")
model = NeuralNetwork(**checkpoint['model_config'])
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
</code></code></pre> <h2>Conclusion</h2> <p>PyTorch's architecture provides a robust foundation for deep learning development through its integrated components: tensor computations, automatic differentiation, and neural network modules. The framework's design enables efficient model implementation through dynamic computation graphs, GPU acceleration, and intuitive APIs for data processing and model construction.</p> <p>For continued learning and implementation guidance, refer to PyTorch's official documentation which provides comprehensive updates on best practices, optimizations, and emerging capabilities. This ensures your deep learning applications remain aligned with current framework standards and performance benchmarks.</p> <div><hr/></div> <p>Thanks for reading NeuraForge: AI Unleashed! </p> <p>If you enjoyed this deep dive into AI/ML concepts, please consider subscribing to our newsletter for more technical content and practical insights. Your support helps grow our community and keeps the learning going! Don't forget to share with peers who might find it valuable. &#129504;&#10024;</p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p> <p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[From Tensors to Neural Networks: Understanding Core Components]]></summary></entry><entry><title type="html">Value-Based Policy Training in Reinforcement Learning</title><link href="https://jnk234.github.io/blog/2024/value-based-policy-training-in-reinforcement-learning/" rel="alternate" type="text/html" title="Value-Based Policy Training in Reinforcement Learning"/><published>2024-09-30T14:05:33+00:00</published><updated>2024-09-30T14:05:33+00:00</updated><id>https://jnk234.github.io/blog/2024/value-based-policy-training-in-reinforcement-learning</id><content type="html" xml:base="https://jnk234.github.io/blog/2024/value-based-policy-training-in-reinforcement-learning/"><![CDATA[<h2>Introduction</h2> <p>Reinforcement Learning (RL) is a branch of artificial intelligence that focuses on creating agents capable of making smart decisions by interacting with their environment through trial and error. This interaction is facilitated by a feedback loop involving states, actions, and rewards. The environment provides a state, the agent takes an action, and the environment responds with a reward and a new state. The goal of RL is to find a policy that maximizes the expected return when the agent acts according to it.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3448" height="4592" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4592,&quot;width&quot;:3448,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;white robot toy holding black tablet&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="white robot toy holding black tablet" title="white robot toy holding black tablet" srcset="https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1534723328310-e82dad3ee43f?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxyb2JvdCUyMGludGVsbGlnZW50fGVufDB8fHx8MTcyNzY3ODcyMXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Owen Beard</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h3>Policy and Decisions - Recap</h3> <p>At the heart of RL is the concept of a policy, which <strong>determines what action to take given the current state</strong>. Policies can be deterministic, always returning the same action for a given state, or stochastic, outputting a probability distribution over possible actions. The ultimate goal is to find the optimal policy (&#960;*) that maximizes the expected return. </p> <p>There are two approaches to <strong>finding an optimal policy</strong> for the RL problem at hand:</p> <h4>Policy-based Approach</h4> <ul><li><p>Policy-based methods directly train the policy to determine which action to take in a given state. </p></li><li><p>This is achieved by optimizing the policy function to maximize the expected rewards. The policy is typically represented by a neural network and is trained without a value function (used to tell how good the agent is at particular state or to take particular action - Used in Value based Approach). </p></li><li><p>The policy is not defined by hand but is learned through training.</p></li></ul> <h4><strong>Value-Based Approach</strong></h4> <ul><li><p>Value-based methods work indirectly by learning a value function that estimates how good it is to be in a particular state or which action to take in a given state.</p></li><li><p>The value function is trained in value based approach and the policy is defined by hand i.e. there is a fixed policy function. For example,<strong> a greedy policy </strong>always chooses the action/state that leads to the highest value.</p></li><li><p>Based on the information provided by the Value function (i.e. usually which state is more valuable or which action is best to take), the policy will decide the next action/state to move to.</p></li></ul> <p>In value-based methods, the link between the value function and policy is crucial. The <strong>policy uses the value function to make decisions</strong>. The trained value function outputs the action-value pair for each state, which is used by the predefined policy function to choose the relevant action.</p> <p>For example, the <strong>epsilon-greedy policy</strong> balances exploration and exploitation by choosing the action with the highest value most of the time but occasionally selects a random action.</p> <p>Going on we will focus more on the Value based functions and its different variations.</p> <h3>Different types of Value based Functions</h3> <p>There are two types of value based functions that can be used to to get the expected return or reward for agent to take a decision according to a fixed policy. </p> <h4><strong>State-Value Function</strong></h4> <p>The state-value function, <em><strong>V(s)</strong></em>, outputs the expected return if the <strong>agent</strong> starts in a <strong>state s</strong> and follows the policy forever afterward. It is defined as:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V(s) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t=s \\right]\n\n&quot;,&quot;id&quot;:&quot;OAVNLLZQGZ&quot;}" data-component-name="LatexBlockToDOM"></div> <p><strong>Interpretation</strong>:</p> <ul><li><p><strong>Rt</strong>: The immediate reward received at time t.</p></li><li><p><strong>&#947;</strong>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><strong>S_t = s</strong>: The condition that the agent is in state s at time t.</p></li></ul> <p>This formula calculates the expected cumulative reward starting from state s and following the policy. It considers the immediate reward and the discounted future rewards.</p> <p>Imagine a maze where each state has a value representing how good it is to be in that state. The state-value function assigns these values based on the expected return from starting in that state and following the policy.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ju4u!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg" width="1400" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ju4u!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ju4u!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F41b7fae6-8642-4852-9626-a47fedc1fe36_1400x788.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Source: https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c</figcaption></figure></div> <h4>Action-Value Function</h4> <p>The action-value function, <em><strong>Q(s,a)</strong></em>, returns the expected return if the agent starts in <strong>state s</strong>, takes <strong>action a</strong>, and then follows the policy forever after. It is defined as:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;Q(s,a) = \\mathbb{E} \\left[ R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots \\mid S_t=s, A_t=a \\right]&quot;,&quot;id&quot;:&quot;ELWFLVAQWL&quot;}" data-component-name="LatexBlockToDOM"></div> <p><br/><strong>Interpretation</strong>:</p> <ul><li><p><em><strong>Rt</strong></em>: The immediate reward received at time t.</p></li><li><p><em><strong>&#947;</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><em><strong>S_t = s</strong></em>: The condition that the agent is in state <em><strong>s</strong></em> at time t.</p></li><li><p><em><strong>A_t = a</strong></em>: The condition that the agent takes action <em><strong>a</strong></em> at time t.</p></li></ul> <p><strong>Visual Representation</strong>:<br/>Picture a similar maze but with each state-action pair having a value. The action-value function evaluates how good it is to take a specific action in a given state, considering the expected return from that action and the subsequent policy.</p> <h3>The need for Bellman Equation</h3> <p>In the different types of Value Functions mentioned above, we can see that the expected return to make a decision/take an action depends on agent starting in a state <em><strong>s</strong></em> and following the policy forever or until the episode ends and then summing the rewards. This is a computationally expensive process if it has to be repeated at every state. </p> <p>The Bellman equation provides a solution by breaking down the value function into smaller, manageable parts. It simplifies the calculation by considering the <strong>immediate reward</strong> and the <strong>discounted value of the next state</strong>:</p> <div class="latex-rendered" data-attrs="{&quot;persistentExpression&quot;:&quot;V(s) = \\max_a \\left( R(s,a) + \\gamma V(s') \\right)\n&quot;,&quot;id&quot;:&quot;HIRLROAHVO&quot;}" data-component-name="LatexBlockToDOM"></div> <p><strong>Interpretation</strong>:</p> <ul><li><p><em><strong>V(s)</strong></em>: The value of being in state <em><strong>s</strong></em>.</p></li><li><p><em><strong>R(s,a)</strong></em>: The immediate reward received when taking action <em><strong>a</strong></em> in state <em><strong>s</strong></em>.</p></li><li><p><em><strong>&#947;</strong></em>: The discount factor, which determines how much future rewards are valued compared to immediate rewards.</p></li><li><p><em><strong>V(s')</strong></em>: The value of the next state s'.</p></li></ul> <p>This recursive equation allows for efficient computation of state values without needing to calculate the expected return for each state from scratch. The Bellman equation is crucial for making value-based methods computationally feasible.</p> <h2><strong>Conclusion</strong></h2> <p>Understanding these concepts is essential for grasping the fundamentals of reinforcement learning. The Bellman equation plays a pivotal role in making value-based methods efficient, and visualizing state-value and action-value functions helps in understanding how agents make decisions based on these values. </p> <div><hr/></div> <p>This article is part of our ongoing series on Reinforcement Learning and represents Issue #5 in the "Insights in a Jiffy" collection. We encourage you to read our previous issues in this series for a more comprehensive understanding <a href="https://neuraforge.substack.com/t/reinforcement-learning">here</a>. Each article builds upon the concepts introduced in earlier posts, providing a holistic view of the Reinforcement Learning landscape.</p> <p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a class="button primary button-wrapper" href="https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MTU1NDQ1MSwicG9zdF9pZCI6MTQ4MzQ1MDc1LCJpYXQiOjE3Mjc2Nzc2MTIsImV4cCI6MTczMDI2OTYxMiwiaXNzIjoicHViLTE3NzA3ODEiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0._AB1HTVUrfuX4SuC2RqSrXHYHsyHqhBANsbntnmIFCM"><span>Share</span></a></p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p> <p></p> <p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Insights in a Jiffy #5: How Agents Use State-Value and Action-Value Functions to Optimize Choices]]></summary></entry><entry><title type="html">Understanding Reinforcement Learning: Policy-Based and Value-Based Approaches</title><link href="https://jnk234.github.io/blog/2024/understanding-reinforcement-learning-policy-based-and-value-based-approaches/" rel="alternate" type="text/html" title="Understanding Reinforcement Learning: Policy-Based and Value-Based Approaches"/><published>2024-09-02T00:30:13+00:00</published><updated>2024-09-02T00:30:13+00:00</updated><id>https://jnk234.github.io/blog/2024/understanding-reinforcement-learning-policy-based-and-value-based-approaches</id><content type="html" xml:base="https://jnk234.github.io/blog/2024/understanding-reinforcement-learning-policy-based-and-value-based-approaches/"><![CDATA[<h2>Introduction</h2> <p>Reinforcement Learning (RL) is a fascinating field of artificial intelligence that focuses on how agents learn to make decisions in complex environments. In this blog, we'll explore the two main approaches for solving RL problems: <strong>policy-based and value-based methods</strong>. Let's dive in and uncover how these strategies enable machines to learn and make optimal choices.</p> <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"/><img src="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="3024" height="3780" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3780,&quot;width&quot;:3024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;man standing in the middle of woods&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="man standing in the middle of woods" title="man standing in the middle of woods" srcset="https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1529079018732-bdb88456f8c2?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHxkZWNpc2lvbnxlbnwwfHx8fDE3MjUxMjk4NDh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"/></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Photo by <a href="true">Vladislav Babienko</a> on <a href="https://unsplash.com">Unsplash</a></figcaption></figure></div> <h2>The Policy: The Agent's Brain</h2> <p>At the heart of RL is the concept of a policy, which can be thought of as the agent's brain. Here's what you need to know:</p> <ul><li><p>A policy (often denoted as &#960;) is a function that determines what action to take given the current state.</p></li><li><p>It defines the agent's behaviour at any given time.</p></li><li><p>The ultimate goal in RL is to find the optimal policy (&#960;*) that maximizes the expected return when the agent acts according to it.</p></li></ul> <p>Imagine a robot learning to navigate a maze. The policy would be the rules the robot follows to decide which direction to move based on its current position and what it can "see" around it.</p> <h2>Two Approaches to Train the RL Agent</h2> <p>There are two main ways to train an RL agent:</p> <ol><li><p>Directly: Policy-based methods</p></li><li><p>Indirectly: Value-based methods</p></li></ol> <p>Let's explore each of these approaches.</p> <h3>Policy-Based Methods</h3> <p>Policy-based methods directly teach the agent which action to take in a given state. They work by optimizing the policy function to maximize the expected rewards.</p> <p>There are two types of policies in policy-based methods:</p> <ol><li><p><strong>Deterministic Policies</strong>: For a given state, these always return the same action. Therefore, <br/><em><strong>action = policy(state)</strong></em><br/>Example: If our maze-solving robot is at a T-junction, a deterministic policy might always choose to turn right.</p></li><li><p><strong>Stochastic Policies</strong>: These output a probability distribution over possible actions. Therefore, <br/><em><strong>policy(actions | state) = probability distribution over the set of actions given the current state</strong></em><br/>Example: At the same T-junction, a stochastic policy might assign a 70% chance to turn right and a 30% chance to turn left.</p></li></ol> <h4>Example of a Policy-Based Method: REINFORCE Algorithm</h4> <p>The REINFORCE algorithm is a classic policy-based method. Here's how it might work for our maze-solving robot:</p> <ol><li><p>The robot starts with a random policy.</p></li><li><p>It attempts to solve the maze multiple times, keeping track of the actions it took and the rewards it received.</p></li><li><p>After each attempt, it adjusts its policy:</p><ul><li><p>Actions that led to solving the maze quickly are made more likely.</p></li><li><p>Actions that lead to dead ends or longer paths are made less likely.</p></li></ul></li><li><p>Over time, the robot learns a policy that consistently solves the maze efficiently.</p></li></ol> <h3>Value-Based Methods</h3> <p>Value-based methods work indirectly by teaching the agent to estimate how good it is to be in a particular state, or how good it is to take a specific action in a given state.</p> <p>Key concepts in value-based methods:</p> <ul><li><p>State Value (V): The expected total reward if the agent starts in a specific state and follows the current policy.</p></li><li><p>Action Value (Q): The expected total reward if the agent takes a specific action in a given state and then follows the current policy.</p></li></ul> <p>In value-based methods, the agent chooses actions that lead to states with higher values.</p> <h4>Example of a Value-Based Method: Q-Learning</h4> <p>Q-learning is a popular value-based method that learns the quality of actions in states, represented by Q-values. Here's a simplified explanation of how it works:</p> <ol><li><p>Q-values represent the expected cumulative reward of taking a particular action in a given state and then following the optimal policy thereafter.</p></li><li><p>The Q-function maps state-action pairs to these Q-values.</p></li></ol> <p>For our maze-solving robot:</p> <ol><li><p>The robot starts with no knowledge of the maze, so all Q-values are initialized to zero.</p></li><li><p>As the robot explores the maze, it updates its estimates of the Q-values for each state-action pair.</p></li><li><p>The robot chooses actions based on these Q-values, balancing between exploiting known good actions and exploring new ones.</p></li><li><p>Over time, the Q-values converge, and the robot learns to choose actions that lead it efficiently through the maze.</p></li></ol> <h2>Deep Reinforcement Learning</h2> <p>Deep Reinforcement Learning (DRL) combines RL with deep neural networks to handle high-dimensional state spaces and complex environments. In DRL, neural networks are used to approximate either the policy (in policy-based methods) or the value function (in value-based methods).</p> <p>For our maze-solving robot, imagine if instead of a simple grid-based maze, it had to navigate a complex 3D environment using camera inputs. This would create a high-dimensional state space that traditional RL methods struggle with. DRL can handle this by:</p> <ol><li><p>Using convolutional neural networks to process visual input and extract relevant features.</p></li><li><p>Employing deep neural networks to approximate the Q-function (in Deep Q-Networks) or the policy function (in policy gradient methods).</p></li></ol> <p>A popular DRL algorithm is the Deep Q-Network (DQN), which extends Q-learning by using a deep neural network to approximate the Q-function, taking raw pixels as input and outputting Q-values for each possible action.</p> <h2>Conclusion</h2> <p>In this "Insights in a Jiffy," we've introduced the core concepts of reinforcement learning &#8211; policies and values &#8211; and how they form the foundation of how agents learn to make optimal choices. Both policy-based and value-based methods have their strengths, and the advent of deep reinforcement learning has further expanded their capabilities.</p> <p>However, we've only scratched the surface. Each of these approaches deserves a deeper dive to truly understand their intricacies and applications. Stay tuned for future issues where we'll dedicate entire articles to explore policy-based methods, value-based methods, and deep reinforcement learning in much greater detail.</p> <p>The journey into the world of autonomous decision-making is just beginning, and there's much more to discover in the exciting field of reinforcement learning.</p> <div><hr/></div> <p>This article is part of our ongoing series on Reinforcement Learning and represents Issue #4 in the "Insights in a Jiffy" collection. We encourage you to read our previous issues in this series for a more comprehensive understanding <a href="https://neuraforge.substack.com/t/reinforcement-learning">here</a>. Each article builds upon the concepts introduced in earlier posts, providing a holistic view of the Reinforcement Learning landscape.</p> <p>If you enjoyed this blog, please click the &#10084;&#65039; button, share it with your peers, and subscribe for more content. Your support helps spread the knowledge and grow our community.</p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/p/understanding-reinforcement-learning?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p> <p class="button-wrapper" data-attrs="{&quot;url&quot;:&quot;https://neuraforge.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a class="button primary" href="https://neuraforge.substack.com/subscribe?"><span>Subscribe now</span></a></p> <p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Insights in a Jiffy #4: How Agents Learn to Make Optimal Choices]]></summary></entry></feed>